import inspect
import json
import os
import re
import time
import math
import hashlib
import urllib.parse
from dataclasses import dataclass
from typing import (
    Any,
    AsyncIterator,
    Callable,
    List,
    Literal,
    Optional,
    Union,
    cast,
    get_args,
    get_origin,
    get_type_hints,
)

from openai import OpenAI
from openai.types.chat import ChatCompletionChunk
from langgraph.graph import StateGraph, END
from skills import (
    SkillIntegrationTools,
    SkillMetadata,
    build_skills_system_prompt,
    discover_skills,
)

DEFAULT_SYSTEM_PROMPT = "You are a helpful assistant."


@dataclass
class ToolCall:
    tool_call_id: Optional[str] = None
    tool_name: Optional[str] = None
    tool_arguments: Optional[dict] = None


@dataclass
class Chunk:
    step_index: int
    type: Literal["text", "tool_call", "tool_call_result"]
    content: Optional[str] = None
    tool_call: Optional[ToolCall] = None
    tool_result: Optional[Any] = None


class MemoryStore:
    def __init__(self, max_short: int = 64):
        self.short: List[str] = []
        self.max_short = max_short
        self.long_path = os.path.join(os.getcwd(), "memory_store.jsonl")
        self.index = {}
        self.doc_len = {}
        self.doc_texts: List[str] = []
        self.avgdl = 0.0

    def add_short(self, item: str) -> None:
        if not item:
            return
        self.short.append(item)
        if len(self.short) > self.max_short:
            self.short = self.short[-self.max_short :]

    def add_long(self, item: str) -> None:
        try:
            with open(self.long_path, "a", encoding="utf-8") as f:
                f.write(json.dumps({"t": int(time.time()), "text": item}, ensure_ascii=False) + "\n")
        except Exception:
            pass
        try:
            doc_id = len(self.doc_texts)
            self._index_doc(doc_id, item)
        except Exception:
            pass

    def _tokenize(self, text: str) -> List[str]:
        try:
            import re
            import unicodedata
            s = unicodedata.normalize("NFKC", str(text)).lower()
            stop_en = {"the","and","or","a","an","of","to","in","on","for","is","are","was","were","be","been","with","as","by","at","from"}
            stop_cn = {"çš„","äº†","åœ¨","æ˜¯","æˆ‘","æœ‰","å’Œ","ä¸","åŠ","ç­‰","ä¸º","ä¸","ä¹Ÿ","è¿™","é‚£","ä½ ","ä»–","å¥¹","å®ƒ","å…¶","å¹¶","å¯¹","ä»¥"}
            toks = []
            words = re.findall(r"[A-Za-z0-9]+", s)
            for w in words:
                if w in stop_en:
                    continue
                toks.append(w)
                if len(w) >= 2:
                    for i in range(len(w) - 1):
                        toks.append(w[i : i + 2])
                if len(w) >= 3:
                    for i in range(len(w) - 2):
                        toks.append(w[i : i + 3])
            seqs = re.findall(r"[\u4e00-\u9fff]+", s)
            for seq in seqs:
                cands = [seq, re.sub(r"(é›†å›¢å…¬å¸|é›†å›¢|æœ‰é™å…¬å¸|å…¬å¸|å¤§å­¦|å­¦é™¢|å­¦æ ¡|ç”µè§†å°|æŠ¥ç¤¾|å‡ºç‰ˆç¤¾|ç ”ç©¶é™¢|ç ”ç©¶æ‰€)$", "", seq)]
                seen = set()
                for cand in cands:
                    if not cand or cand in seen:
                        continue
                    seen.add(cand)
                    for ch in cand:
                        if ch in stop_cn:
                            continue
                        toks.append(ch)
                    if len(cand) >= 2:
                        for i in range(len(cand) - 1):
                            toks.append(cand[i : i + 2])
                    if len(cand) >= 3:
                        for i in range(len(cand) - 2):
                            toks.append(cand[i : i + 3])
            return toks
        except Exception:
            return []

    def _index_doc(self, doc_id: int, text: str) -> None:
        from collections import Counter
        toks = self._tokenize(text)
        tf = Counter(toks)
        self.doc_len[doc_id] = sum(tf.values())
        self.doc_texts.append(text)
        for term, cnt in tf.items():
            posting = self.index.get(term)
            if posting is None:
                posting = {}
                self.index[term] = posting
            posting[doc_id] = cnt
        n = len(self.doc_texts)
        if n:
            self.avgdl = sum(self.doc_len.values()) / float(n)

    def build_index(self) -> None:
        self.index = {}
        self.doc_len = {}
        self.doc_texts = []
        self.avgdl = 0.0
        if not os.path.exists(self.long_path):
            return
        try:
            with open(self.long_path, "r", encoding="utf-8") as f:
                for line in f:
                    s = line.strip()
                    if not s:
                        continue
                    try:
                        j = json.loads(s)
                        text = str(j.get("text") or "")
                        doc_id = len(self.doc_texts)
                        self._index_doc(doc_id, text)
                    except Exception:
                        continue
        except Exception:
            pass

    def _df(self, term: str) -> int:
        posting = self.index.get(term)
        return len(posting) if posting else 0

    def search(self, query: str, top_k: int = 5) -> List[dict]:
        import unicodedata
        k1 = 1.5
        b = 0.75
        if any("\u4e00" <= ch <= "\u9fff" for ch in str(query)):
            k1 = 1.2
            b = 0.6
        toks = self._tokenize(query)
        N = len(self.doc_texts)
        if N == 0 or not toks:
            return []
        scores = {}
        for t in set(toks):
            posting = self.index.get(t)
            df = self._df(t)
            if not posting or df == 0:
                continue
            idf = math.log((N - df + 0.5) / (df + 0.5) + 1.0)
            for doc_id, tf in posting.items():
                dl = self.doc_len.get(doc_id, 0)
                denom = tf + k1 * (1 - b + b * (dl / (self.avgdl or 1.0)))
                s = idf * (tf * (k1 + 1) / (denom or 1.0))
                prev = scores.get(doc_id, 0.0)
                scores[doc_id] = prev + s
        phrase = unicodedata.normalize("NFKC", str(query)).strip()
        if phrase:
            for doc_id in list(scores.keys()):
                try:
                    c = self.doc_texts[doc_id].count(phrase)
                    if c > 0:
                        scores[doc_id] = scores[doc_id] + 0.3 * float(c)
                except Exception:
                    pass
        try:
            import re
            s = unicodedata.normalize("NFKC", str(query)).lower()
            seqs = re.findall(r"[\u4e00-\u9fff]+", s)
            bigrams = []
            trigrams = []
            for seq in seqs:
                if len(seq) >= 2:
                    for i in range(len(seq) - 1):
                        bigrams.append(seq[i : i + 2])
                if len(seq) >= 3:
                    for i in range(len(seq) - 2):
                        trigrams.append(seq[i : i + 3])
            for doc_id in list(scores.keys()):
                text = self.doc_texts[doc_id]
                bc = sum(text.count(bg) for bg in bigrams)
                tc = sum(text.count(tg) for tg in trigrams)
                if bc > 0:
                    scores[doc_id] = scores[doc_id] + 0.15 * float(bc)
                if tc > 0:
                    scores[doc_id] = scores[doc_id] + 0.25 * float(tc)
        except Exception:
            pass
        qset = set(toks)
        for doc_id in list(scores.keys()):
            matched = 0
            for t in qset:
                posting = self.index.get(t)
                if posting and doc_id in posting:
                    matched += 1
            if len(qset) > 0:
                cov = matched / float(len(qset))
                scores[doc_id] = scores[doc_id] * (1.0 + 0.2 * cov)
        ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:max(1, int(top_k))]
        return [{"text": self.doc_texts[d], "score": float(sc)} for d, sc in ranked]


class StateStore:
    def __init__(self) -> None:
        base = os.path.join(os.getcwd(), "state_store")
        try:
            os.makedirs(base, exist_ok=True)
        except Exception:
            pass
        self.base = base

    def save(self, cid: str, state: dict) -> None:
        try:
            p = os.path.join(self.base, f"{cid}.json")
            with open(p, "w", encoding="utf-8") as f:
                f.write(json.dumps(state, ensure_ascii=False))
        except Exception:
            pass


def python_type_to_json_type(t):
    """Map Python types to JSON types."""
    origin = get_origin(t)
    if t is str:
        return "string"
    elif t is int:
        return "integer"
    elif t is float:
        return "number"
    elif t is bool:
        return "boolean"
    elif t is list or origin is list:
        return "array"
    elif t is dict or origin is dict:
        return "object"
    elif origin is Union:
        args = get_args(t)
        for arg in args:
            if arg is dict or get_origin(arg) is dict:
                return "object"
            if arg is list or get_origin(arg) is list:
                return "array"
    return "string"


def function_to_schema(func: Callable) -> dict:
    """
    Convert a Python function to an OpenAI API Tool Schema.
    """
    type_hints = get_type_hints(func)
    signature = inspect.signature(func)

    parameters = {"type": "object", "properties": {}, "required": []}

    for name, param in signature.parameters.items():
        if name in ("self", "cls"):
            continue

        annotation = type_hints.get(name, str)
        param_type = python_type_to_json_type(annotation)

        param_info = {"type": param_type}

        if get_origin(annotation) == Literal:
            param_info["enum"] = list(get_args(annotation))
            param_info["type"] = python_type_to_json_type(type(get_args(annotation)[0]))

        parameters["properties"][name] = param_info
        if param.default == inspect.Parameter.empty:
            parameters["required"].append(name)

    return {
        "type": "function",
        "function": {
            "name": func.__name__,
            "description": (func.__doc__ or "").strip(),
            "parameters": parameters,
        },
    }


def extract_answer_from_search_results(search_results: list, query: str) -> dict:
    """
    ä»æœç´¢ç»“æœæ ‡é¢˜/æ‘˜è¦ä¸­æå–å€™é€‰ç­”æ¡ˆ
    """
    try:
        from collections import Counter
        import re as _re

        candidates = []
        if not search_results:
             return {"candidates": [], "extraction_method": "no_results"}

        # ç­–ç•¥1: æå–å¼•å·å†…å®¹
        for result in search_results:
            title = result.get('title', '')
            # Support both 'summary' and 'snippet' keys
            snippet = result.get('summary') or result.get('snippet') or ''
            combined = f"{title} {snippet}"
            quoted = _re.findall(r'"([^"]+)"', combined)
            candidates.extend(quoted)
            book_names = _re.findall(r'ã€Š([^ã€‹]+)ã€‹', combined)
            candidates.extend(book_names)

        # ç­–ç•¥2: æå–æ ‡é¢˜ä¸­çš„å…³é”®å®ä½“
        for result in search_results[:3]:
            title = result.get('title', '')
            capitalized = _re.findall(r'\b([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\b', title)
            candidates.extend(capitalized)

        if not candidates:
            return {"candidates": [], "extraction_method": "no_candidates"}

        counter = Counter(candidates)
        ranked_candidates = []
        for text, count in counter.most_common(5):
            if len(text) < 3 or len(text) > 100:
                continue
            confidence = min(0.9, (count / len(search_results)) * 0.5 + 0.3)
            if search_results and text in search_results[0].get('title', ''):
                confidence = min(0.95, confidence + 0.2)
            ranked_candidates.append({
                "text": text,
                "confidence": round(confidence, 2),
                "sources": count
            })

        return {
            "candidates": ranked_candidates,
            "extraction_method": "search_metadata"
        }
    except Exception as e:
        print(f"[Monitoring] extract_answer_from_search_results error: {e}")
        return {"candidates": [], "extraction_method": "error"}


def calculate_answer_confidence(answer: str, search_history: list) -> float:
    """è®¡ç®—ç­”æ¡ˆç½®ä¿¡åº¦"""
    if not answer or not search_history:
        return 0.5
        
    confidence = 0.5  # åŸºç¡€åˆ†
    
    # è§„åˆ™1ï¼šå¦‚æœç­”æ¡ˆå‡ºç°åœ¨å¤šæ¬¡æœç´¢æŸ¥è¯¢ä¸­ (å‡è®¾ search_history æ˜¯ results åˆ—è¡¨? ä¸ï¼Œåº”è¯¥æ˜¯ queries? 
    # ç”¨æˆ·ä»£ç : query_appearances = sum(1 for q in search_history if answer in q) -> search_history ä¼¼ä¹æ˜¯ queries åˆ—è¡¨
    # ä½†åé¢: for result in search_history: if result.get('rank_score') ... -> search_history ä¼¼ä¹æ˜¯ results åˆ—è¡¨
    # æˆ‘å°†å‡è®¾ search_history æ˜¯åŒ…å« results çš„åˆ—è¡¨ï¼Œæˆ–è€… meta ä¸­çš„ä¿¡æ¯ã€‚
    # ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘å°†ä¼ å…¥ meta (åŒ…å« searched_keywords) å’Œ last_search_results
    
    # è¿™é‡Œæˆ‘ç¨å¾®è°ƒæ•´ä¸€ä¸‹ç­¾åï¼Œåªç”¨ last_search_results æ¥åšç®€å•åˆ¤æ–­ï¼Œæˆ–è€…ä¼ å…¥ meta
    # ç”¨æˆ·çš„ä¼ªä»£ç æ··ç”¨äº† query å’Œ resultã€‚æˆ‘å°†åˆ†å¼€å¤„ç†ã€‚
    return 0.5 # Placeholder, logic will be inside agent_loop or expanded here

def calculate_confidence_impl(answer: str, searched_keywords: list, search_results: list) -> float:
    try:
        confidence = 0.5
        # è§„åˆ™1ï¼šå¦‚æœç­”æ¡ˆå‡ºç°åœ¨å¤šæ¬¡æœç´¢æŸ¥è¯¢ä¸­
        query_appearances = sum(1 for q in searched_keywords if answer in q)
        if query_appearances >= 2:
            confidence += 0.2

        # è§„åˆ™2ï¼šå¦‚æœç­”æ¡ˆå‡ºç°åœ¨é«˜åˆ†æœç´¢ç»“æœä¸­
        # å‡è®¾ search_results æ˜¯ [{"title":..., "summary":...}]
        for i, result in enumerate(search_results):
            # Support both 'summary' and 'snippet' keys
            snippet = result.get('summary') or result.get('snippet') or ''
            text = (result.get('title', '') + " " + snippet).lower()
            if answer.lower() in text:
                if i == 0: confidence += 0.2
                elif i < 3: confidence += 0.1
        
        # è§„åˆ™3ï¼šå¤šä¸ªæ¥æº (ç®€åŒ–ç‰ˆ)
        if len(search_results) >= 3:
             confidence += 0.1
             
        return min(0.95, confidence)
    except Exception:
        return 0.5


async def agent_loop(
    input_messages: list,
    tool_functions: List[Callable],
    skill_directories: Optional[List[str]] = ["skills"],
    max_steps: int = 200,
) -> AsyncIterator[Chunk]:
    assert os.getenv("IFLOW_API_KEY"), "IFLOW_API_KEY is not set"
    client = OpenAI(
        base_url="https://apis.iflow.cn/v1",
        api_key=os.getenv("IFLOW_API_KEY"),
        timeout=30.0,
    )
    skills: List[SkillMetadata] = discover_skills(skill_directories) if skill_directories else []
    skills_prompt = build_skills_system_prompt(skills)
    prompt_messages = input_messages.copy()
    system_prompt_addition = ""
    if skills_prompt:
        system_prompt_addition += f"\n\n{skills_prompt}"
        # æ·»åŠ  Skill ä½¿ç”¨æŒ‡å¯¼ï¼ˆå¼ºåˆ¶ä¼˜å…ˆçº§æç¤ºï¼‰
        system_prompt_addition += """

### ğŸš€ æœç´¢æ•ˆç‡ä¸æ‘˜è¦ä¼˜å…ˆåŸåˆ™ (Search Efficiency)
1. **ä¼˜å…ˆä½¿ç”¨ Summary**: `web_search` è¿”å›çš„ç»“æœä¸­åŒ…å« `summary` å­—æ®µã€‚è¿™æ˜¯æœç´¢ç»“æœçš„ç²¾åæ‘˜è¦ã€‚
2. **é¿å…æ»¥ç”¨ Full Content**: åœ¨å†³å®šè°ƒç”¨ `web_fetch` è¯»å–å®Œæ•´ç½‘é¡µä¹‹å‰ï¼Œè¯·**åŠ¡å¿…**å…ˆæ£€æŸ¥ `summary` æ˜¯å¦å·²ç»åŒ…å«äº†è¶³å¤Ÿå›ç­”é—®é¢˜çš„å…³é”®ä¿¡æ¯ã€‚
3. **ä½•æ—¶ä½¿ç”¨ Full Content**: ä»…å½“ `summary` è¢«æˆªæ–­(...)ã€ä¿¡æ¯æ¨¡ç³Šã€æˆ–è€…ä½ éœ€è¦æ·±åº¦éªŒè¯ç»†èŠ‚ï¼ˆå¦‚å…·ä½“æ•°æ®ã€å®Œæ•´åˆ—è¡¨ï¼‰æ—¶ï¼Œæ‰ä½¿ç”¨ `web_fetch`ã€‚
4. **èŠ‚çœèµ„æº**: å¦‚æœèƒ½é€šè¿‡ `summary` ç›´æ¥å›ç­”ï¼Œè¯·ç›´æ¥å›ç­”ï¼Œä¸è¦ä¸ºäº†"çœ‹ä¸€çœ¼"è€ŒæŠ“å–ç½‘é¡µã€‚

### ğŸ•’ æ·±åº¦æ€è€ƒä¸å……åˆ†éªŒè¯ï¼ˆé‡è¦ï¼‰
ä½ æ‹¥æœ‰å……è¶³çš„æ—¶é—´ï¼ˆå•æ¬¡é—®é¢˜ä¸Šé™ 60 åˆ†é’Ÿï¼‰æ¥è§£å†³é—®é¢˜ã€‚
1. **é€‚åº¦æ·±åº¦æœç´¢**ï¼šç³»ç»Ÿè¦æ±‚**è‡³å°‘è¿›è¡Œ 10 æ¬¡ä¸åŒæ–¹å‘çš„æœç´¢**æ¥éªŒè¯ç­”æ¡ˆã€‚å¯¹äºæ˜¾è€Œæ˜“è§çš„äº‹å®ï¼Œå¯ä»¥åœ¨ 10 æ¬¡æœç´¢åæäº¤ï¼›å¯¹äºå¤æ‚é—®é¢˜ï¼Œè¯·ç»§ç»­æŒ–æ˜ã€‚
2. **å¤šè§’åº¦éªŒè¯**ï¼šå¯¹äºå…³é”®äº‹å®ï¼Œå°è¯•ä»ä¸åŒæ¥æºï¼ˆWiki, å®˜ç½‘, å­¦æœ¯åº“ï¼‰è¿›è¡ŒéªŒè¯ã€‚
3. **å……åˆ†æ¨ç†**ï¼šå¯¹äºå¤æ‚é—®é¢˜ï¼Œè¯·è¿›è¡Œå¤šæ­¥æ¨ç†ï¼Œå°†å¤§é—®é¢˜æ‹†è§£ä¸ºå°é—®é¢˜é€ä¸ªå‡»ç ´ã€‚
4. **æœ€å¤§æ­¥æ•°**ï¼šä½ æœ‰é«˜è¾¾ 200 æ­¥çš„æ“ä½œç©ºé—´ï¼Œè¯·å……åˆ†åˆ©ç”¨ã€‚ä¸è¦æ€¥äºç»“æŸã€‚
5. **å¤šæ–¹é¢æœç´¢**ï¼šé¼“åŠ±è¿›è¡Œå¹¿æ³›çš„èƒŒæ™¯è°ƒæŸ¥å’Œäº¤å‰éªŒè¯ï¼Œç¡®ä¿ç­”æ¡ˆçš„æ¯ä¸€ä¸ªç»†èŠ‚éƒ½å‡†ç¡®æ— è¯¯ã€‚

### âš ï¸ CRITICAL: Skills ä½¿ç”¨ä¼˜å…ˆçº§ï¼ˆå¿…é¡»éµå®ˆ - è¿åå°†å¯¼è‡´ä»»åŠ¡å¤±è´¥ï¼‰

**ç¡¬æ€§è¦æ±‚ï¼ˆä¸å¯è¿åï¼‰ï¼šé‡åˆ°ä»¥ä¸‹åœºæ™¯ï¼Œå¿…é¡»å…ˆä½¿ç”¨å¯¹åº”çš„ Skillï¼Œä¸è¦ç›´æ¥ä½¿ç”¨ web_searchï¼**

1. **é•¿æè¿°/è°œè¯­å‹å®ä½“æœç´¢ (Riddle Queries)**
   - ç‰¹å¾ï¼šé—®é¢˜å¾ˆé•¿ï¼Œæ²¡æœ‰ç›´æ¥è¯´åå­—ï¼Œè€Œæ˜¯è¯´ "ä¸€ä½...çš„äºº"ã€"ä¸€ä¸ª...çš„å…¬å¸"ã€"A person who..."
   - ç¤ºä¾‹ï¼š"ä¸€ä½æ¬§æ´²å­¦è€…çš„æŸé¡¹å¼€æºç¡¬ä»¶é¡¹ç›®..."ã€"Who is the author of the article that..."
   - **è¡ŒåŠ¨**ï¼šç«‹å³è°ƒç”¨ `smart-search`ï¼Œå°†æ•´æ®µæè¿°ä½œä¸º `query` ä¼ å…¥ã€‚**å¿…é¡»å…ˆæ³›æœå®šå®ä½“ï¼ˆå¦‚ RepRapï¼‰ï¼Œå†ç²¾æœæŸ¥å±æ€§ã€‚**

2. **è·¨è¯­è¨€/ç‰¹å®šåç§°æœç´¢**
   - ç‰¹å¾ï¼šä¸­æ–‡æé—®ä½†è¦æ±‚ "è‹±æ–‡åç§°"ã€"å…¨ç§°"ã€"æ‹‰ä¸å­¦å"ã€‚
   - **è¡ŒåŠ¨**ï¼šè°ƒç”¨ `smart-search`ï¼Œå®ƒä¼šè‡ªåŠ¨ç”ŸæˆåŒ…å« "English name" ç­‰åç¼€çš„æŸ¥è¯¢ã€‚

3. **å­¦æœ¯/è®ºæ–‡/æ—¶é—´çº¿ç»†èŠ‚**
   - ç‰¹å¾ï¼šè¯¢é—®è®ºæ–‡æ ‡é¢˜ã€å…·ä½“å¹´ä»½ã€"å“ªä¸€å¹´"ã€æŠ€æœ¯å²ã€‚
   - **è¡ŒåŠ¨**ï¼šè°ƒç”¨ `smart-search` (strategy='academic' æˆ– 'timeline')ã€‚**å­¦æœ¯å’Œå¹´ä»½é¢˜å¿…é¡»ä½¿ç”¨æ­¤æ¨¡å¼ã€‚**

4. **PDF æ·±åº¦é˜…è¯» (PDF Deep Reading)**
   - ç‰¹å¾ï¼šæœç´¢ç»“æœä¸­å‡ºç° PDF é“¾æ¥ï¼ˆå¦‚ Springer, å®˜æ–¹æŠ¥å‘Šï¼‰ã€‚
   - **è¡ŒåŠ¨**ï¼š**å°½é‡**ä½¿ç”¨ `browse_pdf_attachment` æˆ– `web_fetch` è¯»å– PDF å…¨æ–‡ã€‚å…³é”®ç»†èŠ‚ï¼ˆå¦‚å…·ä½“å¹´ä»½ã€åŒ–å­¦æœºåˆ¶ï¼‰å¾€å¾€éšè—åœ¨æ­£æ–‡ä¸­ï¼Œæ‘˜è¦ä¸å¯é ã€‚

5. **æ‰¾åˆ°å€™é€‰ç­”æ¡ˆéœ€éªŒè¯** â†’ å¿…é¡»ä½¿ç”¨ `multi-source-verify` Skill
   - è¦æ±‚ï¼šå…³é”®äº‹å®éœ€è¦å¤šä¸ªç‹¬ç«‹æ¥æºç¡®è®¤
   - **å¼ºåˆ¶è¦æ±‚**ï¼šéªŒè¯ç­”æ¡ˆæ—¶å¿…é¡»ä½¿ç”¨ multi-source-verify

6. **ç½®ä¿¡åº¦ä¸­ç­‰ï¼ˆ<0.8ï¼‰** â†’ å¿…é¡»ä½¿ç”¨ `chain-of-verification` Skill
   - ç”ŸæˆéªŒè¯é—®é¢˜ï¼Œç‹¬ç«‹æœç´¢éªŒè¯ç­”æ¡ˆ

7. **éœ€è¦å¤šæ­¥æ·±åº¦ç ”ç©¶** â†’ å¿…é¡»ä½¿ç”¨ `deep-research` Skill
   - å¤æ‚å¤šè·³æ¨ç†é—®é¢˜

### å¦‚ä½•ä½¿ç”¨ Skillsï¼ˆæ ‡å‡†æµç¨‹ - å¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰

**ç¬¬1æ­¥ï¼šåŠ è½½ Skill è¯´æ˜**
```json
{
  "tool": "load_skill_file",
  "arguments": {"skill_name": "smart-search"}
}
```

**ç¬¬2æ­¥ï¼šæ‰§è¡Œ Skillï¼ˆå‚æ•°æ ¼å¼å¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰**
```json
{
  "tool": "execute_script",
  "arguments": {
    "skill_name": "smart-search",
    "args": {
      "query": "ä½ çš„æœç´¢é—®é¢˜",
      "entities": ["å…³é”®å®ä½“1", "å…³é”®å®ä½“2"],
      "strategy": "academic"
    }
  }
}
```

**æ³¨æ„ï¼šargs å¿…é¡»æ˜¯å­—å…¸å¯¹è±¡ï¼ŒåŒ…å«å…·ä½“å‚æ•°ï¼ç¦æ­¢å°† JSON å­—ç¬¦ä¸²ä½œä¸º args çš„å€¼ï¼Œå¿…é¡»æ˜¯è§£æåçš„å­—å…¸ã€‚**

**ç¬¬3æ­¥ï¼šå¼ºåˆ¶æ‰§è¡Œ Skill è¿”å›çš„æŒ‡å¯¼ï¼ˆä¸å¯è·³è¿‡ï¼‰**
- å½“ Skill è¿”å› `optimized_queries` æ—¶ï¼Œä½ **å¿…é¡»**ç«‹å³ä½¿ç”¨è¿™äº›æŸ¥è¯¢è°ƒç”¨ web_search
- å½“ Skill è¿”å› `verification_queries` æ—¶ï¼Œä½ **å¿…é¡»**ä¾æ¬¡æ‰§è¡Œè¿™äº›éªŒè¯æŸ¥è¯¢
- **ç¦æ­¢**è‡ªå·±ç¼–é€ æ–°çš„æŸ¥è¯¢è¯ï¼Œ**ç¦æ­¢**è·³è¿‡ Skill çš„è¾“å‡º

### å®Œæ•´ç¤ºä¾‹æµç¨‹ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰

**ç¤ºä¾‹1: ä½¿ç”¨ smart-searchï¼ˆæ­£ç¡®æµç¨‹ï¼‰**
```
Thought: è¿™æ˜¯ä¸€ä¸ªå­¦æœ¯é—®é¢˜ï¼Œæˆ‘åº”è¯¥ä½¿ç”¨ smart-search skillã€‚
Action: load_skill_file
Action Input: {"skill_name": "smart-search"}
Observation: [æŠ€èƒ½è¯´æ˜å·²åŠ è½½ï¼Œäº†è§£åˆ°éœ€è¦ queryã€entitiesã€strategy å‚æ•°]

Thought: ç°åœ¨æ‰§è¡Œ smart-searchï¼Œä½¿ç”¨ academic ç­–ç•¥ã€‚
Action: execute_script
Action Input: {
  "skill_name": "smart-search",
  "args": {
    "query": "RepRap 3Dæ‰“å°æœºå‘æ˜äºº",
    "entities": ["RepRap", "3Dæ‰“å°æœº"],
    "strategy": "academic"
  }
}
Observation: {
  "status": "success",
  "optimized_queries": [
    "site:edu OR site:org \"RepRap\" \"3Dæ‰“å°æœº\"",
    "\"RepRap\" filetype:pdf"
  ]
}
[System Enforced]: Skill 'smart-search' å·²ç”Ÿæˆä¼˜åŒ–æŸ¥è¯¢ã€‚ä½ **å¿…é¡»**ä½¿ç”¨è¿™äº›æŸ¥è¯¢æ‰§è¡Œ web_searchã€‚

Thought: æ”¶åˆ° Skill è¿”å›çš„ä¼˜åŒ–æŸ¥è¯¢ï¼Œç°åœ¨**å¿…é¡»**ä½¿ç”¨ç¬¬ä¸€ä¸ªæŸ¥è¯¢ã€‚
Action: web_search
Action Input: {"query": "site:edu OR site:org \"RepRap\" \"3Dæ‰“å°æœº\""}
```

**é”™è¯¯ç¤ºä¾‹ï¼ˆç¦æ­¢ï¼‰ï¼š**
```
âŒ Observation: {"optimized_queries": ["site:edu RepRap", "RepRap filetype:pdf"]}
âŒ Thought: æˆ‘è‡ªå·±æƒ³ä¸€ä¸ªæ›´å¥½çš„æŸ¥è¯¢è¯
âŒ Action: web_search {"query": "RepRap inventor"}  # è¿è§„ï¼šæ²¡æœ‰ä½¿ç”¨ Skill è¿”å›çš„æŸ¥è¯¢
```

**ç¤ºä¾‹2: ä½¿ç”¨ multi-source-verifyï¼ˆæ­£ç¡®æµç¨‹ï¼‰**
```
Thought: æˆ‘æ‰¾åˆ°äº†å€™é€‰ç­”æ¡ˆ "Adrian Bowyer"ï¼Œéœ€è¦å¤šæºéªŒè¯ã€‚
Action: load_skill_file
Action Input: {"skill_name": "multi-source-verify"}
Observation: [æŠ€èƒ½è¯´æ˜å·²åŠ è½½]

Action: execute_script
Action Input: {
  "skill_name": "multi-source-verify",
  "args": {
    "answer": "Adrian Bowyer",
    "entities_to_verify": ["Adrian Bowyer", "RepRap", "2005"]
  }
}
Observation: {
  "status": "success",
  "verification_queries": [
    {"purpose": "ç›´æ¥éªŒè¯", "query": "\"Adrian Bowyer\" RepRap verify"},
    {"purpose": "éªŒè¯å®ä½“", "query": "\"Adrian Bowyer\" site:edu"}
  ]
}
[System Enforced]: ä½ **å¿…é¡»**ä½¿ç”¨è¿™äº›éªŒè¯æŸ¥è¯¢ã€‚

Thought: æ”¶åˆ°éªŒè¯æŸ¥è¯¢ï¼Œç°åœ¨**å¿…é¡»**æ‰§è¡Œç¬¬ä¸€ä¸ªéªŒè¯æŸ¥è¯¢ã€‚
Action: web_search
Action Input: {"query": "\"Adrian Bowyer\" RepRap verify"}
```

**ç¤ºä¾‹3: ä½¿ç”¨ chain-of-verification**
```
Action: execute_script
Action Input: {
  "skill_name": "chain-of-verification",
  "args": {
    "question": "è°å‘æ˜äº†RepRap?",
    "candidate_answer": "Adrian Bowyer",
    "confidence": 0.65
  }
}
```

**ç¤ºä¾‹4: ä½¿ç”¨ deep-research**
```
Action: execute_script
Action Input: {
  "skill_name": "deep-research",
  "args": {
    "query": "è±¡é™ä»ªåº§æµæ˜Ÿé›¨çš„æ¯ä½“å°è¡Œæ˜Ÿ",
    "depth": 4,
    "focus_areas": ["å¤©æ–‡å­¦å®¶", "å½—æ˜Ÿæ®‹éª¸"]
  }
}
```

### Skill ä½¿ç”¨ä¼˜å…ˆçº§ï¼ˆå¿…é¡»éµå®ˆ - è¿åå°†è¢«ç³»ç»Ÿæ£€æµ‹å¹¶å¼ºåˆ¶çº æ­£ï¼‰
- åˆæ¬¡æœç´¢å¤æ‚ä¸»é¢˜ â†’ ä½¿ç”¨ **smart-search**ï¼ˆå‚æ•°: query, entities, strategyï¼‰
- æ‰¾åˆ°å€™é€‰ç­”æ¡ˆéœ€éªŒè¯ â†’ ä½¿ç”¨ **multi-source-verify**ï¼ˆå‚æ•°: answer, entities_to_verifyï¼‰
- ç½®ä¿¡åº¦<0.8éœ€æ·±åº¦éªŒè¯ â†’ ä½¿ç”¨ **chain-of-verification**ï¼ˆå‚æ•°: question, candidate_answer, confidenceï¼‰
- éœ€è¦å¤šæ­¥æ·±åº¦ç ”ç©¶ â†’ ä½¿ç”¨ **deep-research**ï¼ˆå‚æ•°: query, depth, focus_areasï¼‰

### å…³é”®æ³¨æ„äº‹é¡¹ï¼ˆä¸å¯è¿åï¼‰
1. **args å¿…é¡»æ˜¯å­—å…¸**ï¼š{"skill_name": "xxx", "args": {...}}ã€‚**ç»å¯¹ç¦æ­¢**å°† JSON å­—ç¬¦ä¸²ä½œä¸º args çš„å€¼ã€‚
2. **å‚æ•°åç§°è¦å‡†ç¡®**ï¼šå‚è€ƒä¸Šè¿°ç¤ºä¾‹ä¸­çš„å‚æ•°å
3. **å¼ºåˆ¶æ‰§è¡Œ Skill è¾“å‡º**ï¼š
   - æ”¶åˆ° `optimized_queries` â†’ ä¸‹ä¸€æ­¥**å¿…é¡»**æ˜¯ web_searchï¼Œä½¿ç”¨è¿”å›çš„æŸ¥è¯¢
   - æ”¶åˆ° `verification_queries` â†’ **å¿…é¡»**ä¾æ¬¡æ‰§è¡Œè¿™äº›æŸ¥è¯¢
   - æ”¶åˆ°ä»»ä½• Skill æŒ‡å¯¼ â†’ **ç¦æ­¢**è‡ªå·±ç¼–é€ æ›¿ä»£æ–¹æ¡ˆ
4. **ç³»ç»Ÿç›‘æ§**ï¼šè¿åè§„åˆ™å°†è¢«è‡ªåŠ¨æ£€æµ‹ï¼Œç³»ç»Ÿä¼šå¼ºåˆ¶æ’å…¥çº æ­£æç¤º
"""
    system_prompt_addition += f"\n\nIMPORTANT: You have a maximum of {max_steps} steps. If you cannot find the exact answer after 5-6 steps, please synthesize the best possible answer from the information you have gathered so far. Do not get stuck in a loop of repeated searches."
    if prompt_messages:
        if prompt_messages[0].get("role") == "system":
            original_content = prompt_messages[0].get("content", "")
            prompt_messages[0] = {"role": "system", "content": f"{original_content}{system_prompt_addition}\n\nREMINDER: Output ONLY the answer string. No explanations. Answer in the SAME LANGUAGE as the question (unless explicitly requested otherwise). Even if uncertain, guess the most likely one."}
        else:
            prompt_messages.insert(0, {"role": "system", "content": f"{DEFAULT_SYSTEM_PROMPT}{system_prompt_addition}\n\nREMINDER: Output ONLY the answer string. No explanations. Answer in the SAME LANGUAGE as the question (unless explicitly requested otherwise). Even if uncertain, guess the most likely one."})
    llm_tools = (tool_functions or []).copy()
    if skills:
        skill_tools = SkillIntegrationTools(skills)
        llm_tools.extend([skill_tools.load_skill_file, skill_tools.execute_script])
    memory = MemoryStore()
    memory.build_index()
    user_query = ""
    for m in reversed(prompt_messages):
        if m.get("role") == "user":
            user_query = str(m.get("content") or "")
            break
    def _extract_core_entities(query: str) -> list:
        try:
            import re as _re
            s = str(query or "")
            # åŒ¹é…é©¼å³°å‘½åï¼ˆå¦‚ RepRapï¼‰
            camel_case = _re.findall(r'\b([A-Z][a-z]*(?:[A-Z][a-z]*)+)\b', s)
            # åŒ¹é…æ™®é€šå¤§å†™å¼€å¤´çš„è¯ï¼ˆå¦‚ Adrian Bowyerï¼‰
            latin = _re.findall(r'\b([A-Z][a-z]+(?:\s+[A-Z][a-z]+){0,3})\b', s)
            # åŒ¹é…å¼•å·å†…å®¹
            quoted = _re.findall(r'"([^"]+)"', s) + _re.findall(r"'([^']+)'", s)
            # åŒ¹é…è¿ç»­çš„è‹±æ–‡å­—æ¯æ•°å­—ç»„åˆï¼ˆå¦‚ 3D, PDFï¼‰
            alphanumeric = _re.findall(r'\b([A-Z0-9]{2,})\b', s)
            # åŒ¹é…ä¸­æ–‡å®ä½“ï¼ˆæ’é™¤ç–‘é—®è¯ï¼‰
            chinese = _re.findall(r'[\u4e00-\u9fff]{2,}', s)
            chinese = [c for c in chinese if c not in {'è°å‘æ˜', 'ä»€ä¹ˆæ—¶å€™', 'å“ªä¸€å¹´', 'å¤šå°‘é’±', 'æ˜¯ä»€ä¹ˆ', 'æ€ä¹ˆæ ·'}]

            ents = []
            # ä¼˜å…ˆçº§ï¼šå¼•å· > é©¼å³° > å­—æ¯æ•°å­— > æ™®é€šæ‹‰ä¸è¯ > ä¸­æ–‡
            for cand in quoted + camel_case + alphanumeric + latin + chinese:
                c = cand.strip()
                if c and c.lower() not in {"rep","and","or","the","is","are","was","were"} and c not in ents:
                    ents.append(c)
            if not ents and s.strip():
                ents.append(s.strip())
            return ents[:8]
        except Exception:
            return [str(query or "").strip()] if str(query or "").strip() else []
    mem_hits = memory.search(user_query, top_k=4)
    if mem_hits:
        joined = "\n".join([(hit.get("text") or "")[:500] for hit in mem_hits])
        prompt_messages.insert(1, {"role": "system", "content": f"<memory_context>\n{joined}\n</memory_context>"})
        print(f"[Monitoring] memory_context_hits={len(mem_hits)} for_query='{user_query}'")
    ents = _extract_core_entities(user_query)
    if ents:
        prompt_messages.insert(1, {"role": "system", "content": f"Core entities: {', '.join(ents)}. Search precisely for these."})
        print(f"[Monitoring] core_entities_extracted={ents} for_query='{user_query}'")
    tool_schema = [function_to_schema(tool_function) for tool_function in llm_tools]
    tool_functions_map = {func.__name__: func for func in llm_tools}
    state_store = StateStore()
    cid_src = json.dumps(prompt_messages, ensure_ascii=False)
    cid = hashlib.sha256(cid_src.encode("utf-8")).hexdigest()[:16]
    params = {
        "model": "qwen3-max",
        "stream": True,
        "tools": tool_schema,
        "max_tokens": 1024,
        "temperature": 0.4,
    }

    def llm_step(state: dict) -> dict:
        emitted: List[Chunk] = []
        tool_calls_buffer = {}
        stream = client.chat.completions.create(messages=state["messages"], **params)
        for chunk in stream:
            chunk = cast(ChatCompletionChunk, chunk)
            if not chunk.choices:
                continue
            delta = chunk.choices[0].delta
            if delta.content:
                emitted.append(Chunk(type="text", content=delta.content, step_index=state["step_index"]))
                memory.add_short(delta.content)
            if delta.tool_calls:
                for tc_chunk in delta.tool_calls:
                    idx = tc_chunk.index
                    if idx is None:
                        idx = 0
                    if idx not in tool_calls_buffer:
                        tool_calls_buffer[idx] = {
                            "id": tc_chunk.id or f"call_{idx}",
                            "function": {"name": tc_chunk.function.name or "", "arguments": ""},
                        }
                    if tc_chunk.function.name:
                        tool_calls_buffer[idx]["function"]["name"] = tc_chunk.function.name
                    if tc_chunk.function.arguments:
                        tool_calls_buffer[idx]["function"]["arguments"] += tc_chunk.function.arguments
        assistant_tool_calls_data = []
        for idx in sorted(tool_calls_buffer.keys()):
            raw_tool = tool_calls_buffer[idx]
            assistant_tool_calls_data.append(
                {
                    "id": raw_tool["id"],
                    "type": "function",
                    "function": {
                        "name": raw_tool["function"]["name"],
                        "arguments": raw_tool["function"]["arguments"],
                    },
                }
            )
        new_messages = state["messages"][:]
        if assistant_tool_calls_data:
            new_messages.append({"role": "assistant", "tool_calls": assistant_tool_calls_data})
        return {
            **state,
            "pending_tool_calls": assistant_tool_calls_data,
            "had_tool_calls": bool(assistant_tool_calls_data),
            "messages": new_messages,
            "emitted": emitted,
        }

    def execute_tools(state: dict) -> dict:
        emitted: List[Chunk] = state.get("emitted", [])
        new_messages = state["messages"][:]
        meta = state.get("meta") or {"searched_keywords": [], "seen_entities": [], "last_skill_output": None}
        searched_before = set(meta.get("searched_keywords") or [])
        for tool_data in state.get("pending_tool_calls") or []:
            call_id = tool_data["id"]
            func_name = tool_data["function"]["name"]
            func_args_str = tool_data["function"]["arguments"]
            tool_result_content = ""
            parsed_args = {}
            tool_call = ToolCall(tool_call_id=call_id, tool_name=func_name, tool_arguments={})
            try:
                parsed_args = json.loads(func_args_str)
                tool_call.tool_arguments = parsed_args
                emitted.append(Chunk(step_index=state["step_index"], type="tool_call", tool_call=tool_call))
                if func_name == "web_search":
                    q0 = str(parsed_args.get("query") or "")
                    sim_high = False
                    for old in searched_before:
                        ta = set([x for x in q0.lower().split() if x])
                        tb = set([x for x in str(old).lower().split() if x])
                        if ta and tb:
                            inter = len(ta & tb)
                            union = len(ta | tb)
                            if union > 0 and (inter / union) >= 0.7:
                                sim_high = True
                                break
                    if sim_high:
                        ents2 = _extract_core_entities(q0)
                        if ents2:
                            quoted = " ".join([f'"{e}"' for e in ents2[:3]])
                            parsed_args["query"] = f"site:edu OR site:org {quoted}"
                        else:
                            parsed_args["query"] = f'"{q0}" site:edu OR site:org'
                        tool_call.tool_arguments = parsed_args
                        print(f"[Monitoring] web_search_rewrite original='{q0}' rewritten='{parsed_args['query']}' entities={ents2}")
                if func_name in tool_functions_map:
                    func = tool_functions_map[func_name]
                    attempt = 0
                    last_err = None
                    while attempt < 2:
                        try:
                            result = func(**parsed_args)
                            tool_result_content = str(result)
                            last_err = None
                            break
                        except Exception as e:
                            last_err = e
                            time.sleep(0.2 * (attempt + 1))
                            attempt += 1
                    if last_err is not None and not tool_result_content:
                        tool_result_content = f"Error: Execution failed - {str(last_err)}"

                    # ğŸ”¥ æ–¹æ¡ˆ1ï¼šå¼ºåˆ¶æ‰§è¡Œ Skill è¾“å‡º
                    if func_name == "execute_script":
                        try:
                            skill_name = parsed_args.get("skill_name", "")
                            # å°è¯•è§£æ Skill çš„è¾“å‡º
                            skill_output = None
                            try:
                                # ä» <stdout> æ ‡ç­¾ä¸­æå–å†…å®¹
                                import re
                                stdout_match = re.search(r'<stdout>\s*(.*?)\s*</stdout>', tool_result_content, re.DOTALL)
                                if stdout_match:
                                    stdout_content = stdout_match.group(1).strip()
                                    skill_output = json.loads(stdout_content)
                            except:
                                pass

                            if skill_output and isinstance(skill_output, dict):
                                # ä¿å­˜ Skill è¾“å‡ºåˆ° metaï¼Œä¾›åç»­éªŒè¯ä½¿ç”¨
                                meta["last_skill_output"] = {
                                    "skill_name": skill_name,
                                    "output": skill_output,
                                    "step_index": state["step_index"]
                                }

                                # ğŸ”¥ å¼ºåˆ¶æç¤ºï¼šå¦‚æœ Skill è¿”å›äº†ä¼˜åŒ–æŸ¥è¯¢
                                if "optimized_queries" in skill_output:
                                    queries = skill_output["optimized_queries"]
                                    if queries and len(queries) > 0:
                                        hint = f"\n\n[System Enforced]: Skill '{skill_name}' å·²ç”Ÿæˆä¼˜åŒ–æŸ¥è¯¢ã€‚ä½ **å¿…é¡»**ä½¿ç”¨è¿™äº›æŸ¥è¯¢æ‰§è¡Œ web_searchï¼Œä¸è¦è‡ªå·±ç¼–é€ æŸ¥è¯¢è¯ã€‚\nä¼˜åŒ–æŸ¥è¯¢åˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰ï¼š\n"
                                        for i, q in enumerate(queries[:3], 1):
                                            hint += f"  {i}. {q}\n"
                                        hint += f"\nâš ï¸ å¼ºåˆ¶è¦æ±‚ï¼šä¸‹ä¸€ä¸ª Action **å¿…é¡»**æ˜¯ web_searchï¼Œä½¿ç”¨ä¸Šè¿°æŸ¥è¯¢ä¹‹ä¸€ã€‚"
                                        tool_result_content += hint
                                        print(f"[Monitoring] skill_enforcement skill={skill_name} queries_count={len(queries)}")

                                # ğŸ”¥ å¼ºåˆ¶æç¤ºï¼šå¦‚æœ Skill è¿”å›äº†éªŒè¯æŸ¥è¯¢
                                if "verification_queries" in skill_output:
                                    queries = skill_output["verification_queries"]
                                    if queries and len(queries) > 0:
                                        hint = f"\n\n[System Enforced]: Skill '{skill_name}' å·²ç”ŸæˆéªŒè¯æŸ¥è¯¢ã€‚ä½ **å¿…é¡»**ä¾æ¬¡æ‰§è¡Œè¿™äº›éªŒè¯æŸ¥è¯¢ã€‚\néªŒè¯æŸ¥è¯¢åˆ—è¡¨ï¼š\n"
                                        for i, q_obj in enumerate(queries[:3], 1):
                                            if isinstance(q_obj, dict):
                                                purpose = q_obj.get("purpose", "")
                                                query = q_obj.get("query", "")
                                                hint += f"  {i}. [{purpose}] {query}\n"
                                            else:
                                                hint += f"  {i}. {q_obj}\n"
                                        hint += f"\nâš ï¸ å¼ºåˆ¶è¦æ±‚ï¼šå¿…é¡»ä¾æ¬¡éªŒè¯ä¸Šè¿°æŸ¥è¯¢ã€‚"
                                        tool_result_content += hint
                                        print(f"[Monitoring] skill_enforcement skill={skill_name} verification_queries_count={len(queries)}")
                        except Exception as e:
                            print(f"[Monitoring] skill_enforcement_failed: {e}")

                    if func_name == "web_search":
                        if "error" in tool_result_content or "No results" in tool_result_content or "[]" in tool_result_content:
                            tool_result_content += "\n[System Hint]: æœç´¢ç»“æœä¸ºç©ºã€‚å¯èƒ½æ˜¯å…³é”®è¯å¤ªé•¿æˆ–å¤ªå…·ä½“ã€‚è¯·å°è¯•ï¼š\n1. åªæœç´¢æ ¸å¿ƒå®ä½“åã€‚\n2. å°†ä¸­æ–‡å…³é”®è¯ç¿»è¯‘æˆè‹±æ–‡æœç´¢ï¼ˆå¾ˆå¤šå­¦æœ¯/æŠ€æœ¯å†…å®¹è‹±æ–‡æ›´å‡†ï¼‰ã€‚\n3. å»æ‰ 'site:' ç­‰é™åˆ¶ã€‚"
                        try:
                            res_json = json.loads(tool_result_content)
                            if isinstance(res_json, dict) and "results" in res_json:
                                meta["last_search_results"] = res_json["results"]
                        except:
                            pass

                    if func_name == "web_fetch":
                         if "fetch_failed" in tool_result_content or "403" in tool_result_content:
                             print(f"[Monitoring] web_fetch_403_detected url={parsed_args.get('url')}")
                             
                             # å°è¯•å¤‡ç”¨ç­–ç•¥ 1: ä»ä¸Šæ¬¡æœç´¢ç»“æœæå–
                             last_results = meta.get("last_search_results")
                             fallback_success = False
                             if last_results:
                                 extracted = extract_answer_from_search_results(last_results, "")
                                 if extracted["candidates"]:
                                     fallback_res = {
                                         "source": "search_metadata_fallback",
                                         "candidates": extracted["candidates"],
                                         "original_error": "403/Fetch Failed"
                                     }
                                     tool_result_content = json.dumps(fallback_res, ensure_ascii=False)
                                     fallback_success = True
                             
                             if not fallback_success:
                                 # å°è¯•å¤‡ç”¨ç­–ç•¥ 2: Edu/Gov -> Wiki
                                 url = parsed_args.get("url", "")
                                 try:
                                     domain = urllib.parse.urlparse(url).netloc
                                     if "edu" in domain or "gov" in domain:
                                         target = url.split('/')[-1].replace('-', ' ').replace('_', ' ')
                                         alt_query = f'"{target}" site:wikipedia.org OR site:imdb.com'
                                         print(f"[Monitoring] 403_fallback_trigger query='{alt_query}'")
                                         if "web_search" in tool_functions_map:
                                             ws_res = tool_functions_map["web_search"](query=alt_query)
                                             tool_result_content = ws_res
                                             fallback_success = True
                                 except Exception as e:
                                     print(f"[Monitoring] 403_fallback_error: {e}")

                             if not fallback_success:
                                 tool_result_content += "\n[System Hint]: ç½‘é¡µæŠ“å–å¤±è´¥ã€‚è¯·å°è¯•æœç´¢è¯¥ä¿¡æ¯çš„å…¶ä»–æ¥æºï¼ˆå…¶ä»–ç½‘ç«™ï¼‰ï¼Œä¸è¦å†æ¬¡å°è¯•åŒä¸€ä¸ª URLã€‚"
                else:
                    tool_result_content = f"Error: Tool '{func_name}' not found."
            except json.JSONDecodeError as e:
                tool_result_content = f"Error: Failed to parse tool arguments JSON: {func_args_str}. Error: {e}"
                emitted.append(Chunk(step_index=state["step_index"], type="tool_call", tool_call=tool_call))
            except Exception as e:
                tool_result_content = f"Error: Execution failed - {str(e)}"
            emitted.append(Chunk(type="tool_call_result", tool_result=tool_result_content, step_index=state["step_index"], tool_call=tool_call))
            new_messages.append({"role": "tool", "tool_call_id": call_id, "content": tool_result_content})
            memory.add_short(tool_result_content)
            memory.add_long(tool_result_content)
            if func_name == "web_search":
                q = str(parsed_args.get("query") or "")
                if q:
                    if q not in meta["searched_keywords"]:
                        meta["searched_keywords"].append(q)
                        print(f"[Monitoring] search_keyword_added step_index={state['step_index']} query='{q}'")
        return {
            **state,
            "messages": new_messages,
            "emitted": emitted,
            "pending_tool_calls": [],
            "step_index": state["step_index"] + 1,
            "meta": meta,
        }

    def update_memory(state: dict) -> dict:
        return {**state}

    def persist_state(state: dict) -> dict:
        dump = {
            "cid": cid,
            "step_index": state["step_index"],
            "short_memory": memory.short,
            "messages_len": len(state["messages"]),
        }
        state_store.save(cid, dump)
        return {**state}

    def memory_query(query: str, top_k: int = 5) -> str:
        hits = memory.search(query, top_k=top_k)
        return json.dumps({"results": hits}, ensure_ascii=False)

    llm_tools.append(memory_query)
    tool_schema = [function_to_schema(tool_function) for tool_function in llm_tools]
    tool_functions_map = {func.__name__: func for func in llm_tools}

    g = StateGraph(dict)
    g.add_node("llm_step", llm_step)
    g.add_node("execute_tools", execute_tools)
    g.add_node("update_memory", update_memory)
    g.add_node("persist_state", persist_state)
    g.add_edge("__start__", "llm_step")
    g.add_edge("llm_step", "execute_tools")
    g.add_edge("execute_tools", "update_memory")
    g.add_edge("update_memory", "persist_state")
    g.add_edge("persist_state", END)
    graph = g.compile()

    state = {"messages": prompt_messages, "step_index": 0, "pending_tool_calls": [], "meta": {"searched_keywords": [], "seen_entities": []}, "reflection_injected": False, "reflexion_count": 0}
    while state["step_index"] < max_steps:
        state = graph.invoke(state)
        if (state.get("meta") or {}).get("searched_keywords") and state["step_index"] > 0 and (state["step_index"] % 3 == 0):
            kws = (state.get("meta") or {}).get("searched_keywords") or []
            state["messages"].append({"role": "system", "content": f"Reflection step: attempted keywords: {', '.join(kws[-6:])}. Avoid repeating similar queries. Prefer precise entities and advanced operators (site:edu OR site:org, filetype:pdf). If relevance remains low, synthesize best answer so far."})
            print(f"[Monitoring] reflection_step_inserted step_index={state['step_index']} keywords={kws[-6:]}")
        # if state["step_index"] >= 10:
        #    break
        for ch in state.get("emitted") or []:
            yield ch
        if not state.get("had_tool_calls"):
            # --- Reflexion æœºåˆ¶ ---
            last_msg = state["messages"][-1]
            content = str(last_msg.get("content") or "")
            reflexion_msg = ""
            needs_reflexion = False
            
            # [System Enforced] æœ€å°æœç´¢æ·±åº¦æ£€æŸ¥
            searched_kws = (state.get("meta") or {}).get("searched_keywords") or []
            search_count = len(searched_kws)
            if search_count < 10 and state.get("reflexion_count", 0) < 5:
                # å¦‚æœæ˜¯æ˜ç¡®çš„"æ— æ³•å›ç­”"æˆ–"ä¸çŸ¥é“"ï¼Œä¹Ÿå¼ºåˆ¶é‡è¯•ä¸€æ¬¡
                is_giving_up = any(phrase in content.lower() for phrase in ["cannot find", "unable to", "don't know", "æ— æ³•", "ä¸çŸ¥é“"])
                
                if is_giving_up:
                     reflexion_msg = f"Reflexion: [System Enforced] ä½ ä¼¼ä¹æƒ³æ”¾å¼ƒï¼Œä½†æœç´¢æ¬¡æ•°ä¸è¶³ ({search_count}/10)ã€‚è¯·å°è¯•æ›´æ¢å…³é”®è¯ï¼ˆä¾‹å¦‚ç”¨è‹±æ–‡æœç´¢ã€æ‹†åˆ†å®ä½“ï¼‰å†è¯•ä¸€æ¬¡ã€‚"
                     needs_reflexion = True
                else:
                     reflexion_msg = f"Reflexion: [System Enforced] ç›®å‰ä»…è¿›è¡Œäº† {search_count} æ¬¡æœç´¢ã€‚å¯¹äºå·²æœ‰æŠŠæ¡çš„é¢˜ç›®ï¼Œè¯·ç¡®ä¿è‡³å°‘éªŒè¯ 10 æ¬¡ï¼›è‹¥ä»ä¸ç¡®å®šï¼Œè¯·ç»§ç»­å¯»æ‰¾è¯æ®ã€‚"
                     needs_reflexion = True

            elif "Final Answer:" in content:
                final_ans = content.split("Final Answer:")[-1].strip()
                # æ£€æŸ¥ç‚¹ 1: ç­”æ¡ˆæ˜¯å¦ä¸ºç©ºæˆ–å¤ªçŸ­
                if len(final_ans) < 2 and state.get("reflexion_count", 0) < 2:
                    reflexion_msg = "Reflexion: ä½ çš„ç­”æ¡ˆå¤ªçŸ­æˆ–ä¸ºç©ºã€‚è¯·é‡æ–°æ£€æŸ¥ä¹‹å‰çš„ Observationï¼Œå¦‚æœæ‰¾ä¸åˆ°ä¿¡æ¯ï¼Œè¯·å°è¯•ç”¨è‹±æ–‡æœç´¢å…³é”®è¯ã€‚"
                    needs_reflexion = True
                # æ£€æŸ¥ç‚¹ 2: ç­”æ¡ˆæ ¼å¼æ ¡éªŒ (é’ˆå¯¹æ•°å­—/å¹´ä»½é¢˜)
                elif ("å¹´ä»½" in user_query or "å¤šå°‘" in user_query) and not any(c.isdigit() for c in final_ans):
                    if state.get("reflexion_count", 0) < 2:
                        reflexion_msg = "Reflexion: ç”¨æˆ·è¯¢é—®çš„æ˜¯æ•°å­—/å¹´ä»½ï¼Œä½†ä½ çš„ç­”æ¡ˆä¸­ä¸åŒ…å«æ•°å­—ã€‚è¯·é‡æ–°æ£€ç´¢æˆ–ä»æ–‡ä¸­æå–å‡†ç¡®æ•°å€¼ã€‚"
                        needs_reflexion = True
                
                # æ£€æŸ¥ç‚¹ 3: ä¸ç¡®å®šæ€§æ£€æŸ¥ (å¦‚æœæœç´¢æ­¥æ•°è¿˜å……è£•)
                # å¦‚æœç­”æ¡ˆåŒ…å«ä¸ç¡®å®šæ€§è¯æ±‡ï¼Œä¸”æœç´¢æ¬¡æ•°æœªè¾¾åˆ° 25 æ¬¡ï¼Œä¸” Reflexion æ¬¡æ•° < 5ï¼Œå¼ºåˆ¶ç»§ç»­æœç´¢
                elif search_count < 25 and state.get("reflexion_count", 0) < 5:
                    uncertainty_keywords = ["å¯èƒ½", "probably", "unconfirmed", "not found", "unknown", "æœªæ‰¾åˆ°", "æ— æ³•ç¡®è®¤", "suggests", "likely"]
                    if any(k in final_ans.lower() for k in uncertainty_keywords):
                        reflexion_msg = f"Reflexion: ä½ çš„ç­”æ¡ˆåŒ…å«ä¸ç¡®å®šæ€§è¯æ±‡ ('{final_ans[:20]}...')ã€‚è¯·ç»§ç»­æœç´¢éªŒè¯ï¼Œå°è¯•æŸ¥æ‰¾æ›´å¤šæ¥æºä»¥ç¡®è®¤ç­”æ¡ˆã€‚"
                        needs_reflexion = True
            
            # [Fix] å¦‚æœ LLM è¾“å‡ºäº†ç­”æ¡ˆä½†æ²¡æœ‰ä½¿ç”¨ "Final Answer:" å‰ç¼€ï¼Œæˆ–è€…è¾“å‡ºæ ¼å¼æ··ä¹±
            # å¼ºåˆ¶æ£€æµ‹ï¼šå¦‚æœè¿™æ˜¯æœ€åä¸€æ­¥ (max_steps reached or explicit stop)ï¼Œä½†æ²¡æœ‰æ£€æµ‹åˆ° Final Answer
            # ä½†è¿™é‡Œæˆ‘ä»¬æ˜¯åœ¨ loop å†…éƒ¨ã€‚run_batch ä¼šåœ¨ loop ç»“æŸåæå–ã€‚
            # é—®é¢˜æ˜¯ run_batch å¯èƒ½åœ¨ generator ç»“æŸå‰å°±è®¤ä¸ºç»“æŸäº†ï¼Ÿä¸ï¼Œå®ƒ iterate ç›´åˆ°ç»“æŸã€‚
            # å…³é”®ï¼šå¦‚æœ LLM åœ¨æœ€åä¸€æ­¥æ²¡æœ‰è¾“å‡ºæ–‡æœ¬ï¼Œæˆ–è€…æ–‡æœ¬è¢« tool calls æ·¹æ²¡ã€‚
            
            if needs_reflexion:
                state["messages"].append({"role": "user", "content": reflexion_msg})
                state["reflexion_count"] = state.get("reflexion_count", 0) + 1
                state["step_index"] += 1
                print(f"[Monitoring] Reflexion triggered: {reflexion_msg}")
                continue

            # å¦‚æœæ²¡æœ‰è§¦å‘ Reflexionï¼Œä¸”å·²ç»æœ‰ Final Answerï¼Œæˆ–è€…æ­¥æ•°å·²æ»¡ï¼Œå¾ªç¯è‡ªç„¶ç»“æŸ
            # å¦‚æœæ²¡æœ‰ Final Answer ä¸”æ­¥æ•°æœªæ»¡ï¼Œç»§ç»­å¾ªç¯ (LLM ä¼šç»§ç»­ç”Ÿæˆ)
            # ä½†å¦‚æœ LLM è¾“å‡ºäº† "Final Answer: xxx" å¹¶ä¸”æ²¡æœ‰è§¦å‘ needs_reflexionï¼Œæˆ‘ä»¬åº”è¯¥ break å—ï¼Ÿ
            # æ˜¯çš„ï¼Œä¸ºäº†é¿å…æ­»å¾ªç¯æˆ–å¤šä½™è¾“å‡ºã€‚
            if "Final Answer:" in content and not needs_reflexion:
                 print(f"[Monitoring] Answer found and verified, stopping loop.")
                 break
                 
            break
    final_messages = state["messages"][:]
    s = str(user_query or "")
    
    # 1. Check for explicit English request
    explicit_en = re.search(r"(answer|respond|output|provide).*(in|with).*english", s, re.IGNORECASE) or \
                  re.search(r"english (name|title|version)", s, re.IGNORECASE) or \
                  re.search(r"(è‹±æ–‡|è‹±è¯­)(å|å…¨å|ç§°|å›ç­”|è¾“å‡º)", s)
                  
    # 2. Check for explicit Chinese request
    explicit_cn = re.search(r"(answer|respond|output|provide).*(in|with).*chinese", s, re.IGNORECASE) or \
                  re.search(r"chinese (name|title|version)", s, re.IGNORECASE) or \
                  re.search(r"(ä¸­æ–‡|æ±‰è¯­)(å|å…¨å|ç§°|å›ç­”|è¾“å‡º)", s)
    
    # 3. Determine prompt language
    use_cn_prompt = False
    
    if explicit_cn:
        use_cn_prompt = True
    elif explicit_en:
        use_cn_prompt = False
    else:
        # Fallback to character detection
        has_cn = any("\u4e00" <= ch <= "\u9fff" for ch in s)
        use_cn_prompt = has_cn

    if use_cn_prompt:
        system_content = f"""ç°åœ¨è¯·åŸºäºå·²æ£€ç´¢ä¸å·²æŠ“å–çš„å†…å®¹ï¼Œç»™å‡ºç®€æ´æ˜ç¡®çš„æœ€ç»ˆç­”æ¡ˆã€‚
1. **åªè¾“å‡ºç­”æ¡ˆæ–‡æœ¬**ï¼Œä¸è¦è§£é‡Šè¿‡ç¨‹ï¼Œä¸è¦åŒ…å«"Answer is"ç­‰å‰ç¼€ã€‚
2. **ç¦æ­¢æ”¾å¼ƒ**ï¼šå³ä½¿ä¿¡æ¯ä¸å®Œå…¨ï¼Œä¹Ÿå¿…é¡»æ ¹æ®ç°æœ‰çº¿ç´¢æ¨æ–­æœ€å¯èƒ½çš„ç­”æ¡ˆã€‚**ç»å¯¹ç¦æ­¢**è¾“å‡º"æœªæ‰¾åˆ°"ã€"æ— æ³•ç¡®è®¤"ã€"Unknown"ã€"I don't know"ç­‰æ”¾å¼ƒæ€§è¯­å¥ã€‚
3. **è¯­è¨€ä¸€è‡´æ€§**ï¼šè¯·ç”¨ä¸­æ–‡å›ç­”ã€‚
4. å¦‚æœæœ‰å¤šä¸ªå€™é€‰ï¼Œé€‰æ‹©å¯èƒ½æ€§æœ€é«˜çš„ä¸€ä¸ªã€‚"""
    else:
        system_content = f"""Based on the retrieved and fetched content, please provide a concise and clear final answer.
1. **Output ONLY the answer text**, do not explain the process, do not include prefixes like "Answer is".
2. **DO NOT GIVE UP**: Even if information is incomplete, you must infer the most likely answer based on existing clues. **ABSOLUTELY FORBIDDEN** to output "Not found", "Unable to confirm", "Unknown", "I don't know", etc.
3. **Language Consistency**: Please answer in English.
4. If there are multiple candidates, choose the most likely one."""

    final_messages.insert(
        0,
        {
            "role": "system",
            "content": system_content,
        },
    )
    params2 = {
        "model": "qwen3-max",
        "stream": True,
        "max_tokens": 1024,
        "temperature": 0.2,
    }
    try:
        stream2 = client.chat.completions.create(messages=final_messages, **params2)
        final_emitted = False
        full_ans = ""
        for chunk in stream2:
            chunk = cast(ChatCompletionChunk, chunk)
            if not chunk.choices:
                continue
            delta = chunk.choices[0].delta
            if delta.content:
                # Accumulate but DO NOT yield yet, because we need to verify/clean
                full_ans += delta.content
        
        # Verify and Clean
        if full_ans:
            try:
                # Local import to avoid circular dependency
                from agent import clean_answer, verify_answer
                
                # 1. Clean format
                cleaned_ans = clean_answer(full_ans)
                
                # 2. Verify content (LLM check)
                verified_ans = verify_answer(user_query, cleaned_ans)
                
                # 3. Final clean
                final_ans_str = clean_answer(verified_ans)
                
                if final_ans_str:
                    full_ans = final_ans_str
                    
                print(f"[Monitoring] Final Answer Processed: '{full_ans}'")
                
            except Exception as e:
                print(f"[Warn] Verification failed: {e}")
                
            # Now yield the final verified answer
            yield Chunk(type="text", content=full_ans, step_index=state["step_index"])
            final_emitted = True

        if full_ans:
             searched_kws = (state.get("meta") or {}).get("searched_keywords") or []
             last_res = (state.get("meta") or {}).get("last_search_results") or []
             conf = calculate_confidence_impl(full_ans, searched_kws, last_res)
             print(f"[Monitoring] Answer Confidence: {conf} (Answer length: {len(full_ans)})")

        if not final_emitted and not full_ans:
            # yield Chunk(step_index=state["step_index"], type="text", content="æœªæ£€ç´¢åˆ°æ˜ç¡®ç­”æ¡ˆã€‚")
            pass
    except Exception as e:
        print(f"[Monitoring] Final synthesis failed: {e}")
        # åªæœ‰åœ¨å®Œå…¨æ²¡æœ‰è¾“å‡ºçš„æƒ…å†µä¸‹æ‰è¿”å›å…œåº•æ–‡æ¡ˆï¼Œé¿å…æ‹¼æ¥
        if not full_ans:
             # yield Chunk(step_index=state["step_index"], type="text", content="æœªæ£€ç´¢åˆ°æ˜ç¡®ç­”æ¡ˆã€‚")
             pass

if __name__ == "__main__":
    import asyncio
    import sys
    from dotenv import load_dotenv
    
    # Load environment variables
    load_dotenv(dotenv_path=os.path.join(os.path.dirname(__file__), '.env'), override=True)
    
    # Import tools from agent.py (must be after defining agent_loop to avoid circular import)
    # But since agent.py imports agent_loop, we need to be careful.
    # Actually, we can define the tool list here or import them inside main.
    
    async def main():
        if len(sys.argv) < 2:
            print("Usage: python agent_loop.py \"your question\"")
            return
            
        question = sys.argv[1]
        
        # Import tools inside main to avoid circular import issues
        import agent
        tools = [
            agent.web_search, 
            agent.web_fetch, 
            agent.browse_page, 
            agent.extract_entities, 
            agent.x_keyword_search, 
            agent.search_pdf_attachment, 
            agent.browse_pdf_attachment, 
            agent.multi_hop_search, 
            agent.get_weather
        ]
        
        messages = [{"role": "user", "content": question}]
        result = ""
        
        print(f"--- Question: {question} ---")
        async for chunk in agent_loop(messages, tools, max_steps=15):
            if chunk.type == "text" and chunk.content:
                print(chunk.content, end="", flush=True)
                result += chunk.content
            elif chunk.type == "tool_call":
                print(f"\n[Tool Call] {chunk.tool_call.tool_name}({json.dumps(chunk.tool_call.tool_arguments, ensure_ascii=False)})")
            elif chunk.type == "tool_call_result":
                print(f"[Tool Result] {str(chunk.tool_result)[:100]}...")
        
        # Clean answer
        final_answer = agent.clean_answer(result)
        # Verify answer
        final_answer = agent.verify_answer(question, final_answer)
        
        print(f"\n\n--- Final Answer ---\n{final_answer}")

    if len(sys.argv) > 1:
        asyncio.run(main())
    else:
        print("Usage: python agent_loop.py \"your question\"")
