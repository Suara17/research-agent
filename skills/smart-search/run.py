#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Smart Search Skill v3 - ç»“æ„åŒ–æ„å›¾åˆ†æä¸åˆ—è¡¨ç”Ÿæˆå¢å¼ºç‰ˆ
"""

import os
import json
import sys
import re
import io

# [Fix] Force UTF-8 encoding for stdout/stderr to prevent UnicodeEncodeError on Windows
# especially when printing foreign characters (e.g., Mongolian, Cyrillic)
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

from openai import OpenAI

def _load_env():
    """åŠ è½½ .env æ–‡ä»¶"""
    try:
        paths = [
            os.path.join(os.getcwd(), ".env"),
            os.path.join(os.path.dirname(os.path.abspath(__file__)), ".env"),
            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))), ".env")
        ]
        for p in paths:
            if os.path.exists(p):
                with open(p, "r", encoding="utf-8") as f:
                    for line in f:
                        line = line.strip()
                        if not line or line.startswith("#") or "=" not in line:
                            continue
                        k, v = line.split("=", 1)
                        if k not in os.environ:
                            os.environ[k.strip()] = v.strip().strip("'").strip('"')
    except Exception:
        pass

def _get_client():
    api_key = os.environ.get("IFLOW_API_KEY")
    if not api_key:
        return None
    return OpenAI(base_url="https://apis.iflow.cn/v1", api_key=api_key, timeout=15.0)

def _analyze_query_with_llm(query: str, original_entities: list, excluded_entities: list = [], feedback: str = "") -> dict:
    """
    ä½¿ç”¨ LLM è¿›è¡Œç»“æ„åŒ–æŸ¥è¯¢åˆ†æ
    è¿”å›: {
        "intent": "person_search" | "fact_check" | "list_generation" | "riddle",
        "keywords_en": ["keyword1", "keyword2"],
        "keywords_native": ["å…³é”®è¯1", "å…³é”®è¯2"],
        "search_queries": ["query1", "query2"]
    }
    """
    client = _get_client()
    if not client:
        return None

    # ğŸ”¥ ä¼˜åŒ–åçš„ Promptï¼šåˆ†å±‚æœç´¢ + æƒå¨æ¥æºä¼˜å…ˆ + ç¡¬çº¦æŸå‰ç½®
    prompt = f"""You are a Search Engine Optimization Expert with expertise in hierarchical verification strategies.
Analyze the user query and generate TARGETED search queries with authoritative source prioritization.

User Query: "{query}"
Known Entities: {json.dumps(original_entities, ensure_ascii=False)}
Excluded Entities: {json.dumps(excluded_entities, ensure_ascii=False)} (MUST be excluded)
Previous Feedback/Failure Context: "{feedback}" (CRITICAL: Adjust queries to address this feedback)

Analysis Tasks:
1. **Identify Intent**: Is it looking for a specific person, checking a fact, solving a riddle, or **Resolving a Conflict** (e.g. "A is at B but not in B's city")?
2. **Extract Constraints (CRITICAL)**: 
   - **Hard Constraints**: Dates (e.g., "1990s", "2024"), Legal Terms ("Constitution", "Amendment"), Locations ("Southern Europe"), Roles ("Head of Government" vs "Head of State").
   - **Soft Constraints**: Themes ("Scandal", "Corruption"), Attributes ("Studied abroad", "Relatives").
   - **Negative/Relational Constraints**: "Not situated in that city", "Outside the capital", "Partner of".
   - âš ï¸ STRATEGY: You MUST generate queries that combine Hard Constraints to narrow the field BEFORE adding Soft Constraints.

3. **Cross-Lingual**: If the topic implies a non-English country (e.g., Mongolia, Japan), generate queries in English AND that specific language context.
   - Analyze the CONTENT constraints (e.g., "1990s constitution", "mineral corruption") to infer the correct region.
   
4. **ğŸ”¥ HIERARCHICAL SEARCH STRATEGY (CRITICAL)** - Generate queries in this priority order:

   **A. Hard Constraint Filtering (First Pass)**:
      - Combine Date + Legal/Formal Term + Broad Region/Category.
      - Example: "Constitution enacted 1990-1995 amended 2017-2019 head of state powers" (No soft constraints yet).
      - Example: "List of Prime Ministers appointed by Head of State in [Region]"

   **B. For Education/Background Verification**:
      Priority 1: "Person Name" + university + site:edu (e.g., "John Doe site:harvard.edu alumni")
      Priority 2: site:linkedin.com "Person Name" education
      Priority 3: "Person Name" + parliament/government + site:.gov biography

   **C. For Scandal/Corruption Verification**:
      Priority 1: "Person Name" + scandal + site:reuters.com OR site:bbc.com (authoritative news)
      Priority 2: "Person Name" + relatives + assets + investigation

   **D. For Conflict/Riddle Resolution (Spatial/Logic)**:
      - If query implies "A at B but not in B's city": Search for "A location vs B location", "A branches", "A partnership B", "A history original location".
      - Example: "Museum A location" AND "Venue B location" (Separate queries to verify mismatch).
      - Query: "List of partners of Venue B" 

   **E. General Strategy** (for any "Who is..." questions):
      Query 1: "List of..." (CRITICAL to prevent premature convergence)
      Query 2: Hard Constraints ONLY (to find the right country/context)
      Query 3: Full detailed query

5. **ğŸ”¥ AUTHORITATIVE SOURCE PRIORITY**:
   - Education Background: site:.edu > site:linkedin.com > site:parliament.gov > general news
   - Historical Events: site:wikipedia.org > site:.gov > site:.edu > general news
   - Scandal/Corruption: site:reuters.com OR site:bbc.com > local investigative journalism

   IMPORTANT: Place site: filters at the BEGINNING of queries for better search precision.

6. **Negative Constraints (Postponed)**: Append exclusion terms at the END of queries to avoid over-filtering.
   Format: "main query keywords site:authoritative.source -ExcludedEntity1 -ExcludedEntity2"

7. **Feedback Adaptation**: If 'Previous Feedback' is provided, you MUST generate queries that specifically target the missing information.

Output JSON format ONLY:
{{
    "intent": "string",
    "primary_language_of_topic": "string (e.g., English, Chinese, Mongolian)",
    "extracted_keywords": ["str"],
    "hard_constraints": ["str"],
    "verification_focus": "string",
    "generated_queries": [
        "Priority 1: Hard Constraint Filter Query",
        "Priority 2: Authoritative Verification Query",
        "List generation query",
        "Precise keyword query -exclude",
        "Cross-lingual query (if applicable) -exclude"
    ]
}}
"""
    try:
        response = client.chat.completions.create(
            model="qwen3-max",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,
            response_format={"type": "json_object"},
            max_tokens=512
        )
        content = response.choices[0].message.content
        return json.loads(content)
    except Exception as e:
        print(f"DEBUG: LLM Analysis failed: {e}", file=sys.stderr)
        return None

def _fallback_keyword_extraction(text: str) -> list:
    """æ­£åˆ™å…œåº•æå–é€»è¾‘ (åŸ _extract_keywords çš„ç®€åŒ–ç‰ˆ)"""
    stopwords = {'what', 'who', 'find', 'search', 'question', 'answer', 'the', 'a', 'in', 'of', 'and'}
    words = re.findall(r'[\u4e00-\u9fff]{2,}|[a-zA-Z0-9]+', text)
    return [w for w in words if w.lower() not in stopwords and not (w.isdigit() and len(w)<4)]

def main():
    try:
        _load_env()
        # å‚æ•°è§£æé€»è¾‘ä¿æŒä¸å˜
        args_file = os.environ.get("SKILL_ARGS_FILE")
        if args_file and os.path.exists(args_file):
            with open(args_file, 'r', encoding='utf-8') as f:
                args = json.load(f)
        else:
            args_json = os.environ.get("SKILL_ARGS", "{}")
            args = json.loads(args_json) if isinstance(args_json, str) else args_json

        if isinstance(args, str):
            try: args = json.loads(args)
            except: pass
        
        query = args.get("query", "")
        entities = args.get("entities", [])
        excluded = args.get("excluded_entities", [])
        feedback = args.get("feedback", "")
        strategy = args.get("strategy", "auto")

        if not query:
            print(json.dumps({"error": "No query provided"}))
            sys.exit(1)

        # 1. å°è¯• LLM ç»“æ„åŒ–åˆ†æ (ä¼˜å…ˆ)
        llm_result = _analyze_query_with_llm(query, entities, excluded, feedback)
        
        final_queries = []
        keywords = []
        
        if llm_result:
            # ä½¿ç”¨ LLM ç”Ÿæˆçš„é«˜è´¨é‡æŸ¥è¯¢
            final_queries = llm_result.get("generated_queries", [])
            keywords = llm_result.get("extracted_keywords", [])
            
            # ç­–ç•¥è¡¥ä¸ï¼šå¦‚æœ LLM æ²¡æœ‰ç”Ÿæˆ wiki æŸ¥è¯¢ï¼Œæ‰‹åŠ¨è¡¥ä¸€ä¸ª
            has_site = any("site:" in q for q in final_queries)
            if not has_site:
                base_kw = " ".join(keywords[:5])
                extra_q = f'{base_kw} site:wikipedia.org OR site:baike.baidu.com'
                if excluded:
                    extra_q += " " + " ".join([f"-{e}" for e in excluded])
                final_queries.append(extra_q)
                
        else:
            # 2. é™çº§å›é€€ï¼šåŸºäºè§„åˆ™çš„ç”Ÿæˆ (åŸé€»è¾‘çš„ç²¾ç®€ç‰ˆ)
            keywords = _fallback_keyword_extraction(query)
            base_search = " ".join(keywords[:8])
            
            # æ„é€ æ’é™¤åç¼€
            neg_suffix = ""
            if excluded:
                 neg_suffix = " " + " ".join([f"-{e}" for e in excluded])
            
            final_queries.append(base_search + neg_suffix) # åŸºç¡€æŸ¥è¯¢
            final_queries.append(f"{base_search} wikipedia{neg_suffix}") # ç™¾ç§‘æŸ¥è¯¢
            
            # ç®€å•çš„è§„åˆ™è¡¥å……
            if "who" in query.lower() or "list" in query.lower():
                final_queries.append(f"List of {base_search}{neg_suffix}")
            if any(k in query.lower() for k in ['year', 'when', 'date']):
                final_queries.append(f"{base_search} timeline{neg_suffix}")

        # 3. ç»“æœå»é‡ä¸æ¸…æ´—
        unique_queries = []
        seen = set()
        for q in final_queries:
            q_clean = re.sub(r'\s+', ' ', q).strip()
            if q_clean and q_clean.lower() not in seen:
                seen.add(q_clean.lower())
                unique_queries.append(q_clean)

        result = {
            "status": "success",
            "strategy_used": llm_result.get("intent", "fallback") if llm_result else "regex_fallback",
            "original_query": query,
            "optimized_queries": unique_queries[:5], # é™åˆ¶è¿”å›æ•°é‡
            "tips": "å·²åˆ©ç”¨ LLM åˆ†ææ„å›¾å¹¶ç”Ÿæˆç»“æ„åŒ–æŸ¥è¯¢ã€‚" if llm_result else "LLM åˆ†æè¶…æ—¶ï¼Œä½¿ç”¨åŸºç¡€å…³é”®è¯æŸ¥è¯¢ã€‚"
        }
        print(json.dumps(result, ensure_ascii=False, indent=2))

    except Exception as e:
        print(json.dumps({"error": str(e), "status": "failed"}), file=sys.stdout)
        sys.exit(1)

if __name__ == "__main__":
    main()
