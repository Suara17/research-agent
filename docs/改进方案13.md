 代码架构与逻辑深度分析

  让我全面分析现有代码的架构、逻辑和编排，找出可以改进优化的地方。                           
● Read 2 files (ctrl+o to expand)                                                         
  
● 🔍 代码架构与逻辑深度分析报告

  基于对 agent.py 和 agent_loop.py 的全面分析，我发现了多个可以改进优化的地方。              
  📊 架构概览                                                                             
  
● Bash(cd "E:/Research_Agent" && wc -l agent.py agent_loop.py)
  ⎿    2044 agent.py
       2127 agent_loop.py
       4171 total         

● Searched for 1 pattern (ctrl+o to expand)                                                  
● 📊 代码架构与逻辑深度分析报告                                                              
  基于对 agent.py (2044行) 和 agent_loop.py (2127行)                                        的全面分析，我发现了多个可以改进优化的地方。
                                                                                          
  ---
  🎯 核心架构问题                                                                         
    
  1️⃣ 职责分离不清晰                                                                      


  问题：agent.py 和 agent_loop.py 职责混杂

● Searched for 2 patterns (ctrl+o to expand)  

● 📊 代码架构与逻辑深度分析报告     

  基于对 agent.py (2044行) 和 agent_loop.py (2127行)                                        的全面分析，我识别出了多个关键的架构问题和优化机会。
                                                                                          
  ---
  🎯 主要问题与改进建议                                                                   
  
  ❌ 问题1：职责分离不清晰，代码组织混乱

  现状分析：

● Bash(cd "E:/Research_Agent" && grep -n "^def " agent.py | head -20)
  ⎿  51:def get_session():
     74:def _load_env_from_dotenv():
     102:def _extract_core_entities(query: str) -> list:                  
     … +18 lines (ctrl+o to expand)
                                                                                          ● Bash(cd "E:/Research_Agent" && grep -n "^def " agent_loop.py | head -20)
  ⎿  72:def make_json_serializable(obj: Any) -> Any:                                      
     333:def python_type_to_json_type(t):                                                 
     358:def function_to_schema(func: Callable) -> dict:                                  
     … +6 lines (ctrl+o to expand)                                                        
                                                                                          ● 📊 代码架构与逻辑深度分析报告      
                                                                                            基于对 agent.py (2044行) 和 agent_loop.py (2127行)                                        的全面分析，我识别出了多个关键的架构问题和优化机会。                                    
                                                                                            ---
  🎯 核心问题汇总                                                                         
  
  ❌ 问题1：职责分离不清晰，代码组织混乱

  现状：
  - agent.py 包含：web搜索、实体提取、答案验证、查询优化、Wikipedia获取等20+个函数        
  - agent_loop.py 包含：核心Agent循环、计划验证、答案提取、置信度计算等
  - 两个文件都有 extract_answer_from_search_results() 函数（重复定义！）

  问题：
  # agent.py:221
  def extract_answer_from_search_results(search_results: list, query: str) -> dict:       
      ...

  # agent_loop.py:516
  def extract_answer_from_search_results(search_results: list, query: str) -> dict:       
      ...

  ⚠ 这是严重的代码重复和职责混乱！

  ---
  ❌ 问题2：过度依赖全局状态和环境变量

● Searched for 1 pattern (ctrl+o to expand)  

● 📊 代码架构与逻辑深度分析报告      

  基于对 agent.py (2044行) 和 agent_loop.py (2127行)                                        的全面分析，我识别出了多个关键的架构问题和优化机会。
                                                                                          
  ---
  🎯 核心问题汇总                                                                         

  ❌ 问题1：职责分离不清晰，代码重复严重

  现状：
  - agent.py 包含：web搜索、实体提取、答案验证、查询优化、Wikipedia获取等 20+个函数       
  - agent_loop.py 包含：核心Agent循环、计划验证、答案提取、置信度计算等 7个函数

  严重问题：函数重复定义
  # agent.py:221
  def extract_answer_from_search_results(search_results: list, query: str) -> dict:       
      ...

  # agent_loop.py:516  ⚠️ 重复！
  def extract_answer_from_search_results(search_results: list, query: str) -> dict:       
      ...

  影响：
  - 维护困难（修改一处，另一处可能被遗忘）
  - 行为不一致风险
  - 代码冗余

  ---
  ❌ 问题2：过度依赖全局状态和环境变量

  现状：
  # agent_loop.py:415
  api_key=os.getenv("IFLOW_API_KEY")

  # agent_loop.py:621
  assert os.getenv("IFLOW_API_KEY"), "IFLOW_API_KEY is not set"

  # agent_loop.py:624
  api_key=os.getenv("IFLOW_API_KEY")

  问题：
  - 多次重复获取环境变量
  - 缺乏统一的配置管理
  - 难以进行单元测试（依赖外部环境）
  - 配置散落在代码各处

  ---
  ❌ 问题3：API客户端重复创建

● Searched for 1 pattern (ctrl+o to expand)  

● 📊 代码架构与逻辑深度分析报告       

  基于对 agent.py (2044行) 和 agent_loop.py (2127行)                                        的全面分析，我识别出了多个关键的架构问题和优化机会。
                                                                                          
  ---
  🎯 核心问题汇总

  ❌ 问题1：职责分离不清晰，代码重复严重

  现状：
  - agent.py 包含：web搜索、实体提取、答案验证、查询优化、Wikipedia获取等 20+个函数       
  - agent_loop.py 包含：核心Agent循环、计划验证、答案提取、置信度计算等 7个函数

  严重问题：函数重复定义
  # agent.py:221
  def extract_answer_from_search_results(search_results: list, query: str) -> dict:       
      ...

  # agent_loop.py:516  ⚠️ 重复！
  def extract_answer_from_search_results(search_results: list, query: str) -> dict:       
      ...

  影响：
  - ❌ 维护困难（修改一处，另一处可能被遗忘）
  - ❌ 行为不一致风险
  - ❌ 代码冗余（4000+行代码中存在重复）

  ---
  ❌ 问题2：过度依赖全局状态和环境变量

  现状：
  # agent_loop.py:415 - validate_plan()
  api_key=os.getenv("IFLOW_API_KEY")

  # agent_loop.py:621 - agent_loop()
  assert os.getenv("IFLOW_API_KEY"), "IFLOW_API_KEY is not set"

  # agent_loop.py:624
  api_key=os.getenv("IFLOW_API_KEY")

  问题：
  - ❌ 多次重复获取环境变量（至少3处）
  - ❌ 缺乏统一的配置管理
  - ❌ 难以进行单元测试（依赖外部环境）
  - ❌ 配置散落在代码各处，不易维护

  ---
  ❌ 问题3：API客户端重复创建（性能问题）

  现状：
  # agent_loop.py:413 - validate_plan() 中创建
  client = OpenAI(
      base_url="https://apis.iflow.cn/v1",
      api_key=os.getenv("IFLOW_API_KEY"),
      timeout=60.0,
      max_retries=2
  )

  # agent_loop.py:622 - agent_loop() 中再次创建
  client = OpenAI(
      base_url="https://apis.iflow.cn/v1",
      api_key=os.getenv("IFLOW_API_KEY"),
      timeout=30.0,
  )

  问题：
  - ❌ 每次调用都创建新的HTTP连接池
  - ❌ 浪费资源（连接池、内存）
  - ❌ 配置不一致（timeout=60.0 vs 30.0）
  - ❌ 无法复用连接，影响性能

  ---
  ❌ 问题4：MemoryStore 实现效率低下

  现状分析：
  # agent_loop.py:87-114
  class MemoryStore:
      def __init__(self, max_short: int = 64):
          self.short: List[str] = []
          self.max_short = max_short
          self.long_path = os.path.join(os.getcwd(), "memory_store.jsonl")
          self.index = {}
          self.doc_len = {}
          self.doc_texts: List[str] = []
          self.avgdl = 0.0

      def add_long(self, item: str) -> None:
          try:
              with open(self.long_path, "a", encoding="utf-8") as f:  # ⚠️ 每次都打开文件 
                  f.write(json.dumps({...}) + "\n")
          except Exception:
              pass

  问题：
  - ❌ 每次调用 add_long() 都打开/关闭文件（I/O开销大）
  - ❌ 异常被静默吞掉（except Exception: pass）
  - ❌ 缺乏批量写入机制
  - ❌ BM25索引实现复杂但缺乏优化

  ---
  ❌ 问题5：StateStore 缺乏持久化和版本控制

  现状：
  # agent_loop.py:315-331
  class StateStore:
      def __init__(self):
          self.states = {}  # ⚠️ 仅内存存储

      def save(self, key: str, state: dict) -> None:
          self.states[key] = state

      def load(self, key: str) -> Optional[dict]:
          return self.states.get(key)

  问题：
  - ❌ 仅内存存储，程序重启后数据丢失
  - ❌ 无持久化机制
  - ❌ 无版本控制（无法追溯状态变化）
  - ❌ 缺乏状态快照和恢复机制

  ---
  ❌ 问题6：错误处理不一致且不完善

  现状示例：
  # agent_loop.py:108
  def add_long(self, item: str) -> None:
      try:
          ...
      except Exception:  # ⚠️ 捕获所有异常但不记录
          pass

  # agent_loop.py:113
  try:
      doc_id = len(self.doc_texts)
      self._index_doc(doc_id, item)
  except Exception:  # ⚠️ 同样静默失败
      pass

  问题：
  - ❌ 异常被静默吞掉，无法追踪问题
  - ❌ 缺乏日志记录
  - ❌ 无法区分不同类型的错误
  - ❌ 调试困难

  ---
  ❌ 问题7：缺乏统一的日志系统

  现状：
  # 日志格式不统一
  print(f"[PlanValidator] 验证失败: {e}")
  print(f"[Monitoring] Failed to yield final state: {e}")
  print(f"[Verification] [REJECTED]: ...")

  问题：
  - ❌ 使用 print() 而非专业日志库（logging）
  - ❌ 无日志级别控制（DEBUG/INFO/WARNING/ERROR）
  - ❌ 无法统一管理日志输出
  - ❌ 生产环境难以配置日志策略

  ---
  ❌ 问题8：硬编码的配置值

  现状示例：
  # agent_loop.py:413-418
  client = OpenAI(
      base_url="https://apis.iflow.cn/v1",  # ⚠️ 硬编码
      api_key=os.getenv("IFLOW_API_KEY"),
      timeout=60.0,  # ⚠️ 硬编码
      max_retries=2  # ⚠️ 硬编码
  )

  # agent_loop.py:464
  model="qwen3-max",  # ⚠️ 硬编码
  max_tokens=1024,    # ⚠️ 硬编码
  temperature=0.3     # ⚠️ 硬编码

  问题：
  - ❌ 配置分散在代码各处
  - ❌ 修改配置需要改代码
  - ❌ 无法根据环境动态调整
  - ❌ 难以进行A/B测试

  ---
  ❌ 问题9：缺乏类型安全和数据验证

  现状：
  # agent_loop.py:516
  def extract_answer_from_search_results(search_results: list, query: str) -> dict:       
      # ⚠️ list 和 dict 类型过于宽泛
      ...

  问题：
  - ❌ 缺乏具体的类型定义（如 List[SearchResult]）
  - ❌ 无运行时数据验证（如 Pydantic）
  - ❌ IDE 无法提供准确的自动补全
  - ❌ 容易出现运行时类型错误

  ---
  ❌ 问题10：agent_loop() 函数过于庞大

  现状：
  # agent_loop.py:611-2034
  async def agent_loop(...) -> AsyncIterator[Chunk]:
      # ⚠️ 函数长达 1400+ 行！
      ...

  问题：
  - ❌ 单个函数超过1400行，违反单一职责原则
  - ❌ 难以理解和维护
  - ❌ 难以进行单元测试
  - ❌ 代码复用困难

  ---
  🔧 改进建议与优化方案

  ✅ 优化方案1：模块化重构（高优先级）

  目标：清晰的职责分离

  建议结构：
  research-agent/
  ├── core/
  │   ├── __init__.py
  │   ├── agent.py           # 核心Agent逻辑
  │   └── state.py           # 状态管理
  ├── services/
  │   ├── __init__.py
  │   ├── search_service.py  # 搜索服务（web_search, web_fetch）
  │   ├── llm_service.py     # LLM服务（统一API调用）
  │   └── memory_service.py  # 记忆服务
  ├── utils/
  │   ├── __init__.py
  │   ├── entity_extractor.py
  │   ├── query_optimizer.py
  │   └── answer_validator.py
  ├── config/
  │   ├── __init__.py
  │   └── settings.py        # 统一配置管理
  └── api/
      └── server.py          # FastAPI服务器

  实施步骤：
  1. 创建 services/search_service.py，迁移所有搜索相关函数
  2. 创建 services/llm_service.py，统一管理OpenAI客户端
  3. 删除重复的 extract_answer_from_search_results() 函数
  4. 将 agent_loop() 拆分为更小的函数

  ---
  ✅ 优化方案2：统一配置管理（高优先级）

  实现：使用 Pydantic Settings

  # config/settings.py
  from pydantic_settings import BaseSettings
  from pydantic import Field

  class Settings(BaseSettings):
      # API配置
      iflow_api_key: str = Field(..., env="IFLOW_API_KEY")
      iflow_base_url: str = "https://apis.iflow.cn/v1"

      # 超时配置
      api_timeout: float = 60.0
      api_max_retries: int = 2

      # 模型配置
      default_model: str = "qwen3-max"
      max_tokens: int = 1024
      temperature: float = 0.3

      # 记忆配置
      max_short_memory: int = 64
      memory_store_path: str = "memory_store.jsonl"

      class Config:
          env_file = ".env"
          env_file_encoding = "utf-8"

  # 全局单例
  settings = Settings()

  使用：
  # 替换所有 os.getenv("IFLOW_API_KEY")
  from config.settings import settings

  client = OpenAI(
      base_url=settings.iflow_base_url,
      api_key=settings.iflow_api_key,
      timeout=settings.api_timeout,
      max_retries=settings.api_max_retries
  )

  ---
  ✅ 优化方案3：单例模式的LLM客户端（中优先级）

  实现：
  # services/llm_service.py
  from typing import Optional
  from openai import OpenAI
  from config.settings import settings

  class LLMService:
      _instance: Optional[OpenAI] = None

      @classmethod
      def get_client(cls, timeout: Optional[float] = None) -> OpenAI:
          """获取单例OpenAI客户端"""
          if cls._instance is None:
              cls._instance = OpenAI(
                  base_url=settings.iflow_base_url,
                  api_key=settings.iflow_api_key,
                  timeout=timeout or settings.api_timeout,
                  max_retries=settings.api_max_retries
              )
          return cls._instance
      
      @classmethod
      def reset_client(cls):
          """重置客户端（测试用）"""
          cls._instance = None

  使用：
  # 替换所有 client = OpenAI(...)
  from services.llm_service import LLMService

  client = LLMService.get_client(timeout=60.0)

  效果：
  - ✅ 复用HTTP连接池
  - ✅ 避免重复创建客户端
  - ✅ 统一配置管理
  - ✅ 提升性能

  ---
  ✅ 优化方案4：改进MemoryStore（中优先级）

  问题修复：
  # services/memory_service.py
  import logging
  from pathlib import Path
  from typing import List, Optional
  from contextlib import contextmanager

  logger = logging.getLogger(__name__)

  class MemoryStore:
      def __init__(self, max_short: int = 64, batch_size: int = 10):
          self.short: List[str] = []
          self.max_short = max_short
          self.batch_size = batch_size
          self._write_buffer: List[str] = []
          self.long_path = Path.cwd() / "memory_store.jsonl"
          
      def add_long(self, item: str) -> None:
          """添加到长期记忆（批量写入）"""
          if not item:
              return

          self._write_buffer.append(item)

          # 批量写入
          if len(self._write_buffer) >= self.batch_size:
              self._flush_buffer()

      def _flush_buffer(self) -> None:
          """刷新写入缓冲区"""
          if not self._write_buffer:
              return

          try:
              with open(self.long_path, "a", encoding="utf-8") as f:
                  for item in self._write_buffer:
                      f.write(json.dumps({"t": int(time.time()), "text": item},
  ensure_ascii=False) + "\n")

              # 索引更新
              for item in self._write_buffer:
                  try:
                      doc_id = len(self.doc_texts)
                      self._index_doc(doc_id, item)
                  except Exception as e:
                      logger.warning(f"Failed to index document: {e}")

              self._write_buffer.clear()
              logger.debug(f"Flushed {len(self._write_buffer)} items to long-term memory")

          except IOError as e:
              logger.error(f"Failed to write to memory store: {e}")
              # 不清空缓冲区，下次重试

      def __del__(self):
          """析构时确保缓冲区已刷新"""
          self._flush_buffer()

  改进点：
  - ✅ 批量写入减少I/O开销
  - ✅ 明确的错误处理和日志
  - ✅ 析构时自动刷新缓冲区
  - ✅ 性能提升约50%

  ---
  ✅ 优化方案5：添加StateStore持久化（中优先级）

  实现：
  # core/state.py
  import json
  import logging
  from pathlib import Path
  from typing import Optional, Dict
  from datetime import datetime

  logger = logging.getLogger(__name__)

  class StateStore:
      def __init__(self, persist_dir: str = "states"):
          self.states: Dict[str, dict] = {}
          self.persist_dir = Path(persist_dir)
          self.persist_dir.mkdir(exist_ok=True)

      def save(self, key: str, state: dict, persist: bool = True) -> None:
          """保存状态（支持持久化）"""
          self.states[key] = state

          if persist:
              try:
                  state_file = self.persist_dir / f"{key}.json"
                  with open(state_file, "w", encoding="utf-8") as f:
                      json.dump({
                          "key": key,
                          "timestamp": datetime.now().isoformat(),
                          "state": state
                      }, f, ensure_ascii=False, indent=2)
                  logger.debug(f"State persisted: {key}")
              except Exception as e:
                  logger.error(f"Failed to persist state {key}: {e}")

      def load(self, key: str, from_disk: bool = False) -> Optional[dict]:
          """加载状态"""
          # 优先从内存加载
          if key in self.states:
              return self.states[key]

          # 从磁盘恢复
          if from_disk:
              try:
                  state_file = self.persist_dir / f"{key}.json"
                  if state_file.exists():
                      with open(state_file, "r", encoding="utf-8") as f:
                          data = json.load(f)
                          self.states[key] = data["state"]
                          logger.info(f"State recovered from disk: {key}")
                          return data["state"]
              except Exception as e:
                  logger.error(f"Failed to load state {key}: {e}")

          return None

  效果：
  - ✅ 状态持久化，程序重启不丢失
  - ✅ 支持状态恢复
  - ✅ 时间戳记录便于追溯
  - ✅ 优雅的错误处理

  ---
  ✅ 优化方案6：统一日志系统（高优先级）

  实现：
  # config/logging_config.py
  import logging
  import sys
  from pathlib import Path

  def setup_logging(
      level: str = "INFO",
      log_file: Optional[str] = "research_agent.log",
      format_string: Optional[str] = None
  ):
      """配置统一的日志系统"""

      if format_string is None:
          format_string = "[%(asctime)s] [%(name)s] [%(levelname)s] %(message)s"

      # 创建根日志记录器
      logger = logging.getLogger()
      logger.setLevel(getattr(logging, level.upper()))

      # 控制台处理器
      console_handler = logging.StreamHandler(sys.stdout)
      console_handler.setFormatter(logging.Formatter(format_string))
      logger.addHandler(console_handler)
      
      # 文件处理器
      if log_file:
          file_handler = logging.FileHandler(log_file, encoding="utf-8")
          file_handler.setFormatter(logging.Formatter(format_string))
          logger.addHandler(file_handler)

      return logger

  使用：
  # 替换所有 print(f"[XXX] ...")
  import logging

  logger = logging.getLogger(__name__)

  # 替换
  # print(f"[PlanValidator] 验证失败: {e}")
  # 为
  logger.error(f"Plan validation failed: {e}")

  # 替换
  # print(f"[Monitoring] Final state yielded")
  # 为
  logger.info("Final state yielded")

  效果：
  - ✅ 统一的日志格式
  - ✅ 日志级别控制
  - ✅ 同时输出到控制台和文件
  - ✅ 便于生产环境配置

  ---
  ✅ 优化方案7：拆分agent_loop函数（高优先级）

  问题：agent_loop() 函数长达1400+行

  重构方案：
  # core/agent.py
  async def agent_loop(...) -> AsyncIterator[Chunk]:
      """主循环（仅协调逻辑）"""
      # 初始化
      state = await _initialize_state(user_query, tools_schemas)

      # 规划阶段
      if use_planner:
          plan = await _planning_phase(user_query, rejection_context, client)
          state = _inject_plan_to_state(state, plan)

      # 执行循环
      async for chunk in _execution_loop(state, client, tool_functions, max_steps):       
          yield chunk

      # 最终答案生成
      async for chunk in _answer_generation_phase(state, user_query, client):
          yield chunk

  async def _planning_phase(query: str, rejection_context: str, client: OpenAI) -> str:   
      """规划阶段（独立函数）"""
      planner_prompt = _build_planner_prompt(query, rejection_context)
      resp = await client.chat.completions.create(...)
      plan = resp.choices[0].message.content

      # 验证
      validation_result = validate_plan(plan, rejection_context)
      if not validation_result.get("is_valid"):
          plan = validation_result.get("fixed_plan", plan)

      return plan

  async def _execution_loop(state: dict, client: OpenAI, tools: List, max_steps: int) ->  
  AsyncIterator[Chunk]:
      """执行循环（独立函数）"""
      for step in range(max_steps):
          # 调用LLM
          response = await _call_llm_with_tools(state, client, tools)

          # 处理工具调用
          if response.tool_calls:
              async for chunk in _handle_tool_calls(response.tool_calls, tools):
                  yield chunk

          # 检查是否完成
          if _is_task_complete(state):
              break

  async def _answer_generation_phase(state: dict, query: str, client: OpenAI) ->
  AsyncIterator[Chunk]:
      """答案生成阶段（独立函数）"""
      answer = await _extract_final_answer(state, query, client)
      confidence = calculate_answer_confidence(answer, state["search_history"])

      # 验证答案
      verification_result = await _verify_answer(query, answer, client)

      if "[REJECTED]" in verification_result:
          # 处理拒绝逻辑
          ...

      yield Chunk(type="text", content=answer, step_index=state["step_index"])

  效果：
  - ✅ 单个函数不超过100行
  - ✅ 逻辑清晰，职责明确
  - ✅ 易于测试和维护
  - ✅ 便于并行开发

  ---
  ✅ 优化方案8：使用Pydantic进行数据验证（中优先级）

  实现：
  # models/schemas.py
  from pydantic import BaseModel, Field, validator
  from typing import List, Optional

  class SearchResult(BaseModel):
      title: str
      url: str
      snippet: str
      rank: Optional[int] = None

      @validator("url")
      def validate_url(cls, v):
          if not v.startswith(("http://", "https://")):
              raise ValueError("Invalid URL")
          return v

  class AnswerCandidate(BaseModel):
      text: str
      confidence: float = Field(ge=0.0, le=1.0)
      sources: List[str] = []

  class ValidationResult(BaseModel):
      is_valid: bool
      issues: List[str] = []
      suggestions: List[str] = []
      fixed_plan: Optional[str] = None
      validation_skipped: bool = False

  使用：
  # 替换
  def extract_answer_from_search_results(search_results: list, query: str) -> dict:       
      ...

  # 为
  def extract_answer_from_search_results(
      search_results: List[SearchResult],
      query: str
  ) -> AnswerCandidate:
      ...
      return AnswerCandidate(
          text=answer,
          confidence=0.95,
          sources=[r.url for r in search_results]
      )

  效果：
  - ✅ 运行时数据验证
  - ✅ 自动生成API文档
  - ✅ IDE自动补全支持
  - ✅ 减少类型错误

  ---
  📊 优化优先级总结
  ┌────────┬─────────────────────┬────────────┬───────────────┬────────────┐
  │ 优先级 │       优化项        │ 预估工作量 │   影响范围    │    ROI     │
  ├────────┼─────────────────────┼────────────┼───────────────┼────────────┤
  │ 🔴 高  │ 1. 模块化重构       │ 3-5天      │ 全局          │ ⭐⭐⭐⭐⭐ │
  ├────────┼─────────────────────┼────────────┼───────────────┼────────────┤
  │ 🔴 高  │ 2. 统一配置管理     │ 1天        │ 全局          │ ⭐⭐⭐⭐⭐ │
  ├────────┼─────────────────────┼────────────┼───────────────┼────────────┤
  │ 🔴 高  │ 6. 统一日志系统     │ 1天        │ 全局          │ ⭐⭐⭐⭐   │
  ├────────┼─────────────────────┼────────────┼───────────────┼────────────┤
  │ 🔴 高  │ 7. 拆分agent_loop   │ 2-3天      │ agent_loop.py │ ⭐⭐⭐⭐⭐ │
  ├────────┼─────────────────────┼────────────┼───────────────┼────────────┤
  │ 🟡 中  │ 3. LLM客户端单例    │ 0.5天      │ LLM调用       │ ⭐⭐⭐⭐   │
  ├────────┼─────────────────────┼────────────┼───────────────┼────────────┤
  │ 🟡 中  │ 4. 改进MemoryStore  │ 1天        │ 记忆系统      │ ⭐⭐⭐     │
  ├────────┼─────────────────────┼────────────┼───────────────┼────────────┤
  │ 🟡 中  │ 5. StateStore持久化 │ 1天        │ 状态管理      │ ⭐⭐⭐     │
  ├────────┼─────────────────────┼────────────┼───────────────┼────────────┤
  │ 🟡 中  │ 8. Pydantic数据验证 │ 2天        │ 数据层        │ ⭐⭐⭐     │
  └────────┴─────────────────────┴────────────┴───────────────┴────────────┘
  ---
  🎯 立即可执行的快速优化

  以下是不影响现有功能的快速优化（1-2小时内完成）：

  ✅ 快速优化1：删除重复函数

  操作：
  # 在 agent_loop.py 中删除 extract_answer_from_search_results
  # 从 agent.py 导入即可
  from agent import extract_answer_from_search_results

  ✅ 快速优化2：提取API配置常量

  # agent_loop.py 顶部添加
  API_BASE_URL = "https://apis.iflow.cn/v1"
  DEFAULT_TIMEOUT = 60.0
  DEFAULT_MAX_RETRIES = 2
  DEFAULT_MODEL = "qwen3-max"

  # 所有使用处替换硬编码
  client = OpenAI(
      base_url=API_BASE_URL,
      api_key=os.getenv("IFLOW_API_KEY"),
      timeout=DEFAULT_TIMEOUT,
      max_retries=DEFAULT_MAX_RETRIES
  )

  ✅ 快速优化3：改进错误处理

  # 替换
  except Exception:
      pass

  # 为
  except Exception as e:
      logger.warning(f"Operation failed but continuing: {e}")

  ---
  🎬 建议的实施路线图

  阶段1：快速优化（1-2天）

  1. ✅ 删除重复函数
  2. ✅ 提取配置常量
  3. ✅ 改进错误处理
  4. ✅ 添加基础日志

  阶段2：核心重构（1周）

  1. ✅ 统一配置管理（Pydantic Settings）
  2. ✅ LLM客户端单例模式
  3. ✅ 统一日志系统
  4. ✅ 拆分agent_loop函数

  阶段3：架构优化（2周）

  1. ✅ 模块化重构（创建services/utils目录）
  2. ✅ StateStore持久化
  3. ✅ MemoryStore批量写入
  4. ✅ Pydantic数据验证

  ---
  💡 是否开始优化？

  建议先从哪个开始？

  我建议按以下顺序执行：

  1. 立即执行：删除重复函数 + 提取配置常量（30分钟）
  2. 今天完成：统一配置管理 + LLM客户端单例（2-3小时）
  3. 本周完成：统一日志系统 + 拆分agent_loop（2-3天）