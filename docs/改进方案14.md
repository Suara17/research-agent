ä¸€ã€æ ¸å¿ƒé—®é¢˜æç‚¼ï¼ˆé€šç”¨è§†è§’ï¼‰                                                      
                                                                                            é—®é¢˜1ï¼šç­”æ¡ˆåˆæˆå™¨çš„"å®å¯ä¸ç­”"ç­–ç•¥å¤±æ•ˆ                                                  â”€

  # Line 316-317: ç¬¬ä¸€æ¬¡å°è¯•
  [AgentEnhancer] ç­”æ¡ˆæ ¼å¼éªŒè¯å¤±è´¥: ç­”æ¡ˆåº”åŒ…å«4ä½å¹´ä»½ï¼Œä½†æœªæ‰¾åˆ°
  [AgentEnhancer] æœ€ç»ˆç­”æ¡ˆ: ç­”æ¡ˆåº”åŒ…å«4ä½å¹´ä»½ï¼Œä½†æœªæ‰¾åˆ° (ç½®ä¿¡åº¦: 0.59)

  # Line 575-577: ç¬¬äºŒæ¬¡å°è¯•
  [Verification] Rejected: The candidate answer provides '1987' but then contradicts      
  itself
  by mentioning '1986' in the reasoning, showing internal inconsistency.

  é€šç”¨é—®é¢˜ï¼š
  - ç½®ä¿¡åº¦0.59å·²è¶…è¿‡é˜ˆå€¼ï¼Œä½†å› æ ¼å¼é—®é¢˜è¢«æ‹’ç»
  - ç¬¬äºŒæ¬¡ç­”æ¡ˆå†…éƒ¨çŸ›ç›¾ï¼ˆ1987 vs 1986ï¼‰
  - æ ¹æœ¬åŸå› ï¼šç­”æ¡ˆåˆæˆå™¨æ²¡æœ‰"ä»è¯æ®ä¸­æ„é€ æœ€ä¼˜ç­”æ¡ˆ"çš„èƒ½åŠ›ï¼Œåªä¼š"åˆ¤æ–­è¯æ®æ˜¯å¦å……åˆ†"

  ç±»æ¯”åœºæ™¯ï¼š
  - é—®"è°å‘æ˜äº†ç”µè¯ï¼Ÿ" â†’ æ‰¾åˆ°"Bell 1876å¹´ç”³è¯·ä¸“åˆ©" â†’
  ç­”æ¡ˆå™¨è¯´"æ²¡æ‰¾åˆ°'å‘æ˜è€…'è¿™ä¸ªè¯ï¼Œè¯æ®ä¸è¶³"
  - é—®"æœ€é«˜çš„å±±æ˜¯å“ªåº§ï¼Ÿ" â†’ æ‰¾åˆ°"ç å³°æµ·æ‹”8848ç±³" â†’ ç­”æ¡ˆå™¨è¯´"æ²¡æ‰¾åˆ°'æœ€é«˜'çš„æ˜ç¡®å£°æ˜"        

  ---
  é—®é¢˜2ï¼šç½‘ç»œè¯·æ±‚çš„"ä¸æƒœä»£ä»·"é‡è¯•ç­–ç•¥

  # Line 231-268: nationalminingmuseum.org.uk ä¸‰æ¬¡SSLè¶…æ—¶
  2026-02-06 13:25:13 ERROR download error (SSL timeout)
  2026-02-06 13:25:35 WARNING Retrying (total=2)
  2026-02-06 13:25:53 WARNING Retrying (total=1)
  2026-02-06 13:26:13 WARNING Retrying (total=0)
  # å•æ¬¡å¤±è´¥è€—æ—¶ 60ç§’

  # è¿™ä¸ªURLåœ¨æ—¥å¿—ä¸­å¤±è´¥äº†6æ¬¡ï¼Œæµªè´¹çº¦6åˆ†é’Ÿ

  é€šç”¨é—®é¢˜ï¼š
  - å¯¹æ‰€æœ‰å¤±è´¥éƒ½ä¸€è§†åŒä»åœ°é‡è¯•3æ¬¡
  - æ²¡æœ‰åŒºåˆ†"æš‚æ—¶æ€§é”™è¯¯"ï¼ˆ503è¶…è½½ï¼‰å’Œ"ç»“æ„æ€§é”™è¯¯"ï¼ˆDNSå¤±è´¥ã€SSLæ¡æ‰‹å¤±è´¥ï¼‰
  - æ²¡æœ‰å¤±è´¥ä¼ æ’­æœºåˆ¶ï¼šç¬¬1æ¬¡SSLå¤±è´¥åï¼Œåº”è¯¥æ ‡è®°è¯¥åŸŸå"å½“å‰ä¸å¯è¾¾"

  ç±»æ¯”åœºæ™¯ï¼š
  - è®¿é—®æ”¿åºœç½‘ç«™403 Forbidden â†’ é‡è¯•3æ¬¡æ¯«æ— æ„ä¹‰ï¼ˆæƒé™é—®é¢˜ï¼‰
  - è®¿é—®å·²å…³é—­çš„æœåŠ¡ â†’ æ¯æ¬¡éƒ½ç­‰30ç§’è¶…æ—¶å†é‡è¯•

  ---
  é—®é¢˜3ï¼šåŠ¨æ€å®ä½“æå–çš„"è´ªå©ªç—‡"

  # Line 112-114
  [Monitoring] core_entities_extracted=['source', 'serper', 'results', 'title',
  'Can Argentinians live in the Falkland Islands?', 'summary', ...]
  [DynamicRetrieval] Detected new entities: ['Can Argentinians live in the Falkland       
  Islands?'...]
  [DynamicRetrieval] Injected 1 memory hits for 'Can Argentinians live in the Falkland    
  Islands?'

  é€šç”¨é—®é¢˜ï¼š
  - ä»æœç´¢ç»“æœçš„å…ƒæ•°æ®ï¼ˆsourceã€titleã€summaryæ ‡ç­¾ï¼‰ä¸­æå–å®ä½“
  - å®Œå…¨æ— å…³çš„æœç´¢ç»“æœï¼ˆç¦å…‹å…°ç¾¤å²›ï¼‰è¢«åŠ å…¥è®°å¿†
  - æ²¡æœ‰è¯­ä¹‰ç›¸å…³æ€§è¿‡æ»¤

  ç±»æ¯”åœºæ™¯ï¼š
  - é—®"Pythonä¹‹çˆ¶æ˜¯è°ï¼Ÿ" â†’ æœç´¢è¿”å›"Pythonè›‡çš„ç”Ÿç‰©åˆ†ç±»" â†’ æå–"è›‡"ã€"çˆ¬è¡ŒåŠ¨ç‰©"ä½œä¸ºå®ä½“    
  - é—®"èå£«æ¯”äºšå‡ºç”Ÿåœ°ï¼Ÿ" â†’ æœç´¢è¿”å›ç”µå½±ã€Šèç¿æƒ…å²ã€‹ â†’ æå–"ç”µå½±"ã€"å¥¥æ–¯å¡"

  ---
  é—®é¢˜4ï¼šåå°„æ£€æŸ¥ç‚¹çš„"æ‰“å¡å¼æ£€æŸ¥"

  # Line 157-159
  [System] Injected Reflection Checkpoint at step 10
  [Monitoring] Reflexion triggered: Reflexion: [System Enforced] ç›®å‰ä»…è¿›è¡Œäº† 6 æ¬¡æœç´¢ã€‚  
  å¯¹äºå·²æœ‰æŠŠæ¡çš„é¢˜ç›®ï¼Œè¯·ç¡®ä¿è‡³å°‘éªŒè¯ 10 æ¬¡ï¼›è‹¥ä»ä¸ç¡®å®šï¼Œè¯·ç»§ç»­å¯»æ‰¾è¯æ®ã€‚

  é€šç”¨é—®é¢˜ï¼š
  - åå°„åªæ˜¯"æ•°æ­¥æ•°"ï¼ˆæœç´¢äº†å‡ æ¬¡ï¼‰
  - æ²¡æœ‰æ£€æŸ¥çº¦æŸæ»¡è¶³åº¦ï¼ˆ5ä¸ªçº¦æŸæ¡ä»¶æ»¡è¶³äº†å‡ ä¸ªï¼Ÿï¼‰
  - æ²¡æœ‰æ£€æŸ¥æœç´¢é‡å¤åº¦ï¼ˆæ˜¯å¦åœ¨åŸåœ°æ‰“è½¬ï¼Ÿï¼‰
  - æ²¡æœ‰çº åèƒ½åŠ›ï¼ˆåªèƒ½è¯´"ç»§ç»­æœ"ï¼Œä¸èƒ½è¯´"æ¢ä¸ªæ–¹å‘æœ"ï¼‰

  ç±»æ¯”åœºæ™¯ï¼š
  - å­¦ç”Ÿåšé¢˜å¡ä½äº† â†’ è€å¸ˆè¯´"å†æƒ³10åˆ†é’Ÿ" â†’ å­¦ç”Ÿç»§ç»­ç”¨é”™è¯¯æ€è·¯æƒ³10åˆ†é’Ÿ
  - è€Œä¸æ˜¯"ä½ è¿™ä¸ªæ–¹å‘é”™äº†ï¼Œè¯•è¯•ä»å¦ä¸€ä¸ªè§’åº¦"

  ---
  äºŒã€é€šç”¨æ”¹è¿›æ–¹æ¡ˆ

  æ”¹è¿›1ï¼šè¯æ®åˆ°ç­”æ¡ˆçš„"æ„é€ èƒ½åŠ›"

  å½“å‰æ¶æ„é—®é¢˜

  # agent_loop.py çš„ç­”æ¡ˆåˆæˆé€»è¾‘ï¼ˆç®€åŒ–ï¼‰
  def synthesize_answer(memory, question):
      # 1. æŠŠæ‰€æœ‰è®°å¿†æ‹¼æˆä¸€ä¸ªprompt
      prompt = f"Question: {question}\nEvidence: {memory}\nAnswer:"

      # 2. è®©LLMç”Ÿæˆç­”æ¡ˆ
      answer = llm.generate(prompt)

      # 3. éªŒè¯æ ¼å¼
      if not validate_format(answer):
          return "[NEED_MORE_EVIDENCE]"

      return answer

  é—®é¢˜ï¼šLLMæ˜¯"è‡ªç”±å‘æŒ¥"ï¼Œæ²¡æœ‰ç»“æ„åŒ–çº¦æŸ

  æ”¹è¿›æ–¹æ¡ˆï¼šç»“æ„åŒ–ç­”æ¡ˆæ„é€ 

  class StructuredAnswerSynthesizer:
      """
      é€šç”¨ç­”æ¡ˆæ„é€ å™¨ï¼šä»è¯æ®å›¾ä¸­æ¨ç†ç­”æ¡ˆ
      é€‚ç”¨äºä»»ä½•å¤šè·³æ¨ç†é—®é¢˜
      """

      def synthesize(self, memory_store, question):
          """
          æ ¸å¿ƒæ€è·¯ï¼š
          1. æ„å»ºè¯æ®å›¾ï¼ˆå®ä½“-å…³ç³»-å®ä½“ï¼‰
          2. åœ¨å›¾ä¸Šæ‰§è¡Œæ¨ç†è·¯å¾„æœç´¢
          3. è¿”å›å¤šä¸ªå€™é€‰ç­”æ¡ˆ+æ¨ç†è·¯å¾„
          """

          # Step 1: æ„å»ºè¯æ®å›¾
          evidence_graph = self._build_evidence_graph(memory_store)

          # Step 2: è¯†åˆ«é—®é¢˜çš„ç›®æ ‡ç±»å‹
          answer_type = self._identify_answer_type(question)
          # ä¾‹å¦‚ï¼šå¹´ä»½ã€äººåã€åœ°ç‚¹ã€æ˜¯éé¢˜

          # Step 3: åœ¨å›¾ä¸Šæœç´¢æ¨ç†è·¯å¾„
          reasoning_paths = self._search_reasoning_paths(
              evidence_graph,
              answer_type
          )

          # Step 4: å¯¹æ¯æ¡è·¯å¾„è¯„åˆ†
          scored_paths = []
          for path in reasoning_paths:
              score = self._score_path(path, evidence_graph)
              scored_paths.append({
                  'answer': path.terminal_node.value,  # ç»ˆç‚¹èŠ‚ç‚¹çš„å€¼
                  'confidence': score,
                  'reasoning_chain': path.to_string(),
                  'supporting_evidence': path.get_evidence_snippets()
              })

          # Step 5: è¿”å›Top-Kç­”æ¡ˆ
          top_k = sorted(scored_paths, key=lambda x: x['confidence'], reverse=True)[:3]   

          return {
              'primary_answer': top_k[0]['answer'],
              'confidence': top_k[0]['confidence'],
              'reasoning': top_k[0]['reasoning_chain'],
              'alternatives': top_k[1:],
              'evidence_graph_size': len(evidence_graph.nodes)  # ç”¨äºè°ƒè¯•
          }

      def _build_evidence_graph(self, memory_store):
          """
          ä»è®°å¿†æ„å»ºæœ‰å‘å›¾
          èŠ‚ç‚¹ï¼šå®ä½“ï¼ˆäººã€åœ°ç‚¹ã€æ—¶é—´ã€æ¦‚å¿µï¼‰
          è¾¹ï¼šå…³ç³»ï¼ˆä½äºã€åˆ›å»ºäºã€èµ„åŠ©ã€åˆä½œï¼‰
          """
          graph = KnowledgeGraph()

          for memory in memory_store.get_all():
              # ç”¨NERæå–å®ä½“
              entities = self._extract_entities(memory.content)

              # ç”¨å…³ç³»æå–æ¨¡å‹æ‰¾å…³ç³»
              relations = self._extract_relations(memory.content, entities)

              for relation in relations:
                  graph.add_edge(
                      source=relation.subject,
                      target=relation.object,
                      relation_type=relation.predicate,
                      evidence=memory.content,
                      confidence=relation.confidence
                  )

          return graph

      def _search_reasoning_paths(self, graph, answer_type):
          """
          åœ¨å›¾ä¸Šæœç´¢ç¬¦åˆç­”æ¡ˆç±»å‹çš„æ¨ç†è·¯å¾„

          ä¾‹å¦‚ï¼šé—®é¢˜è¦æ±‚"å¹´ä»½"
          â†’ æœç´¢æ‰€æœ‰ä»¥"YEAR"ç±»å‹èŠ‚ç‚¹ç»“å°¾çš„è·¯å¾„
          â†’ è·¯å¾„ç¤ºä¾‹ï¼š[Museum A] --opened_in--> [1984] --certified_as--> [5-star]
          """
          # æ‰¾åˆ°æ‰€æœ‰ç¬¦åˆç­”æ¡ˆç±»å‹çš„å€™é€‰èŠ‚ç‚¹
          candidate_nodes = graph.get_nodes_by_type(answer_type)

          paths = []
          for node in candidate_nodes:
              # ä»æ¯ä¸ªå€™é€‰èŠ‚ç‚¹åå‘è¿½æº¯åˆ°é—®é¢˜ä¸­çš„å…³é”®å®ä½“
              path = graph.find_shortest_path(
                  target=node,
                  source_types=['ORGANIZATION', 'PLACE'],  # é—®é¢˜ä¸­çš„å®ä½“ç±»å‹
                  max_hops=5
              )
              if path:
                  paths.append(path)

          return paths

      def _score_path(self, path, graph):
          """
          å¯¹æ¨ç†è·¯å¾„è¯„åˆ†

          è¯„åˆ†ç»´åº¦ï¼š
          1. è·¯å¾„é•¿åº¦ï¼ˆè¶ŠçŸ­è¶Šå¥½ï¼Œå‡å°‘æ¨ç†è·³æ•°ï¼‰
          2. è¾¹çš„ç½®ä¿¡åº¦ï¼ˆå…³ç³»æŠ½å–çš„å¯ä¿¡åº¦ï¼‰
          3. èŠ‚ç‚¹çš„è¯æ®æ•°é‡ï¼ˆå¤šä¸ªæ¥æºéªŒè¯ï¼‰
          4. æ˜¯å¦æ»¡è¶³é—®é¢˜çº¦æŸï¼ˆé€šè¿‡çº¦æŸæ£€æŸ¥å™¨ï¼‰
          """
          # é•¿åº¦æƒ©ç½š
          length_score = 1.0 / (1 + 0.2 * len(path))

          # è¾¹æƒé‡ï¼ˆå…³ç³»ç½®ä¿¡åº¦çš„å¹³å‡ï¼‰
          edge_confidence = np.mean([e.confidence for e in path.edges])

          # èŠ‚ç‚¹éªŒè¯åº¦ï¼ˆæœ‰å¤šå°‘æ¡ç‹¬ç«‹è¯æ®æ”¯æŒè¿™ä¸ªèŠ‚ç‚¹ï¼‰
          node_verification = np.mean([
              len(graph.get_evidence_for_node(n)) for n in path.nodes
          ])

          # çº¦æŸæ»¡è¶³åº¦ï¼ˆå¦‚æœæœ‰çº¦æŸæ£€æŸ¥å™¨ï¼‰
          constraint_score = self._check_constraints(path) if hasattr(self,
  'constraint_checker') else 1.0

          # ç»¼åˆè¯„åˆ†
          total_score = (
              0.3 * length_score +
              0.4 * edge_confidence +
              0.2 * node_verification +
              0.1 * constraint_score
          )

          return total_score

      def _identify_answer_type(self, question):
          """
          ä»é—®é¢˜ä¸­è¯†åˆ«ç­”æ¡ˆç±»å‹

          é€šç”¨è§„åˆ™ï¼š
          - "when/which year/in what year" â†’ YEAR
          - "who" â†’ PERSON
          - "where" â†’ PLACE
          - "how many" â†’ NUMBER
          - "is/does/can" â†’ BOOLEAN
          """
          question_lower = question.lower()

          if any(x in question_lower for x in ['when', 'which year', 'what year']):       
              return 'YEAR'
          elif 'who' in question_lower:
              return 'PERSON'
          elif 'where' in question_lower:
              return 'PLACE'
          elif any(x in question_lower for x in ['how many', 'how much']):
              return 'NUMBER'
          elif any(question_lower.startswith(x) for x in ['is', 'does', 'can', 'did']):   
              return 'BOOLEAN'
          else:
              return 'ENTITY'  # é€šç”¨å®ä½“

  å…³é”®æ”¹è¿›ç‚¹

  1. ä¸å†ä¾èµ–LLMè‡ªç”±å‘æŒ¥ï¼š
    - å½“å‰ï¼šæŠŠæ‰€æœ‰è¯æ®æ‰”ç»™LLMï¼Œè®©å®ƒ"æƒ³å‡º"ç­”æ¡ˆ
    - æ”¹è¿›ï¼šåœ¨ç»“æ„åŒ–å›¾ä¸Šæ‰§è¡Œç¡®å®šæ€§æ¨ç†
  2. å¤šå€™é€‰ç­”æ¡ˆæœºåˆ¶ï¼š
    - å½“å‰ï¼šå•ä¸€ç­”æ¡ˆï¼Œè¦ä¹ˆå¯¹è¦ä¹ˆé”™
    - æ”¹è¿›ï¼šè¿”å›Top-3å€™é€‰+ç½®ä¿¡åº¦ï¼ŒéªŒè¯å™¨å¯ä»¥é€‰æœ€ä¼˜
  3. å¯è§£é‡Šæ€§ï¼š
    - å½“å‰ï¼šLLMç»™ä¸ªç­”æ¡ˆï¼Œä¸çŸ¥é“æ€ä¹ˆæ¥çš„
    - æ”¹è¿›ï¼šæ¯ä¸ªç­”æ¡ˆéƒ½æœ‰å®Œæ•´æ¨ç†é“¾ï¼ˆAâ†’Bâ†’Câ†’ç­”æ¡ˆï¼‰

  ---
  æ”¹è¿›2ï¼šæ™ºèƒ½ç½‘ç»œè¯·æ±‚ç­–ç•¥

  å½“å‰é—®é¢˜

  # æ‰€æœ‰å¤±è´¥éƒ½é‡è¯•3æ¬¡ï¼Œæ¯æ¬¡30ç§’è¶…æ—¶
  def fetch_url(url):
      for i in range(3):
          try:
              return requests.get(url, timeout=30)
          except Exception:
              if i < 2:
                  continue
              else:
                  return fallback_snippet(url)

  æ”¹è¿›æ–¹æ¡ˆï¼šå¤±è´¥åˆ†ç±»+è‡ªé€‚åº”é‡è¯•

  class IntelligentWebFetcher:
      """
      æ™ºèƒ½ç½‘ç»œè¯·æ±‚ï¼šæ ¹æ®å¤±è´¥ç±»å‹å†³å®šæ˜¯å¦é‡è¯•
      """

      # å¤±è´¥ç±»å‹åˆ†ç±»
      TRANSIENT_ERRORS = [
          'ConnectionError',      # æš‚æ—¶æ€§ç½‘ç»œé—®é¢˜
          'Timeout',              # è¶…æ—¶ï¼ˆå¯èƒ½æœåŠ¡å™¨å¿™ï¼‰
          '503',                  # æœåŠ¡ä¸å¯ç”¨
          '429'                   # è¯·æ±‚è¿‡å¤š
      ]

      PERMANENT_ERRORS = [
          'SSLError',             # SSLæ¡æ‰‹å¤±è´¥
          'DNSError',             # åŸŸåè§£æå¤±è´¥
          '404',                  # é¡µé¢ä¸å­˜åœ¨
          '403',                  # ç¦æ­¢è®¿é—®
          '401'                   # éœ€è¦è®¤è¯
      ]

      def __init__(self):
          self.domain_status = {}  # åŸŸåçŠ¶æ€ç¼“å­˜

      def fetch(self, url, max_retries=2):
          """
          æ™ºèƒ½è·å–URLå†…å®¹

          ç­–ç•¥ï¼š
          1. æ£€æŸ¥åŸŸåæ˜¯å¦å·²è¢«æ ‡è®°ä¸º"ä¸å¯è¾¾"
          2. æ ¹æ®é”™è¯¯ç±»å‹å†³å®šæ˜¯å¦é‡è¯•
          3. æ›´æ–°åŸŸåçŠ¶æ€ç¼“å­˜
          """
          domain = urlparse(url).netloc

          # æ£€æŸ¥åŸŸåé»‘åå•
          if self._is_domain_blocked(domain):
              logger.info(f"Domain {domain} is known to be unreachable, using snippet     
  fallback")
              return self._snippet_fallback(url)

          # å°è¯•è·å–
          for attempt in range(max_retries + 1):
              try:
                  timeout = self._adaptive_timeout(domain, attempt)
                  response = requests.get(url, timeout=timeout)

                  # æˆåŠŸ â†’ æ›´æ–°åŸŸåçŠ¶æ€ä¸º"å¯è¾¾"
                  self.domain_status[domain] = {
                      'status': 'reachable',
                      'last_success': time.time()
                  }

                  return response

              except Exception as e:
                  error_type = self._classify_error(e)

                  # æ°¸ä¹…æ€§é”™è¯¯ â†’ ç«‹å³æ”¾å¼ƒï¼Œæ ‡è®°åŸŸå
                  if error_type == 'PERMANENT':
                      logger.warning(f"Permanent error for {domain}: {e}, marking as      
  unreachable")
                      self._mark_domain_blocked(domain, str(e))
                      return self._snippet_fallback(url)

                  # æš‚æ—¶æ€§é”™è¯¯ â†’ åˆ¤æ–­æ˜¯å¦ç»§ç»­é‡è¯•
                  elif error_type == 'TRANSIENT':
                      if attempt < max_retries:
                          wait_time = self._exponential_backoff(attempt)
                          logger.info(f"Transient error, retrying after {wait_time}s")    
                          time.sleep(wait_time)
                          continue
                      else:
                          logger.warning(f"Max retries reached for {url}, using fallback")
                          return self._snippet_fallback(url)

                  # æœªçŸ¥é”™è¯¯ â†’ è°¨æ…é‡è¯•ä¸€æ¬¡
                  else:
                      if attempt == 0:
                          logger.warning(f"Unknown error: {e}, trying once more")
                          continue
                      else:
                          return self._snippet_fallback(url)

      def _classify_error(self, exception):
          """
          åˆ†ç±»é”™è¯¯ç±»å‹
          """
          error_str = str(exception)
          error_class = exception.__class__.__name__

          # æ£€æŸ¥æ˜¯å¦ä¸ºæ°¸ä¹…æ€§é”™è¯¯
          for permanent_keyword in self.PERMANENT_ERRORS:
              if permanent_keyword.lower() in error_str.lower() or \
                 permanent_keyword.lower() in error_class.lower():
                  return 'PERMANENT'

          # æ£€æŸ¥æ˜¯å¦ä¸ºæš‚æ—¶æ€§é”™è¯¯
          for transient_keyword in self.TRANSIENT_ERRORS:
              if transient_keyword.lower() in error_str.lower() or \
                 transient_keyword.lower() in error_class.lower():
                  return 'TRANSIENT'

          return 'UNKNOWN'

      def _is_domain_blocked(self, domain):
          """
          æ£€æŸ¥åŸŸåæ˜¯å¦åœ¨æœ€è¿‘è¢«æ ‡è®°ä¸ºä¸å¯è¾¾
          """
          if domain not in self.domain_status:
              return False

          status = self.domain_status[domain]

          if status['status'] == 'unreachable':
              # æ£€æŸ¥æ˜¯å¦è¿‡äº†"å†·å´æœŸ"ï¼ˆä¾‹å¦‚5åˆ†é’Ÿåå¯ä»¥é‡è¯•ï¼‰
              if time.time() - status['last_failure'] < 300:
                  return True

          return False

      def _mark_domain_blocked(self, domain, reason):
          """
          æ ‡è®°åŸŸåä¸ºä¸å¯è¾¾
          """
          self.domain_status[domain] = {
              'status': 'unreachable',
              'last_failure': time.time(),
              'reason': reason
          }

      def _adaptive_timeout(self, domain, attempt):
          """
          è‡ªé€‚åº”è¶…æ—¶ï¼š
          - é¦–æ¬¡å°è¯•ï¼šçŸ­è¶…æ—¶ï¼ˆ5ç§’ï¼‰å¿«é€Ÿå¤±è´¥
          - åç»­å°è¯•ï¼šé€æ¸å¢åŠ ï¼ˆ10ç§’ã€15ç§’ï¼‰
          """
          base_timeout = 5
          return base_timeout + attempt * 5

      def _exponential_backoff(self, attempt):
          """
          æŒ‡æ•°é€€é¿ï¼š2^attempt ç§’
          """
          return min(2 ** attempt, 10)  # æœ€å¤šç­‰10ç§’

      def _snippet_fallback(self, url):
          """
          å½“URLæ— æ³•è®¿é—®æ—¶ï¼Œä»æœç´¢å¼•æ“è·å–æ‘˜è¦
          """
          # å·²æœ‰å®ç°ï¼Œä¿æŒä¸å˜
          pass

  å…³é”®æ”¹è¿›ç‚¹

  1. å¿«é€Ÿå¤±è´¥ï¼š
    - SSLé”™è¯¯ã€DNSé”™è¯¯ â†’ ç«‹å³æ”¾å¼ƒï¼ˆä¸å†æµªè´¹60ç§’é‡è¯•ï¼‰
    - é¦–æ¬¡è¶…æ—¶ç”¨5ç§’ï¼ˆè€Œé30ç§’ï¼‰
  2. åŸŸåçº§ç¼“å­˜ï¼š
    - nationalminingmuseum.org.ukç¬¬ä¸€æ¬¡å¤±è´¥å â†’ åç»­5åˆ†é’Ÿå†…ç›´æ¥ç”¨snippet
    - é¿å…åŒä¸€ä¸ªååŸŸåè¢«è®¿é—®6æ¬¡
  3. æ—¶é—´èŠ‚çœä¼°ç®—ï¼š
    - å½“å‰ï¼š6æ¬¡å¤±è´¥ Ã— 60ç§’ = 6åˆ†é’Ÿæµªè´¹
    - æ”¹è¿›ï¼š1æ¬¡å¤±è´¥ Ã— 5ç§’ + 5æ¬¡è·³è¿‡ = 5ç§’æµªè´¹

  ---
  æ”¹è¿›3ï¼šè¯­ä¹‰ç›¸å…³æ€§è¿‡æ»¤çš„å®ä½“æå–

  å½“å‰é—®é¢˜

  # ä»æœç´¢ç»“æœçš„JSONç»“æ„ä¸­æå–æ‰€æœ‰å­—æ®µ
  entities = extract_entities({
      "source": "serper",
      "title": "Can Argentinians live in Falkland Islands?",
      "summary": "..."
  })
  # â†’ æå–ï¼š"Can Argentinians"ã€"Falkland Islands"

  æ”¹è¿›æ–¹æ¡ˆï¼šåŸºäºè¯­ä¹‰çš„è¿‡æ»¤

  class SemanticEntityFilter:
      """
      é€šç”¨å®ä½“è¿‡æ»¤å™¨ï¼šåªä¿ç•™ä¸é—®é¢˜è¯­ä¹‰ç›¸å…³çš„å®ä½“
      """

      def __init__(self, question, core_entities):
          """
          Args:
              question: åŸå§‹é—®é¢˜æ–‡æœ¬
              core_entities: ä»é—®é¢˜ä¸­æå–çš„æ ¸å¿ƒå®ä½“ï¼ˆå·²åœ¨åˆå§‹åŒ–æ—¶æå–ï¼‰
          """
          self.question = question
          self.core_entities = core_entities

          # é¢„è®¡ç®—é—®é¢˜çš„embeddingï¼ˆç”¨äºç›¸ä¼¼åº¦è®¡ç®—ï¼‰
          self.question_embedding = self._get_embedding(question)
          self.core_embeddings = [self._get_embedding(e) for e in core_entities]

      def filter_entities(self, raw_entities, threshold=0.3):
          """
          è¿‡æ»¤å®ä½“åˆ—è¡¨ï¼Œåªä¿ç•™ç›¸å…³çš„

          Args:
              raw_entities: ä»æ–‡æ¡£ä¸­æå–çš„åŸå§‹å®ä½“åˆ—è¡¨
              threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ï¼‰

          Returns:
              filtered_entities: è¿‡æ»¤åçš„å®ä½“åˆ—è¡¨
          """
          filtered = []

          for entity in raw_entities:
              # è·³è¿‡å…ƒæ•°æ®å…³é”®è¯
              if self._is_metadata_keyword(entity):
                  continue

              # è®¡ç®—ä¸é—®é¢˜çš„è¯­ä¹‰ç›¸å…³æ€§
              relevance = self._calculate_relevance(entity)

              if relevance >= threshold:
                  filtered.append({
                      'text': entity,
                      'relevance': relevance
                  })
              else:
                  logger.debug(f"Filtered out low-relevance entity: {entity}
  (score={relevance:.2f})")

          # æŒ‰ç›¸å…³æ€§æ’åº
          return sorted(filtered, key=lambda x: x['relevance'], reverse=True)

      def _is_metadata_keyword(self, entity):
          """
          åˆ¤æ–­æ˜¯å¦ä¸ºæœç´¢ç»“æœçš„å…ƒæ•°æ®å­—æ®µ
          """
          metadata_keywords = {
              'source', 'serper', 'serpapi', 'results', 'title',
              'summary', 'url', 'snippet', 'content', 'type'
          }
          return entity.lower() in metadata_keywords

      def _calculate_relevance(self, entity):
          """
          è®¡ç®—å®ä½“ä¸é—®é¢˜çš„ç›¸å…³æ€§

          æ–¹æ³•ï¼š
          1. å­—é¢åŒ¹é…ï¼šå®ä½“æ˜¯å¦å‡ºç°åœ¨é—®é¢˜ä¸­
          2. è¯­ä¹‰ç›¸ä¼¼åº¦ï¼šembedding cosine similarity
          3. å…±ç°ç»Ÿè®¡ï¼šåœ¨åŒä¸€æ–‡æ¡£ä¸­ä¸æ ¸å¿ƒå®ä½“å…±ç°
          """
          # æ–¹æ³•1ï¼šå­—é¢åŒ¹é…ï¼ˆæƒé‡0.4ï¼‰
          literal_score = 0.0
          if entity.lower() in self.question.lower():
              literal_score = 1.0
          elif any(entity.lower() in core.lower() or core.lower() in entity.lower()       
                  for core in self.core_entities):
              literal_score = 0.8

          # æ–¹æ³•2ï¼šè¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆæƒé‡0.6ï¼‰
          entity_embedding = self._get_embedding(entity)

          # ä¸é—®é¢˜çš„ç›¸ä¼¼åº¦
          question_sim = self._cosine_similarity(entity_embedding,
  self.question_embedding)

          # ä¸æ ¸å¿ƒå®ä½“çš„æœ€å¤§ç›¸ä¼¼åº¦
          core_sim = max([
              self._cosine_similarity(entity_embedding, core_emb)
              for core_emb in self.core_embeddings
          ]) if self.core_embeddings else 0.0

          semantic_score = max(question_sim, core_sim)

          # ç»¼åˆè¯„åˆ†
          total_score = 0.4 * literal_score + 0.6 * semantic_score

          return total_score

      def _get_embedding(self, text):
          """
          è·å–æ–‡æœ¬çš„embedding

          å®ç°é€‰é¡¹ï¼š
          1. æœ¬åœ°æ¨¡å‹ï¼šsentence-transformers (all-MiniLM-L6-v2)
          2. APIï¼šOpenAI embeddings
          3. ç®€åŒ–ç‰ˆï¼šTF-IDFå‘é‡ï¼ˆæ— éœ€é¢å¤–ä¾èµ–ï¼‰
          """
          # ç¤ºä¾‹ï¼šä½¿ç”¨ç®€åŒ–çš„TF-IDFï¼ˆå®é™…å¯æ›¿æ¢ä¸ºæ›´å¥½çš„æ¨¡å‹ï¼‰
          return self._simple_tfidf_embedding(text)

      def _cosine_similarity(self, vec1, vec2):
          """
          è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
          """
          dot_product = np.dot(vec1, vec2)
          norm1 = np.linalg.norm(vec1)
          norm2 = np.linalg.norm(vec2)

          if norm1 == 0 or norm2 == 0:
              return 0.0

          return dot_product / (norm1 * norm2)

      def _simple_tfidf_embedding(self, text):
          """
          ç®€åŒ–çš„TF-IDF embeddingï¼ˆä¸ä¾èµ–å¤–éƒ¨æ¨¡å‹ï¼‰
          å®é™…ä½¿ç”¨æ—¶å¯æ›¿æ¢ä¸º sentence-transformers
          """
          # è¿™é‡Œåªæ˜¯ç¤ºæ„ï¼Œå®é™…å®ç°éœ€è¦ç»´æŠ¤ä¸€ä¸ªå…¨å±€è¯æ±‡è¡¨
          words = text.lower().split()
          # è¿”å›ç®€åŒ–çš„è¯è¢‹å‘é‡
          return np.array([hash(w) % 100 for w in words[:10]]).astype(float)

  å®é™…é›†æˆ

  # åœ¨ agent_loop.py ä¸­æ›¿æ¢å®ä½“æå–é€»è¾‘

  # åˆå§‹åŒ–æ—¶
  question_entities = extract_entities(question)
  entity_filter = SemanticEntityFilter(question, question_entities)

  # æ¯æ¬¡æœç´¢å
  search_results = search_engine.search(query)
  raw_entities = extract_entities(search_results)

  # è¿‡æ»¤å®ä½“
  filtered_entities = entity_filter.filter_entities(raw_entities, threshold=0.3)

  # åªæŠŠç›¸å…³å®ä½“åŠ å…¥è®°å¿†
  for entity in filtered_entities:
      memory_store.add_entity(entity['text'], relevance=entity['relevance'])

  å…³é”®æ”¹è¿›ç‚¹

  1. å…ƒæ•°æ®è¿‡æ»¤ï¼š
    - è‡ªåŠ¨è·³è¿‡"source"ã€"serper"ã€"results"ç­‰JSONå­—æ®µ
  2. è¯­ä¹‰ç›¸å…³æ€§ï¼š
    - "Can Argentinians live in Falkland Islands?" ä¸é—®é¢˜ç›¸ä¼¼åº¦ < 0.3 â†’ è¿‡æ»¤æ‰
  3. å¯è°ƒé˜ˆå€¼ï¼š
    - ç®€å•é—®é¢˜ï¼ˆå•è·³ï¼‰ï¼šthreshold=0.5ï¼ˆä¸¥æ ¼è¿‡æ»¤ï¼‰
    - å¤æ‚é—®é¢˜ï¼ˆå¤šè·³ï¼‰ï¼šthreshold=0.3ï¼ˆå®½æ¾ä¿ç•™ï¼‰

  ---
  æ”¹è¿›4ï¼šç›®æ ‡é©±åŠ¨çš„åå°„æœºåˆ¶

  å½“å‰é—®é¢˜

  # æ¯10æ­¥è§¦å‘ä¸€æ¬¡ï¼Œåªæ£€æŸ¥æ­¥æ•°
  if step % 10 == 0:
      reflection = "å·²æœç´¢Xæ¬¡ï¼Œè¯·ç»§ç»­éªŒè¯"

  æ”¹è¿›æ–¹æ¡ˆï¼šçº¦æŸæ»¡è¶³åº¦è¿½è¸ª

  class GoalOrientedReflector:
      """
      ç›®æ ‡é©±åŠ¨çš„åå°„å™¨ï¼šè¿½è¸ªçº¦æŸæ»¡è¶³åº¦ï¼Œè€Œéæ­¥æ•°
      """

      def __init__(self, question):
          """
          ä»é—®é¢˜ä¸­æå–çº¦æŸæ¡ä»¶
          """
          self.constraints = self._extract_constraints(question)
          self.search_history = []
          self.constraint_progress = {c: 0.0 for c in self.constraints}

      def _extract_constraints(self, question):
          """
          ä»é—®é¢˜ä¸­æå–å¿…é¡»æ»¡è¶³çš„çº¦æŸ

          é€šç”¨ç­–ç•¥ï¼š
          1. è¯†åˆ«é™å®šè¯ï¼ˆfive-starã€funded byã€located inï¼‰
          2. è¯†åˆ«å…³ç³»ï¼ˆcollaborates withã€housesã€managesï¼‰
          3. è¯†åˆ«æ—¶é—´çº¦æŸï¼ˆfirstã€earliestã€beforeï¼‰
          """
          constraints = []

          # ç¤ºä¾‹çº¦æŸè¯†åˆ«è§„åˆ™ï¼ˆå¯æ‰©å±•ä¸ºLLMè°ƒç”¨ï¼‰
          text = question.lower()
          
          # è¯†åˆ«è®¤è¯/è¯„çº§çº¦æŸ
          if 'five-star' in text or 'accredited' in text:
              constraints.append({
                  'type': 'ACCREDITATION',
                  'description': 'Must be five-star accredited',
                  'keywords': ['five-star', 'accredited', 'rating']
              })

          # è¯†åˆ«èµ„åŠ©/ç®¡ç†çº¦æŸ
          if 'funded by' in text or 'managed by' in text:
              funder = self._extract_entity_after_keyword(text, ['funded by', 'managed    
  by'])
              constraints.append({
                  'type': 'FUNDING',
                  'description': f'Must be funded/managed by {funder}',
                  'keywords': [funder, 'funding', 'grant']
              })

          # è¯†åˆ«åœ°ç†ä½ç½®çº¦æŸ
          if 'not in' in text or 'outside' in text:
              constraints.append({
                  'type': 'LOCATION_EXCLUSION',
                  'description': 'Must NOT be in certain location',
                  'keywords': ['not in', 'outside', 'different city']
              })

          # è¯†åˆ«ä¸»é¢˜çº¦æŸ
          if 'about' in text or 'exhibition' in text:
              theme = self._extract_entity_after_keyword(text, ['about', 'exhibition on'])
              constraints.append({
                  'type': 'THEME',
                  'description': f'Must have theme/exhibition about {theme}',
                  'keywords': theme.split()
              })

          # è¯†åˆ«æ—¶é—´çº¦æŸ
          if any(w in text for w in ['first', 'earliest', 'when', 'which year']):
              constraints.append({
                  'type': 'TEMPORAL',
                  'description': 'Must identify specific year/date',
                  'keywords': ['year', 'date', 'opened', 'established']
              })

          return constraints

      def reflect(self, current_step, memory_store, evidence_graph=None):
          """
          åœ¨ç‰¹å®šæ—¶æœºè¿›è¡Œåå°„

          è§¦å‘æ¡ä»¶ï¼š
          1. è¾¾åˆ°æ­¥æ•°é˜ˆå€¼ï¼ˆ10ã€20ã€30æ­¥ï¼‰
          2. è¿ç»­3æ¬¡æœç´¢æ— æ–°å®ä½“
          3. ç”¨æˆ·è¯·æ±‚åæ€

          Returns:
              action: 'CONTINUE' / 'CHANGE_STRATEGY' / 'STOP_AND_ANSWER'
              message: åå°„ç»“æœè¯´æ˜
          """
          # æ›´æ–°çº¦æŸæ»¡è¶³åº¦
          self._update_constraint_progress(memory_store, evidence_graph)

          # è®¡ç®—æ•´ä½“è¿›åº¦
          overall_progress = np.mean(list(self.constraint_progress.values()))

          # æ£€æŸ¥æœç´¢é‡å¤åº¦
          recent_queries = [s['query'] for s in self.search_history[-5:]]
          duplicate_rate = self._calculate_duplicate_rate(recent_queries)

          # å†³ç­–é€»è¾‘
          decision = self._make_decision(
              step=current_step,
              progress=overall_progress,
              duplicate_rate=duplicate_rate
          )

          return decision

      def _update_constraint_progress(self, memory_store, evidence_graph):
          """
          è¯„ä¼°æ¯ä¸ªçº¦æŸçš„æ»¡è¶³ç¨‹åº¦ï¼ˆ0-1ï¼‰
          """
          for constraint in self.constraints:
              # åœ¨è®°å¿†ä¸­æœç´¢ä¸çº¦æŸç›¸å…³çš„è¯æ®
              relevant_evidence = memory_store.search_by_keywords(
                  constraint['keywords']
              )

              if evidence_graph:
                  # å¦‚æœæœ‰è¯æ®å›¾ï¼Œæ£€æŸ¥æ˜¯å¦å­˜åœ¨æ»¡è¶³çº¦æŸçš„æ¨ç†è·¯å¾„
                  path_score = evidence_graph.check_constraint_path(constraint)
              else:
                  # ç®€åŒ–ç‰ˆï¼šåŸºäºå…³é”®è¯åŒ¹é…æ•°é‡
                  path_score = min(len(relevant_evidence) / 3.0, 1.0)

              self.constraint_progress[constraint] = path_score

      def _make_decision(self, step, progress, duplicate_rate):
          """
          åŸºäºå½“å‰çŠ¶æ€å†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨
          """
          # æƒ…å†µ1ï¼šè¿›åº¦è‰¯å¥½ï¼Œç»§ç»­
          if progress >= 0.8 and step < 30:
              return {
                  'action': 'STOP_AND_ANSWER',
                  'message': f'çº¦æŸæ»¡è¶³åº¦{progress:.1%}ï¼Œå·²æœ‰è¶³å¤Ÿè¯æ®ï¼Œå»ºè®®åˆæˆç­”æ¡ˆ'      
              }

          # æƒ…å†µ2ï¼šé™·å…¥å¾ªç¯ï¼Œéœ€è¦æ¢ç­–ç•¥
          if duplicate_rate > 0.6 and step > 15:
              unsatisfied = [c for c, score in self.constraint_progress.items() if score <
   0.5]
              return {
                  'action': 'CHANGE_STRATEGY',
                  'message':
  f'æœç´¢é‡å¤åº¦{duplicate_rate:.1%}ï¼Œæœªæ»¡è¶³çº¦æŸï¼š{unsatisfied}ï¼Œå»ºè®®æ›´æ¢å…³é”®è¯æˆ–æœç´¢å¼•æ“'  
              }

          # æƒ…å†µ3ï¼šæ­¥æ•°è¾ƒå°‘ï¼Œç»§ç»­æœç´¢
          if step < 15:
              return {
                  'action': 'CONTINUE',
                  'message': f'å½“å‰è¿›åº¦{progress:.1%}ï¼Œç»§ç»­æœç´¢'
              }

          # æƒ…å†µ4ï¼šæ­¥æ•°è¾ƒå¤šä½†è¿›åº¦ä¸ä½³ï¼Œéœ€è¦äººå·¥ä»‹å…¥
          if step > 30 and progress < 0.5:
              return {
                  'action': 'REQUEST_HUMAN_HELP',
                  'message':
  f'æœç´¢{step}æ­¥ï¼Œä½†çº¦æŸæ»¡è¶³åº¦ä»…{progress:.1%}ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´é—®é¢˜ç†è§£æˆ–æœç´¢ç­–ç•¥'
              }

          # é»˜è®¤ï¼šç»§ç»­
          return {
              'action': 'CONTINUE',
              'message': f'è¿›åº¦{progress:.1%}ï¼Œç»§ç»­æœç´¢æœªæ»¡è¶³çš„çº¦æŸ'
          }

      def _calculate_duplicate_rate(self, recent_queries):
          """
          è®¡ç®—æœ€è¿‘æœç´¢çš„é‡å¤åº¦

          æ–¹æ³•ï¼šè®¡ç®—æŸ¥è¯¢ä¹‹é—´çš„Jaccardç›¸ä¼¼åº¦
          """
          if len(recent_queries) < 2:
              return 0.0

          similarities = []
          for i in range(len(recent_queries) - 1):
              q1_words = set(recent_queries[i].lower().split())
              q2_words = set(recent_queries[i + 1].lower().split())

              jaccard = len(q1_words & q2_words) / len(q1_words | q2_words)
              similarities.append(jaccard)

          return np.mean(similarities)

  é›†æˆåˆ°agent_loop

  # åˆå§‹åŒ–
  reflector = GoalOrientedReflector(question)

  # ä¸»å¾ªç¯ä¸­
  for step in range(max_steps):
      # ... æ‰§è¡Œæœç´¢ã€æå–å®ä½“ç­‰ ...

      # å®šæœŸåå°„
      if step % 10 == 0 or step == max_steps - 5:
          decision = reflector.reflect(
              current_step=step,
              memory_store=memory,
              evidence_graph=evidence_graph  # å¦‚æœå®ç°äº†æ”¹è¿›1
          )

          if decision['action'] == 'STOP_AND_ANSWER':
              logger.info(f"[Reflection] {decision['message']}")
              break

          elif decision['action'] == 'CHANGE_STRATEGY':
              logger.warning(f"[Reflection] {decision['message']}")
              # è§¦å‘ç­–ç•¥è°ƒæ•´ï¼ˆä¾‹å¦‚ï¼šåˆ‡æ¢æœç´¢å¼•æ“ã€æ›´æ¢å…³é”®è¯ï¼‰
              search_strategy.adjust()

          elif decision['action'] == 'REQUEST_HUMAN_HELP':
              logger.error(f"[Reflection] {decision['message']}")
              # å¯ä»¥åœ¨è¿™é‡Œæš‚åœæˆ–é™çº§å¤„ç†

  å…³é”®æ”¹è¿›ç‚¹

  1. ä»"æ•°æ­¥æ•°"åˆ°"çœ‹è¿›åº¦"ï¼š
    - å½“å‰ï¼šæœç´¢10æ¬¡ â†’ "ç»§ç»­æœ"
    - æ”¹è¿›ï¼šçº¦æŸæ»¡è¶³80% â†’ "å¯ä»¥åˆæˆç­”æ¡ˆäº†"
  2. ä¸»åŠ¨çº åï¼š
    - æ£€æµ‹åˆ°é‡å¤æœç´¢ â†’ å»ºè®®"æ¢å…³é”®è¯"
    - è€Œéç»§ç»­åŸåœ°æ‰“è½¬
  3. æå‰ç»ˆæ­¢ï¼š
    - 15æ­¥å°±æ»¡è¶³æ‰€æœ‰çº¦æŸ â†’ ä¸å¿…ç­‰åˆ°40æ­¥

  ---
  ä¸‰ã€å®æ–½ä¼˜å…ˆçº§å’Œæ—¶é—´ä¼°ç®—

  ä¼˜å…ˆçº§çŸ©é˜µ
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚         æ”¹è¿›          â”‚   å½±å“åŠ›   â”‚ å®æ–½éš¾åº¦ â”‚ ä¼˜å…ˆçº§ â”‚ é¢„è®¡å·¥æ—¶ â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ æ”¹è¿›1ï¼šç»“æ„åŒ–ç­”æ¡ˆæ„é€  â”‚ â­â­â­â­â­ â”‚ ğŸ”§ğŸ”§ğŸ”§ğŸ”§ â”‚ P0     â”‚ 2å¤©      â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ æ”¹è¿›2ï¼šæ™ºèƒ½ç½‘ç»œè¯·æ±‚   â”‚ â­â­â­â­   â”‚ ğŸ”§ğŸ”§     â”‚ P0     â”‚ 0.5å¤©    â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ æ”¹è¿›3ï¼šè¯­ä¹‰å®ä½“è¿‡æ»¤   â”‚ â­â­â­     â”‚ ğŸ”§ğŸ”§ğŸ”§   â”‚ P1     â”‚ 1å¤©      â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ æ”¹è¿›4ï¼šç›®æ ‡é©±åŠ¨åå°„   â”‚ â­â­â­â­   â”‚ ğŸ”§ğŸ”§ğŸ”§   â”‚ P1     â”‚ 1.5å¤©    â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  å®æ–½å»ºè®®

  ç¬¬1å‘¨ï¼šå¿«é€Ÿä¿®å¤ï¼ˆP0ï¼‰

  Day 1-2ï¼šå®æ–½æ”¹è¿›2ï¼ˆæ™ºèƒ½ç½‘ç»œè¯·æ±‚ï¼‰
  - æ”¶ç›Šï¼šèŠ‚çœ6åˆ†é’Ÿ/é¢˜ï¼ˆå¯¹äºæœ‰å¤±è´¥URLçš„é¢˜ç›®ï¼‰
  - é£é™©ï¼šä½ï¼ˆé€»è¾‘ç®€å•ï¼Œä¸å½±å“å…¶ä»–æ¨¡å—ï¼‰

  Day 3-5ï¼šå®æ–½æ”¹è¿›1ï¼ˆç»“æ„åŒ–ç­”æ¡ˆæ„é€ ï¼‰
  - æ”¶ç›Šï¼šè§£å†³"æœ‰è¯æ®ä½†ç­”ä¸å‡º"çš„æ ¸å¿ƒé—®é¢˜
  - é£é™©ï¼šä¸­ï¼ˆéœ€è¦ä¿®æ”¹ç­”æ¡ˆåˆæˆé€»è¾‘ï¼Œéœ€å……åˆ†æµ‹è¯•ï¼‰

  ç¬¬2å‘¨ï¼šæ·±åº¦ä¼˜åŒ–ï¼ˆP1ï¼‰

  Day 6-7ï¼šå®æ–½æ”¹è¿›3ï¼ˆè¯­ä¹‰å®ä½“è¿‡æ»¤ï¼‰
  - æ”¶ç›Šï¼šå‡å°‘è®°å¿†å™ªéŸ³ï¼Œæå‡æœç´¢ç²¾å‡†åº¦

  Day 8-9ï¼šå®æ–½æ”¹è¿›4ï¼ˆç›®æ ‡é©±åŠ¨åå°„ï¼‰
  - æ”¶ç›Šï¼šå‡å°‘æ— æ•ˆæœç´¢ï¼Œæå‰ç»ˆæ­¢

  ---
  å››ã€éªŒè¯æ–¹æ¡ˆ

  å›å½’æµ‹è¯•ï¼šåœ¨ç¬¬55é¢˜ä¸Šçš„é¢„æœŸè¡¨ç°
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”
  â”‚     æŒ‡æ ‡     â”‚      å½“å‰      â”‚ æ”¹è¿›å  â”‚ æå‡ â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
  â”‚ æ€»è€—æ—¶       â”‚ 13åˆ†é’Ÿ         â”‚ ~6åˆ†é’Ÿ  â”‚ -54% â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
  â”‚ æœç´¢æ­¥æ•°     â”‚ 40æ­¥ï¼ˆè¾¾ä¸Šé™ï¼‰ â”‚ ~20æ­¥   â”‚ -50% â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
  â”‚ ç½‘ç»œå¤±è´¥è€—æ—¶ â”‚ 6åˆ†é’Ÿ          â”‚ 10ç§’    â”‚ -97% â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
  â”‚ ç­”æ¡ˆçŠ¶æ€     â”‚ 2æ¬¡æ‹’ç»        â”‚ 1æ¬¡é€šè¿‡ â”‚ âœ…   â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
  â”‚ å™ªéŸ³å®ä½“     â”‚ ç¦å…‹å…°ç¾¤å²›ç­‰   â”‚ æ—       â”‚ âœ…   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜
  æ³›åŒ–æµ‹è¯•ï¼šåœ¨å…¶ä»–å¤æ‚é¢˜ç›®ä¸Š

  é€‰æ‹©3ç±»å…¸å‹å¤šè·³é—®é¢˜ï¼š
  1. æ—¶é—´æ¨ç†é¢˜ï¼š"Xäº‹ä»¶å‘ç”Ÿåœ¨Yäº‹ä»¶ä¹‹å‰è¿˜æ˜¯ä¹‹åï¼Ÿ"
  2. å…³ç³»é“¾é¢˜ï¼š"Açš„åˆ›å§‹äººçš„æ¯æ ¡åœ¨å“ªä¸ªåŸå¸‚ï¼Ÿ"
  3. å¤šçº¦æŸé¢˜ï¼š"åŒæ—¶æ»¡è¶³æ¡ä»¶1ã€2ã€3çš„å®ä½“æ˜¯ä»€ä¹ˆï¼Ÿ"

  é¢„æœŸæ”¹è¿›ï¼š
  - ç­”æ¡ˆåˆæˆæˆåŠŸç‡ï¼š60% â†’ 85%
  - å¹³å‡æœç´¢æ­¥æ•°ï¼š35æ­¥ â†’ 20æ­¥
  - å¹³å‡è€—æ—¶ï¼š10åˆ†é’Ÿ â†’ 5åˆ†é’Ÿ

  ---
  äº”ã€æ€»ç»“

  æ ¸å¿ƒæ”¹è¿›ç†å¿µ

  1. ä»"æœç´¢ä¸ºä¸­å¿ƒ"åˆ°"ç›®æ ‡ä¸ºä¸­å¿ƒ"ï¼š
    - ä¸æ˜¯"æœäº†å¤šå°‘æ¬¡"ï¼Œè€Œæ˜¯"çº¦æŸæ»¡è¶³äº†å‡ ä¸ª"
  2. ä»"LLMè‡ªç”±å‘æŒ¥"åˆ°"ç»“æ„åŒ–æ¨ç†"ï¼š
    - ä¸æ˜¯æŠŠæ‰€æœ‰è¯æ®æ‰”ç»™LLMï¼Œè€Œæ˜¯åœ¨å›¾ä¸Šæ‰§è¡Œç¡®å®šæ€§æ¨ç†
  3. ä»"ä¸æƒœä»£ä»·"åˆ°"å¿«é€Ÿå¤±è´¥"ï¼š
    - SSLé”™è¯¯ç«‹å³æ”¾å¼ƒï¼Œä¸æµªè´¹60ç§’é‡è¯•
  4. ä»"å…¨ç›˜æ¥æ”¶"åˆ°"è¯­ä¹‰è¿‡æ»¤"ï¼š
    - ç¦å…‹å…°ç¾¤å²›è¿™ç§æ— å…³å®ä½“ç›´æ¥è¿‡æ»¤

  å¯æµ‹é‡çš„æˆåŠŸæŒ‡æ ‡

  - âœ… ç­”æ¡ˆåˆæˆæˆåŠŸç‡ +25%
  - âœ… å¹³å‡æœç´¢æ­¥æ•° -50%
  - âœ… ç½‘ç»œå¤±è´¥è€—æ—¶ -90%
  - âœ… è®°å¿†å™ªéŸ³å®ä½“ -80%

  ---
  éœ€è¦æˆ‘å¼€å§‹å®æ–½å—ï¼Ÿå»ºè®®å…ˆåšæ”¹è¿›2ï¼ˆæ™ºèƒ½ç½‘ç»œè¯·æ±‚ï¼‰ï¼Œå› ä¸ºï¼š
  1. å®æ–½ç®€å•ï¼ˆåŠå¤©å®Œæˆï¼‰
  2. æ”¶ç›Šæ˜æ˜¾ï¼ˆç«‹å³èŠ‚çœæ—¶é—´ï¼‰
  3. é£é™©æœ€ä½ï¼ˆç‹¬ç«‹æ¨¡å—ï¼Œä¸å½±å“å…¶ä»–é€»è¾‘ï¼‰