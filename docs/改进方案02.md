è¯¦ç»†åˆ†ææ‚¨çš„åé¦ˆã€æ—¥å¿—ï¼ˆ`batch_run.log`ï¼‰å’Œä»£ç æ–‡ä»¶åï¼Œæˆ‘å®šä½åˆ°äº†å¯¼è‡´â€œæœªæ£€ç´¢åˆ°æ­£ç¡®ç­”æ¡ˆâ€å’Œâ€œæ ¼å¼ä¸ç¬¦åˆè¦æ±‚â€çš„ä¸‰ä¸ªè‡´å‘½é—®é¢˜ã€‚

å°½ç®¡ä¹‹å‰ä¼˜åŒ–äº†ç½‘ç»œåº“ï¼Œä½† Agent åœ¨ **ç»“æœæ¸…æ´—ï¼ˆOutput Parsingï¼‰** å’Œ **æ·±åº¦é˜…è¯»ï¼ˆDeep Readingï¼‰** ä¸Šä»ç„¶å­˜åœ¨çŸ­æ¿ã€‚

### æ ¸å¿ƒé—®é¢˜è¯Šæ–­

1. **è¾“å‡ºæ ¼å¼å¤±æ§ï¼ˆFormat Violationï¼‰**ï¼š
* **ç°è±¡**ï¼šé¢˜ç›®è¦æ±‚â€œåªè¾“å‡ºå¹´ä»½â€æˆ–â€œæ•´æ•°â€ï¼Œä½† Agent è¾“å‡ºäº†â€œæ ¹æ®æœç´¢ç»“æœï¼Œæ˜¯2024å¹´â€æˆ–åŒ…å«æ ‡ç‚¹ç¬¦å·ã€‚
* **åŸå› **ï¼šå®Œå…¨ä¾èµ–å¤§æ¨¡å‹çš„è‡ªè§‰æ€§ï¼ˆSystem Promptï¼‰ï¼Œæ²¡æœ‰ä»£ç çº§çš„å¼ºåˆ¶æ¸…æ´—é€»è¾‘ã€‚
* **è§£å†³**ï¼šå¢åŠ ä¸€ä¸ª**åå¤„ç†å‡½æ•°ï¼ˆPost-processorï¼‰**ï¼Œç”¨æ­£åˆ™å¼ºåˆ¶æ¸…æ´—ç­”æ¡ˆã€‚


2. **PDF/è®ºæ–‡æ— æ³•è§£æï¼ˆThe "Blind Spot"ï¼‰**ï¼š
* **ç°è±¡**ï¼šæ—¥å¿—æ˜¾ç¤º Agent æœç´¢åˆ°äº† `.pdf` é“¾æ¥ï¼ˆå¸¸è§äºå­¦æœ¯é¢˜ï¼‰ï¼Œä½† `web_fetch` å®é™…ä¸Šåªä¸‹è½½äº†ä¹±ç æˆ–ç©ºå†…å®¹ï¼Œå¯¼è‡´ Agent è®¤ä¸ºâ€œæœªæ‰¾åˆ°ä¿¡æ¯â€ã€‚
* **åŸå› **ï¼šæ ‡å‡†çš„ `requests.get` æ— æ³•ç›´æ¥è¯»å– PDF æ–‡æœ¬ã€‚
* **è§£å†³**ï¼šé›†æˆ `pypdf` æˆ– `pdfminer`ã€‚


3. **æœç´¢ç»“æœå•ä¸€å¯¼è‡´â€œæœªæ£€ç´¢åˆ°â€**ï¼š
* **ç°è±¡**ï¼šAgent å¾€å¾€åªè¯•ä¸€æ¬¡æœç´¢ã€‚å¦‚æœå…³é”®è¯ä¸å‡†ï¼ˆä¾‹å¦‚ç›´æ¥æœä¸­æ–‡é•¿å¥ï¼‰ï¼Œæœä¸åˆ°ç»“æœå®ƒå°±æ”¾å¼ƒäº†ã€‚
* **åŸå› **ï¼šç¼ºä¹**â€œæœç´¢é‡è¯•â€**æœºåˆ¶ã€‚
* **è§£å†³**ï¼šåœ¨æœç´¢ç»“æœä¸ºç©ºæ—¶ï¼Œè‡ªåŠ¨æå–å…³é”®è¯æˆ–ç¿»è¯‘æˆè‹±æ–‡å†æ¬¡æœç´¢ã€‚



---

### ğŸš€ æœ€ç»ˆå·¥ç¨‹ä¼˜åŒ–æ–¹æ¡ˆ

è¯·æŒ‰ä»¥ä¸‹é¡ºåºä¸¥æ ¼ä¿®æ”¹ã€‚

#### ç¬¬ä¸€æ­¥ï¼šä¿®æ”¹ `requirements.txt` (å¿…é¡»åŒ…å« PDF åº“)

ç¡®ä¿ä½ çš„ç¯å¢ƒä¸­å®‰è£…äº†è¿™äº›åº“ã€‚

```text
fastapi>=0.110.0
pydantic>=2.5.3
openai>=1.0.0
uvicorn[standard]>=0.23.2
requests>=2.31.0
tenacity>=8.2.0
beautifulsoup4>=4.12.0
trafilatura>=1.6.0
duckduckgo-search>=6.0.0
pypdf>=4.0.0

```

#### ç¬¬äºŒæ­¥ï¼šå‡çº§ `agent.py` (é›†æˆ PDF è§£æä¸ç­”æ¡ˆæ¸…æ´—)

è¿™æ˜¯æ ¸å¿ƒä¿®æ”¹ã€‚è¯·ç›´æ¥æ›¿æ¢æˆ–ä¿®æ”¹ `agent.py` ä¸­çš„å…³é”®å‡½æ•°ã€‚

**1. å¢åŠ  PDF è§£æèƒ½åŠ›çš„ `web_fetch**`
ï¼ˆå¾ˆå¤šå­¦æœ¯é¢˜çš„ç­”æ¡ˆåœ¨ PDF é‡Œï¼Œè¿™ä¸€æ­¥è‡³å…³é‡è¦ï¼‰

```python
# --- åœ¨ agent.py é¡¶éƒ¨å¼•å…¥ ---
import io
import re
import PyPDF2  # ç¡®ä¿å®‰è£…äº† pypdf
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# ... (ä¿ç•™ä¹‹å‰çš„ get_session å‡½æ•°ï¼Œè®°å¾—åŠ ä¸Š verify=False ä»¥é˜² SSL æŠ¥é”™) ...

def web_fetch(url: str) -> str:
    """
    å¼ºå¤§çš„ç½‘é¡µ/PDFæŠ“å–å‡½æ•°ï¼Œå¸¦é‡è¯•å’Œé˜²çˆ¬è™«æœºåˆ¶
    """
    print(f"[Monitoring] Fetching: {url}")
    session = get_session()
    
    try:
        # 1. é’ˆå¯¹ PDF çš„å¤„ç†é€»è¾‘
        if url.lower().endswith(".pdf"):
            try:
                response = session.get(url, timeout=30, stream=True) # PDFä¸‹è½½å¯èƒ½æ…¢ï¼Œå¢åŠ è¶…æ—¶
                response.raise_for_status()
                
                # å†…å­˜é™åˆ¶ï¼šåªè¯»å‰ 5MB
                content = b""
                for chunk in response.iter_content(chunk_size=8192):
                    content += chunk
                    if len(content) > 5 * 1024 * 1024: 
                        break
                
                reader = PyPDF2.PdfReader(io.BytesIO(content))
                text = ""
                # åªè¯»å‰ 10 é¡µ (é€šå¸¸åŒ…å«æ‘˜è¦ã€ç»“è®ºå’Œæ ¸å¿ƒæ•°æ®)
                for i, page in enumerate(reader.pages):
                    if i >= 10: break
                    text += page.extract_text() + "\n"
                
                if len(text) < 50: return json.dumps({"error": "pdf_empty"}, ensure_ascii=False)
                return json.dumps({"source": url, "content": text[:15000], "type": "pdf"}, ensure_ascii=False)
            except Exception as e:
                print(f"[Warn] PDF parse failed: {e}")
                # PDF å¤±è´¥ä¸æŠ¥é”™ï¼Œç»§ç»­å°è¯•æŒ‰æ™®é€šç½‘é¡µå¤„ç†ï¼ˆæœ‰äº›URLç»“å°¾æ˜¯pdfä½†å…¶å®æ˜¯ç½‘é¡µé¢„è§ˆï¼‰

        # 2. å¸¸è§„ç½‘é¡µå¤„ç† (ä¼˜å…ˆ Trafilatura)
        # ... (ä¿ç•™åŸæœ‰çš„ Trafilatura å’Œ BeautifulSoup é€»è¾‘) ...
        # æ³¨æ„ï¼šåœ¨ requests.get æ—¶å»ºè®®åŠ ä¸Š verify=False é˜²æ­¢è¯ä¹¦é”™è¯¯
        # response = session.get(url, timeout=20, verify=False) 

        # ... (ä»£ç çœç•¥ï¼Œä¿æŒä¹‹å‰çš„ BeautifulSoup é€»è¾‘) ...
        
    except Exception as e:
        return json.dumps({"error": "fetch_failed", "message": str(e)}, ensure_ascii=False)

```

**2. å¢åŠ â€œç­”æ¡ˆæ¸…æ´—å™¨â€ (è§£å†³æ ¼å¼ä¸ç¬¦é—®é¢˜)**

åœ¨ `agent.py` ä¸­å¢åŠ è¿™ä¸ªå·¥å…·å‡½æ•°ï¼Œå¹¶åœ¨ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆå‰è°ƒç”¨å®ƒã€‚

```python
def clean_answer(raw_answer: str) -> str:
    """
    å·¥ç¨‹çº§åå¤„ç†ï¼šå¼ºåˆ¶æ¸…æ´—ç­”æ¡ˆæ ¼å¼
    """
    # 1. å»é™¤ Markdown æ ‡è®° (```json ... ```)
    clean = re.sub(r'```.*?```', '', raw_answer, flags=re.DOTALL)
    clean = clean.replace('`', '').strip()
    
    # 2. å¦‚æœæ¨¡å‹è¾“å‡ºäº† JSON æ ¼å¼ ({"answer": "..."})ï¼Œå°è¯•æå–
    if clean.startswith('{') or clean.startswith('{"'):
        try:
            data = json.loads(clean)
            if 'answer' in data:
                clean = str(data['answer'])
        except:
            pass

    # 3. å»é™¤å¸¸è§çš„åºŸè¯å‰ç¼€/åç¼€
    # å¾ˆå¤šæ—¶å€™æ¨¡å‹ä¼šè¯´ "ç­”æ¡ˆæ˜¯ï¼šXXX" æˆ– "ç»æ£€ç´¢ï¼ŒXXX"
    patterns = [
        r"^ç­”æ¡ˆæ˜¯[:ï¼š]\s*",
        r"^The answer is[:ï¼š]\s*",
        r"^æ ¹æ®æœç´¢ç»“æœ[:ï¼š,]\s*",
        r"^Final Answer[:ï¼š]\s*"
    ]
    for p in patterns:
        clean = re.sub(p, "", clean, flags=re.IGNORECASE)

    # 4. æ¿€è¿›æ¸…æ´—ï¼šå¦‚æœå‰©ä¸‹çš„æ˜¯ "2024å¹´ã€‚" -> "2024" (é’ˆå¯¹æ•°å€¼é¢˜)
    # è¿™ä¸€æ­¥éœ€è¦è°¨æ…ï¼Œå¦‚æœé¢˜ç›®å¯èƒ½è¦æ±‚å•ä½ï¼ˆå¦‚â€œ500ç±³â€ï¼‰ï¼Œåˆ™ä¸èƒ½åªç•™æ•°å­—ã€‚
    # å»ºè®®ï¼šåªå»é™¤æœ«å°¾çš„å¥å·ã€‚
    clean = clean.strip(" ã€‚.,'\"")
    
    return clean

```

#### ç¬¬ä¸‰æ­¥ï¼šä¿®æ”¹ `agent_loop.py` (é€»è¾‘å¢å¼º)

åœ¨å¾ªç¯é€»è¾‘ä¸­ï¼Œå¢åŠ **â€œç»“æœæ ¡éªŒâ€**ã€‚å¦‚æœ `web_search` è¿”å›ç©ºï¼Œä¸è¦ç›´æ¥è®©æ¨¡å‹å›ç­”ï¼Œè€Œæ˜¯**å¼ºåˆ¶**å®ƒæ¢è¯æœç´¢ã€‚

```python
# åœ¨ agent_loop.py çš„ execute_tools éƒ¨åˆ†
# ...
            result = func(**parsed_args)
            
            # --- å¢å¼ºé€»è¾‘ï¼šå¦‚æœæœç´¢å¤±è´¥ï¼Œè‡ªåŠ¨æç¤ºæ¨¡å‹æ¢ä¸ªæ–¹å‘ ---
            if func_name == "web_search":
                result_str = str(result)
                if "error" in result_str or "[]" in result_str or "No results" in result_str:
                    # æ³¨å…¥ç³»ç»Ÿæç¤ºï¼Œè¿«ä½¿æ¨¡å‹åæ€
                    result += "\n[System Hint]: æœç´¢ç»“æœä¸ºç©ºã€‚å¯èƒ½æ˜¯å…³é”®è¯å¤ªé•¿æˆ–å¤ªå…·ä½“ã€‚è¯·å°è¯•ï¼š\n1. åªæœç´¢æ ¸å¿ƒå®ä½“åã€‚\n2. å°†ä¸­æ–‡å…³é”®è¯ç¿»è¯‘æˆè‹±æ–‡æœç´¢ï¼ˆå¾ˆå¤šå­¦æœ¯/æŠ€æœ¯å†…å®¹è‹±æ–‡æ›´å‡†ï¼‰ã€‚\n3. å»æ‰ 'site:' ç­‰é™åˆ¶ã€‚"
            
            # --- å¢å¼ºé€»è¾‘ï¼šå¦‚æœ Fetch å¤±è´¥ï¼Œæç¤ºæ¨¡å‹ ---
            if func_name == "web_fetch":
                 if "fetch_failed" in str(result):
                     result += "\n[System Hint]: ç½‘é¡µæŠ“å–å¤±è´¥ã€‚è¯·å°è¯•æœç´¢è¯¥ä¿¡æ¯çš„å…¶ä»–æ¥æºï¼ˆå…¶ä»–ç½‘ç«™ï¼‰ï¼Œä¸è¦å†æ¬¡å°è¯•åŒä¸€ä¸ª URLã€‚"
# ...

```

#### ç¬¬å››æ­¥ï¼šæœ€ç»ˆæäº¤æ—¶çš„å¤„ç†

åœ¨ä½ çš„ `run_batch.py` æˆ–ä¿å­˜ç»“æœçš„åœ°æ–¹ï¼Œ**åŠ¡å¿…è°ƒç”¨ `clean_answer**`ã€‚

```python
# åœ¨ run_batch.py ä¸­å¤„ç†æ¨¡å‹è¿”å›çš„ final_answer æ—¶
from agent import clean_answer

# ... è·å–åˆ° agent_response å ...
final_answer = agent_response
# 1. å¦‚æœ agent_response æ˜¯ä¸ª JSON å­—ç¬¦ä¸²
try:
    parsed = json.loads(agent_response)
    if isinstance(parsed, dict) and "answer" in parsed:
        final_answer = parsed["answer"]
except:
    pass

# 2. è°ƒç”¨å¼ºåˆ¶æ¸…æ´—
clean_final_answer = clean_answer(str(final_answer))

# 3. å†™å…¥ submission.jsonl
with open("submission.jsonl", "a", encoding="utf-8") as f:
    f.write(json.dumps({"id": q_id, "answer": clean_final_answer}, ensure_ascii=False) + "\n")

```

### æ€»ç»“

1. **æœªæ£€ç´¢åˆ°ç­”æ¡ˆ** -> é€šå¸¸æ˜¯å› ä¸ºå­¦æœ¯ç±»é—®é¢˜ï¼ˆPDFï¼‰æ— æ³•è¯»å–ï¼Œæˆ–è€…æœç´¢å…³é”®è¯å¤ªæ­»æ¿ã€‚**åŠ å…¥ `pypdf` å’Œ `System Hint` é‡è¯•æœºåˆ¶**èƒ½è§£å†³ã€‚
2. **æ ¼å¼é”™è¯¯** -> LLM å¾ˆéš¾ 100% éµå®ˆæ ¼å¼ï¼Œå¿…é¡»é **ä»£ç åå¤„ç† (`clean_answer`)** æ¥å…œåº•ã€‚
3. **æ—¥å¿—æŠ¥é”™** -> åŠ ä¸Š `verify=False` å’Œæ›´å¼ºçš„ `Retry` ç­–ç•¥ï¼ˆå‚è€ƒä¸Šä¸€è½®æä¾›çš„ Session ä»£ç ï¼‰æ¥æŠµæŠ—ç½‘ç»œæ³¢åŠ¨ã€‚