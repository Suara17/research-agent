你好！在之前的规划器（Planner）优化中，我们解决了 Agent “怎么思考”的问题。现在针对“怎么记住”的问题，我仔细分析了 `agent_loop.py` 中的 `MemoryStore` 实现，发现它目前的实现比较初级，主要存在以下 **3 个瓶颈**，特别是针对中文和长上下文的场景：

### 🧠 现有记忆系统诊断

1. **分词粗糙 (Critical for Chinese)**：
* **现状**：`_tokenize` 方法使用正则 `re.findall(r"[\u4e00-\u9fff]+", s)` 配合 bigram/trigram 切分。
* **问题**：这种“机械切分”无法理解语义。例如“复旦大学”会被切成“复旦”、“大学”、“复旦大学”，但无法关联到“高校”。对于专业术语（如“大语言模型”），这种切分会导致检索噪声极大。


2. **检索单一 (BM25 Only)**：
* **现状**：仅使用基于词频的 BM25 算法。
* **问题**：**语义缺失**。如果用户问“苹果公司创始人”，记忆里存的是“Steve Jobs”，BM25 匹配不到，因为没有重合词。需要 **向量检索 (Vector Search)** 来补充语义关联。


3. **信息“堆砌”而非“消化”**：
* **现状**：`add_long` 直接把原始文本（如大段的 HTML 文本或工具返回结果）存入。
* **问题**：这叫“日志”不叫“记忆”。随着对话进行，大量无用噪声（网页导航、广告文本）被存入，检索出来的也往往是垃圾片段。



---

### 🚀 记忆优化方案

针对研究型 Agent，我建议实施 **“混合检索 + 语义分块”** 的优化方案。

#### 方案一：引入中文分词组件 (Fix Tokenizer)

这是成本最低、收益最高的改动。引入 `jieba` 库来替代简陋的正则分词。

**操作步骤**：

1. 在 `requirements.txt` 中添加 `jieba`。
2. 修改 `agent_loop.py` 中的 `MemoryStore._tokenize` 方法。

**代码修改 (`agent_loop.py`)**：

```python
    def _tokenize(self, text: str) -> List[str]:
        try:
            import jieba
            # 启用搜索引擎模式，提高召回率
            # jieba.cut_for_search 会把“复旦大学”切分为“复旦、大学、复旦大学”
            segs = list(jieba.cut_for_search(text.lower()))
            
            # 过滤停用词 (保留原有逻辑并增强)
            stop_words = {"的","了","在","是","我","有","和","与","及","等","为","不","也","这","那","你","他","她","它","其","并","对","以","the","and","or","is","are"}
            toks = [w for w in segs if w.strip() and w not in stop_words]
            
            return toks
        except ImportError:
            # 降级回原有的正则逻辑
            # ... (保留你原有的代码作为 fallback) ...
            return []

```

#### 方案二：实现混合检索 (Hybrid Search)

针对 Agent 经常需要“模糊回忆”的场景，引入 OpenAI Embeddings 进行向量检索，与现有的 BM25 结合。

**修改 `agent_loop.py` 的 `MemoryStore` 类：**

1. **增加向量存储结构**：
```python
def __init__(self, max_short: int = 64):
    # ... 原有初始化 ...
    self.vectors = {} # {doc_id: [float]}

```


2. **实现 Embedding 获取函数** (复用现有的 client)：
```python
def _get_embedding(self, text: str):
    try:
        client = OpenAI(
            base_url="https://apis.iflow.cn/v1", # 使用你的配置
            api_key=os.getenv("IFLOW_API_KEY"),
        )
        resp = client.embeddings.create(
            model="text-embedding-v3-small", # 或 text-embedding-ada-002
            input=text[:8000] # 注意长度限制
        )
        return resp.data[0].embedding
    except:
        return None

```


3. **入库时生成向量** (`_index_doc` 或 `add_long`):
```python
def add_long(self, item: str) -> None:
    # ... 原有写入文件逻辑 ...

    doc_id = len(self.doc_texts)
    # [新增] 计算向量
    vec = self._get_embedding(item)
    if vec:
        self.vectors[doc_id] = vec

    self._index_doc(doc_id, item) # 保持原有的 BM25 索引

```


4. **升级搜索算法 (Cosine Sim + BM25)**:
```python
def search(self, query: str, top_k: int = 5) -> List[dict]:
    # 1. 获取 Query 向量
    q_vec = self._get_embedding(query)

    # 2. 获取 BM25 分数 (复用原有逻辑，稍作修改返回 dict)
    bm25_results = self._search_bm25(query, top_k=20) # 扩大候选集
    bm25_scores = {d['id']: d['score'] for d in bm25_results}

    # 3. 计算向量相似度
    vec_scores = {}
    if q_vec and self.vectors:
        import numpy as np # 需引入 numpy
        q_arr = np.array(q_vec)
        # 简单的遍历计算 (数据量小的时候很快)
        for doc_id, doc_vec in self.vectors.items():
            d_arr = np.array(doc_vec)
            cosine = np.dot(q_arr, d_arr) / (np.linalg.norm(q_arr) * np.linalg.norm(d_arr))
            vec_scores[doc_id] = cosine

    # 4. 加权融合 (RRF 或 线性加权)
    final_scores = []
    all_ids = set(bm25_scores.keys()) | set(vec_scores.keys())

    alpha = 0.3 # BM25 权重
    beta = 0.7  # Vector 权重 (语义通常更重要)

    for doc_id in all_ids:
        s1 = bm25_scores.get(doc_id, 0.0)
        # 归一化 s1 (假设 BM25 分数大概在 0-10 之间)
        s1_norm = min(s1 / 10.0, 1.0)

        s2 = vec_scores.get(doc_id, 0.0)

        final_score = alpha * s1_norm + beta * s2
        final_scores.append({"id": doc_id, "score": final_score})

    # 5. 排序返回
    final_scores.sort(key=lambda x: x['score'], reverse=True)
    return [{"text": self.doc_texts[item['id']], "score": item['score']} for item in final_scores[:top_k]]

```



#### 方案三：记忆分层与消化 (Memory Distillation)

Agent 最大的问题是记住了太多“垃圾”。比如 `web_fetch` 下来的 HTML 包含大量无关文本。

**优化逻辑**：
在 `execute_tools` 保存结果到 memory 之前，增加一个 **“事实提取 (Fact Extraction)”** 步骤。

**修改建议 (`agent_loop.py` -> `execute_tools`)**：

```python
# 原逻辑：
# memory.add_long(tool_result_content)

# 优化逻辑：
if func_name in ["web_fetch", "browse_page"]: # 只针对重内容工具
    # 使用 LLM 提炼关键事实，而不是存全文
    summary_prompt = f"请从以下内容中提炼出对回答问题有价值的核心事实（实体、时间、数据）：\n{tool_result_content[:4000]}"
    try:
        # 调用 LLM (复用 client)
        summary = client.chat.completions.create(...).choices[0].message.content
        # 存摘要
        memory.add_long(f"Fact Summary from {parsed_args.get('url')}: {summary}")
    except:
        memory.add_long(tool_result_content) # 失败降级
else:
    memory.add_long(tool_result_content)

```

### 🎯 推荐实施路径

考虑到比赛/开发效率，建议优先级如下：

1. **引入 `jieba**`：这是解决中文“搜索不到记忆”的刚需，且无需改动架构，只需改几行代码。
2. **事实提取 (Distillation)**：这能极大提高 context window 的利用率，避免上下文超长。在 `web_fetch` 后加一个简单的 LLM call 即可。
3. **混合检索**：如果你的数据集非常大（成千上万条记录），再考虑引入 Vector Search。如果只是单次任务的几十步操作，BM25 配合 `jieba` 已经足够好用。

### 核心代码整合示例 (`agent_loop.py`)

这是针对 **Step 1 (Jieba)** 的最小化修改，你可以直接替换原有的 `_tokenize`：

```python
    def _tokenize(self, text: str) -> List[str]:
        # 尝试导入 jieba，如果环境没有则回退到正则
        try:
            import jieba
        except ImportError:
            return self._tokenize_fallback(text)

        import unicodedata
        s = unicodedata.normalize("NFKC", str(text)).lower()
        
        # 中文停用词增强
        stop_cn = {"的","了","在","是","我","有","和","与","及","等","为","不","也","这","那","你","他","她","它","其","并","对","以","个","位","名","之","中","或"}
        
        # 使用精确模式 + 搜索引擎模式混合
        # cut_for_search 适合长句检索
        toks = []
        for word in jieba.cut_for_search(s):
            w = word.strip()
            if w and w not in stop_cn and len(w) > 0:
                toks.append(w)
                
        return toks

    def _tokenize_fallback(self, text: str) -> List[str]:
        # ... 这里放你原来那段正则代码 ...

```