通过详细分析你的 `batch_run.log` 和 `agent.py`，你目前遇到的核心瓶颈并非模型智商不够，而是**工程实现的健壮性（Robustness）不足**。

**主要问题诊断：**

1. **致命的网络连接中断 (`RemoteDisconnected`)**：
* 日志中大量出现 `RemoteDisconnected('Remote end closed connection without response')`，特别是在调用搜索接口（SerpApi/Google）时。
* **原因**：你使用了默认的 `requests` 配置或 `urllib`，这在高并发或网络波动（特别是连接海外搜索API）时极易断连。


2. **PDF 解析能力缺失**：
* 你的 `memory_store.jsonl` 中虽然存入了 PDF 的 URL（如 `sciengine.com...pdf`），但 `web_fetch` 并没有专门处理 PDF 的逻辑（只用了 `trafilatura` 和 `BeautifulSoup`）。很多学术类问题（如 Q0, Q1）的答案隐藏在论文 PDF 中，无法解析 PDF 等于“睁眼瞎”。


3. **搜索策略死循环**：
* 日志显示 Agent 经常陷入重复搜索，例如在 Q2 中反复搜索“Quadrantid meteor shower parent body...”，因为之前的搜索结果没有被有效读取或解析，导致 Agent 以为没搜到，于是反复尝试长难句搜索，导致超时。



---

### 🚀 优化方案：工程级重构 (复赛标准)

请按以下步骤修改代码。重点是**增强网络层**和**增加 PDF 阅读能力**。

#### 第一步：修改 `requirements.txt`

增加 PDF 处理库和更强的重试库。

```text
fastapi>=0.110.0
pydantic>=2.5.3
openai>=1.0.0
uvicorn[standard]>=0.23.2
requests>=2.31.0
tenacity>=8.2.0
beautifulsoup4>=4.12.0
trafilatura>=1.6.0
duckduckgo-search>=6.0.0
pypdf>=4.0.0  <-- 新增

```

#### 第二步：重写 `agent.py` 中的网络与工具函数

请直接替换 `agent.py` 中的以下部分。即使你之前有类似的，请用这个**增强版**。

**1. 增强型 Session (解决 RemoteDisconnected)**

```python
# --- 在 agent.py 顶部引入 ---
import io
import PyPDF2  # 需安装 pypdf
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# --- 替换 get_session 函数 ---
def get_session():
    session = requests.Session()
    # 关键优化：增加对 'RemoteDisconnected' 等错误的容忍度
    retry_strategy = Retry(
        total=5,  # 增加重试次数
        backoff_factor=1, # 指数避退：1s, 2s, 4s...
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=["HEAD", "GET", "OPTIONS", "POST"],
        raise_on_status=False
    )
    adapter = HTTPAdapter(max_retries=retry_strategy, pool_connections=10, pool_maxsize=10)
    session.mount("https://", adapter)
    session.mount("http://", adapter)
    
    # 伪装成真实浏览器 (解决部分网站 403)
    session.headers.update({
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.9,zh-CN;q=0.8",
        "Connection": "keep-alive", # 显式保持连接
        "Upgrade-Insecure-Requests": "1"
    })
    return session

```

**2. 支持 PDF 的 Web Fetch (解决学术题无解)**

比赛中大量答案在论文 PDF 里，这个函数必须升级。

```python
# --- 替换 web_fetch 函数 ---
def web_fetch(url: str, max_bytes: int = 200_000) -> str:
    """
    Fetch content reliably, supporting HTML and PDF.
    """
    print(f"[Monitoring] web_fetch called with url='{url}'")
    
    # 1. 针对 PDF 的特殊处理
    if url.lower().endswith(".pdf"):
        try:
            session = get_session()
            resp = session.get(url, timeout=20, stream=True)
            resp.raise_for_status()
            
            # 限制下载大小，防止内存爆炸
            content = b""
            for chunk in resp.iter_content(chunk_size=8192):
                content += chunk
                if len(content) > 5 * 1024 * 1024: # 限制 5MB
                    break
            
            reader = PyPDF2.PdfReader(io.BytesIO(content))
            text = ""
            # 只读前 5 页，通常包含摘要和结论
            for page in reader.pages[:5]: 
                text += page.extract_text() + "\n"
                
            return json.dumps({"source": url, "content": text[:15000], "type": "pdf"}, ensure_ascii=False)
        except Exception as e:
            print(f"[Monitoring] PDF fetch failed: {e}")
            # PDF 失败不退缩，继续尝试当做普通网页处理（有些URL结尾是pdf但实际是网页预览）

    # 2. 常规网页处理 (优先 Trafilatura)
    if _TRAFILATURA_AVAILABLE:
        try:
            downloaded = trafilatura.fetch_url(url)
            if downloaded:
                text = trafilatura.extract(downloaded, include_comments=False, include_tables=True)
                if text and len(text) > 100:
                    return json.dumps({"source": url, "content": text[:15000], "type": "html"}, ensure_ascii=False)
        except Exception as e:
            print(f"[Monitoring] Trafilatura failed: {e}")

    # 3. 兜底 Requests + BeautifulSoup
    try:
        session = get_session()
        resp = session.get(url, timeout=15)
        # 自动检测编码，解决中文乱码
        resp.encoding = resp.apparent_encoding 
        
        soup = BeautifulSoup(resp.text, "html.parser")
        # 强力清洗干扰元素
        for t in soup(["script", "style", "nav", "footer", "header", "noscript", "svg", "button"]):
            t.extract()
            
        text = soup.get_text(separator="\n")
        # 去除多余空行
        text = re.sub(r'\n\s*\n', '\n', text)
        
        return json.dumps({"source": url, "content": text[:15000], "type": "html_fallback"}, ensure_ascii=False)
    except Exception as e:
        return json.dumps({"error": "fetch_failed", "message": str(e)}, ensure_ascii=False)

```

**3. 更稳健的 Search (防止死循环和断连)**

```python
# --- 替换 web_search 函数 ---
def web_search(query: str, top_k: int = 5) -> str:
    """
    Perform a robust web search using multiple providers.
    """
    print(f"[Monitoring] web_search query='{query}'")
    
    # 1. 优先使用 Google Serper (速度最快，结构化最好)
    serper_key = os.getenv("SERPER_API_KEY")
    if serper_key:
        try:
            url = "https://google.serper.dev/search"
            # 增加 gl: cn 可能会限制国际内容，建议去掉或根据问题语言动态调整
            # 这里的 payload 去掉了 hl/gl 强制限制，让 Google 自动判断
            payload = json.dumps({"q": query, "num": top_k}) 
            headers = {'X-API-KEY': serper_key, 'Content-Type': 'application/json'}
            
            # 单独的 timeout 且不复用 session，避免连接池污染
            response = requests.post(url, headers=headers, data=payload, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                results = []
                # 提取 organic 结果
                for item in data.get("organic", []):
                    results.append({
                        "title": item.get("title"),
                        "snippet": item.get("snippet"),
                        "url": item.get("link")
                    })
                # 提取 knowledge graph (往往直接包含答案)
                if "knowledgeGraph" in data:
                    kg = data["knowledgeGraph"]
                    results.insert(0, {
                        "title": kg.get("title", "Knowledge Graph"),
                        "snippet": f"{kg.get('type', '')}: {kg.get('description', '')} {kg.get('attributes', '')}",
                        "url": kg.get("website", "")
                    })
                
                if results:
                    return json.dumps({"source": "serper", "results": results[:top_k]}, ensure_ascii=False)
        except Exception as e:
            print(f"[Monitoring] Serper error: {e}")

    # 2. 备用 DuckDuckGo (免费，但不稳定，增加重试)
    if _DDGS_AVAILABLE:
        try:
            results = []
            # 使用上下文管理器确保资源释放
            with DDGS() as ddgs:
                # 增加 timelimit 参数尝试避免风控
                ddgs_gen = ddgs.text(query, max_results=top_k) 
                for r in ddgs_gen:
                    results.append({
                        "title": r.get("title"),
                        "snippet": r.get("body"),
                        "url": r.get("href")
                    })
            if results:
                return json.dumps({"source": "ddgs", "results": results}, ensure_ascii=False)
        except Exception as e:
            print(f"[Monitoring] DDGS error: {e}")

    return json.dumps({"error": "search_failed", "message": "All providers failed"}, ensure_ascii=False)

```

#### 第三步：优化 Prompt (QueryRequest)

你的 `agent.py` 中的 `QueryRequest.to_messages` 里的 System Prompt 需要微调，增加**“强制引用验证”**和**“防止 URL 幻觉”**。

修改 `content` 部分：

```python
                    "content": (
                        "你是一个专业的 Research Agent。你的唯一目标是给出精准的事实性答案。\n\n"
                        "### 黄金法则\n"
                        "1. **必须验证**: 搜索结果中的摘要（Snippet）往往是错误的或过时的。**你必须使用 `web_fetch` 打开网页或PDF阅读全文**，才能确认答案。\n"
                        "2. **处理 PDF**: 许多学术、历史、法律问题的答案在 PDF 文件中。如果搜索结果包含 PDF 链接，**务必 fetch 它**。\n"
                        "3. **多步推理**: 如果问题复杂（例如“某人的妻子的哥哥导演的电影”），请拆分为简单的子问题一步步搜索。不要试图一次搜出所有关系。\n"
                        "4. **死循环检测**: 如果你连续两次搜索了相似的关键词，或者连续两次 fetch 失败，**立即改变策略**。尝试用英文搜索（即使问题是中文），或者搜索该实体的其他属性。\n"
                        "5. **最终输出**: 严格只输出答案文本。如果是人名，输出全名；如果是数字，输出纯数字。不要输出 '答案是...' 或 '经检索...'。\n"
                        "6. **日期与数字**: 对于“哪一年”、“多少钱”类问题，务必精确匹配，不要估算。\n\n"
                        "### 思考模式\n"
                        "Action -> Observation -> Reflection -> Action ... -> Final Answer"
                    ),

```

#### 第四步：修正 `agent_loop.py` 中的死循环逻辑

你的日志显示 Q2 跑了 400 多秒。这说明 loop 没有及时终止或反思。

在 `agent_loop.py` 的 `execute_tools` 函数中，增加**空结果惩罚**：

```python
# ... 在 execute_tools 函数内部 ...
            try:
                # ... (工具执行代码) ...
                result = func(**parsed_args)
                tool_result_content = str(result)
                
                # --- 新增：智能检测 ---
                if func_name == "web_search":
                    # 检查是否返回了空结果
                    if "No results" in tool_result_content or "[]" in tool_result_content:
                        tool_result_content += "\n[System Warning]: 搜索无结果。建议：\n1. 尝试将中文关键词翻译为英文搜索。\n2. 减少关键词数量，只搜核心实体。\n3. 去掉 site: 等限制条件。"
                # --------------------
# ...

```

### 为什么这些修改能解决你的问题？

1. **Q2 (流星雨题) 失败原因**：Agent 反复搜中文长难句。
* **解决**：System Prompt 提示它“尝试英文搜索”，且 `web_search` 的 warning 会强制它换词。


2. **Q0/Q1 (学术题) 潜在问题**：答案可能在 PDF 里。
* **解决**：新增的 PDF 解析能力能让 Agent “看见”论文内容。


3. **Timeout/Disconnect**：
* **解决**：`Retry` 策略和 `keep-alive` 头的加入，大大减少了因网络波动导致的工具调用失败，保证 Agent 能拿到数据进行推理。



请应用上述代码后，重新运行 `python run_batch.py`。你会发现日志中的红色 Error 会大幅减少，Agent 的推理成功率会显著提高。