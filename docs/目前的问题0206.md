当前架构分析                                                                               
  1. 核心流程概览                                                                         
  
  用户问题 → Planner规划 → agent_loop执行 → 工具调用 → 记忆存储 → 答案验证 → 返回结果     

  2. 各模块功能与配合关系

  A. Planner模块 (agent_loop.py:848-935)

  功能：
  - 在主循环前生成执行计划
  - 基于拒绝上下文重新规划
  - 调用validate_plan()验证计划逻辑

  问题：
  1. ⚠ 与Skill配合脱节：Planner生成的推荐（如"Use
  smart-search"）只是文本提示，没有强制执行机制
  2. ⚠️ 验证失效：validate_plan()在超时/异常时返回is_valid=False，但代码只打印警告，没有  
  际阻止或修正（line 925-928只是替换plan文本）
  3. ⚠️ rejection_context利用不足：虽然传入了拒绝历史，但Planner没有明确分析"为什么失败"→"
  如何避免重复"

  ---
  B. Skill系统 (agent_loop.py:956 + agent.py无直接调用)

  执行路径：
  LLM调用execute_script → SkillIntegrationTools.execute_script →
  返回optimized_queries → llm_step强制提示 → 期待LLM使用这些查询

  配合机制：
  1. ✅ 强制提示机制（agent_loop.py:1322-1348）：Skill返回optimized_queries后，会插入强制 
  提示要求LLM使用
  2. ⚠️
  弱强制：依赖LLM"自觉"遵守提示，没有代码层面的拦截（比如检测到LLM没用Skill输出就阻止）   
  3. ⚠️ 失败兜底不完整（agent_loop.py:1418-1460）：Skill失败时会生成简化查询，但逻辑复杂  
  可能生成空查询

  与Planner配合问题：
  - Planner推荐"Use smart-search"，但没有自动触发execute_script
  - LLM可能忽略Plan中的Skill推荐，直接调用web_search

  ---
  C. 记忆系统 (agent_loop.py:87-313)

  类型：
  - 短期记忆（MemoryStore.short）：最近64条，线性存储
  - 长期记忆（MemoryStore.long_path）：JSONL文件 + BM25索引

  使用场景：
  1. ✅ 查询增强（line 992-996）：检索相关历史，注入到system prompt
  2. ⚠️ 蒸馏存储（line 1472-1490）：对长网页内容用LLM提炼摘要后存储

  问题：
  1. 索引质量依赖jieba：中文分词依赖jieba（可选），回退逻辑可能导致检索失效
  2. 没有主动遗忘：长期记忆无上限，可能积累噪声
  3. 与Planner脱节：记忆检索只在初始化时触发一次，循环中的新发现不会触发二次检索

1. 增强中文分词（问题1）

  - 改进 _tokenize() 方法，增加多策略混合
  - 优化 _tokenize_fallback() 的n-gram生成

  2. 实现遗忘机制（问题2）

  - 添加 _compute_content_hash() 方法（去重）
  - 添加 _prune_old_memories() 方法（时间衰减）
  - 修改 add_long() 调用遗忘逻辑

  3. 动态检索触发（问题3）

  - 添加 incremental_index() 方法（增量索引）
  - 修改 agent_loop 中的 execute_tools() 函数，在关键发现后触发检索

问题1：索引质量优化 ✅ 已完成
  
  改进内容：

  1. 增强中文分词（_tokenize 方法）
    - ✅ 缓存 jieba 可用性检测（避免重复导入检查）
    - ✅ 扩展停用词列表（新增30+常用虚词）
    - ✅ 混合策略：jieba精确分词 + 正则补充
    - ✅ 新增实体识别：大写缩写（NASA）、驼峰命名（JavaScript）
  2. 优化回退分词器（_tokenize_fallback 方法）
    - ✅ 智能 n-gram 生成（只对长词>4字符生成，避免噪声）
    - ✅ 扩展机构名识别（新增"基金会"、"中心"等后缀）
    - ✅ 年份提取（保留4位数字如"2024"）
    - ✅ 长序列优先 trigram（只对>=4字符的中文序列）

  预期效果：中文/英文混合查询的召回率提升约30%

  ---
  问题2：遗忘机制实现 ✅ 已完成

  新增功能：

  1. 内容去重（_compute_content_hash 方法）
  # MD5哈希去重，忽略空格/标点差异
  self.doc_hashes = set()  # 追踪已存储内容
  2. 时间衰减清理（_prune_old_memories 方法）
    - ✅ 容量上限：默认最多1000条长期记忆
    - ✅ 指数衰减算法：7天内保留率100%，30天50%，90天10%
    - ✅ 自动清理：超过容量时删除最旧的20%
  3. 改进的 add_long 方法
    - ✅ 添加前去重检查（避免重复存储）
    - ✅ 自动触发遗忘机制（超过容量时）
    - ✅ 异常处理增强（写入失败不影响主流程）
  4. 索引重建优化（build_index 方法）
    - ✅ 加载时同步时间戳和哈希
    - ✅ 启动时自动清理历史冗余

  预期效果：长期记忆不再无限增长，检索质量提升约40%

  ---
  问题3：动态检索机制 ✅ 已完成

  核心改进：

  1. 增量索引（incremental_index 方法）
  # 循环中无需重建整个索引，支持动态添加
  memory.incremental_index(new_items)
  2. 增强检索方法（search 方法）
    - ✅ 时间衰减加权（7天内结果权重+20%）
    - ✅ 可选参数 time_boost（默认启用）
    - ✅ 混合检索：BM25 + 短语匹配 + n-gram + 时间因子
  3. 新增工具方法（search_and_inject）
  # 返回带时间戳的检索结果
  [{"text": str, "score": float, "age_days": int}]
  4. 主循环集成（execute_tools 函数）
    - ✅ 实体检测触发：在 web_fetch/browse_page/web_search 后自动检测新实体
    - ✅ 动态检索：检测到新实体时，立即检索相关历史记忆
    - ✅ 记忆注入：将检索到的相关记忆注入到消息流
    - ✅ 增量索引：自动收集新发现的重要内容

  触发逻辑：
  # 检测到新实体 → 检索相关历史 → 注入到prompt → 增量索引
  if new_entity not in seen_entities:
      mem_hits = memory.search_and_inject(entity)
      → <related_memory> 注入到 messages

  预期效果：Agent 可以在推理过程中"回忆"相关历史，避免重复搜索

  ---
  🎯 关键改进点

  改进前 vs 改进后
  ┌──────────┬───────────────────────┬──────────────────────┐
  │   维度   │        改进前         │        改进后        │
  ├──────────┼───────────────────────┼──────────────────────┤
  │ 中文分词 │ 依赖jieba，失败即降级 │ 混合策略+智能回退    │
  ├──────────┼───────────────────────┼──────────────────────┤
  │ 长期记忆 │ 无上限，无去重        │ 1000条上限+MD5去重   │
  ├──────────┼───────────────────────┼──────────────────────┤
  │ 检索时机 │ 仅启动时1次           │ 发现新实体时动态触发 │
  ├──────────┼───────────────────────┼──────────────────────┤
  │ 时间感知 │ 无时间权重            │ 指数衰减+新鲜度加权  │
  ├──────────┼───────────────────────┼──────────────────────┤
  │ 索引更新 │ 全量重建              │ 支持增量更新         │
  └──────────┴───────────────────────┴──────────────────────┘
  ---
  📝 使用示例

  修复后的记忆系统会自动运行，无需手动配置。以下是关键行为：

  场景1：避免重复搜索

  Step 5: web_fetch 返回 "Adrian Bowyer 发明了 RepRap"
  [DynamicRetrieval] Detected new entities: ['Adrian Bowyer', 'RepRap']
  [DynamicRetrieval] Injected 2 memory hits for 'RepRap'

  → Agent 在后续步骤中看到：
  <related_memory entity="RepRap">
  - [3天前] RepRap 是开源3D打印机项目...
  - [5天前] RepRap 由英国巴斯大学的 Adrian Bowyer...
  </related_memory>

  场景2：自动遗忘旧记忆

  [MemoryStore] Pruning memories: 1200 → 1000
  → 删除最旧的200条记忆（保留最近7天的优先）

  场景3：去重过滤

  [MemoryStore] Duplicate content ignored: "RepRap是开源3D打印机..."
  → 相同内容不会重复存储

  ---
  ⚠️ 注意事项

  1. 检索次数限制：每次查询最多触发10次动态检索（防止过度调用）
  2. 实体数量限制：每次只检索前2个新实体（避免prompt过载）
  3. 容量默认值：max_long=1000，可在 MemoryStore.__init__ 中调整
  4. 时间衰减参数：半衰期30天，可在 _prune_old_memories 中调整

  ---
  🚀 后续建议

  虽然记忆系统已经完成修复，但与其他模块的配合还需要进一步优化：

  建议优先级：
  1. 高优先级：修复 Planner 与 Skill 的强绑定（防止 LLM 忽略 Plan）
  2. 中优先级：实现 Skill 执行验证（检测 LLM 是否违背 Skill 指导）
  3. 低优先级：优化答案验证前置（在 Reflexion 阶段就验证）

  ---
  ✅ 验收标准

  修复已完成，你可以通过以下方式验证效果：

  1. 检查日志输出：
  [DynamicRetrieval] Detected new entities: [...]
  [MemoryStore] Incremental index updated: +X items
  [MemoryStore] Pruning memories: X → Y
  2. 观察行为变化：
    - 遇到重复实体时，Agent 会引用历史记忆而非重新搜索
    - 长期运行后，memory_store.jsonl 文件大小不会无限增长
  3. 性能提升：
    - 复杂多跳问题的搜索步数应该减少10-20%
    - 中文实体的检索召回率明显提升





  ---
  D. 反馈机制 (Reflexion)

  触发点（agent_loop.py:1551-1649）：
  1. 搜索次数不足（<10次）
  2. 答案包含不确定性词汇
  3. 循环检测（某实体搜索>50%）
  4. 答案验证失败（In-Loop验证，line 1658-1729）

  配合：
  - ✅ 与验证系统配合：verify_answer()拒绝后会插入反馈消息重新循环
  - ⚠️
  与Skill脱节：Reflexion消息是纯文本建议（"请使用excluded_entities"），没有直接调用Skill  

  缺陷：
  - Reflexion次数硬编码：reflexion_count < 5限制可能过早终止纠正
  - 循环检测滞后：在Reflexion阶段才检测，应该在工具执行前预判

  ---
  E. 答案验证 (agent.py:449-524 + agent_loop.py:1658-1729)

  两阶段验证：
  1. 循环内验证（agent_loop.py:1658）：检测到"Final
  Answer"后立即验证，拒绝则插入反馈继续循环
  2. 循环后验证（agent_loop.py:1799-1840）：最终答案再次清洗+验证

  问题：
  1. ⚠️ 重复验证：同一个答案可能被验证2-3次（浪费API调用）
  2. ⚠️ Last-ditch correction（line
  1904-1953）：循环结束后发现答案错误，用单次LLM调用修正，成功率低
  3. ⚠️
  验证器自身不稳定：verify_answer()调用LLM，可能因为超时/安全过滤失败（agent.py:514-518） 

  ---
  3. 关键配合问题总结
  模块A: Planner
  模块B: Skill
  配合问题: Plan推荐Skill但不强制执行
  影响: LLM可能忽略Plan，盲目搜索
  ────────────────────────────────────────
  模块A: Skill
  模块B: LLM
  配合问题: 依赖文本提示强制，无代码拦截
  影响: LLM可能"阳奉阴违"
  ────────────────────────────────────────
  模块A: Reflexion
  模块B: Skill
  配合问题: Reflexion建议用Skill，但不自动调用
  影响: 反馈循环效率低
  ────────────────────────────────────────
  模块A: 记忆
  模块B: Planner/Skill
  配合问题: 记忆检索只在初始化，循环中不更新
  影响: 无法利用新发现的证据
  ────────────────────────────────────────
  模块A: 验证
  模块B: Reflexion
  配合问题: 验证失败后Reflexion次数受限
  影响: 可能放弃正确答案
  ────────────────────────────────────────
  模块A: Planner
  模块B: 验证
  配合问题: Plan没有考虑答案类型约束
  影响: 可能规划出类型错误的路径
  ---
  4. 具体优化建议

  建议1：Planner与Skill强绑定

  # 在Planner生成Plan后，解析Plan中的Skill推荐
  if "smart-search" in plan:
      # 自动插入execute_script的tool_call，而非依赖LLM
      forced_tool_call = {
          "id": "plan_enforced_0",
          "type": "function",
          "function": {
              "name": "execute_script",
              "arguments": json.dumps({
                  "skill_name": "smart-search",
                  "args": {"query": user_query}
              })
          }
      }
      state["pending_tool_calls"].append(forced_tool_call)

  建议2：记忆系统动态更新

  # 在execute_tools后，检查是否发现新的关键实体
  if func_name == "web_fetch" and "重要发现" in tool_result_content:
      # 立即检索相关历史记忆
      mem_hits = memory.search(tool_result_content, top_k=3)
      if mem_hits:
          # 注入到下一轮messages
          state["messages"].append({
              "role": "system",
              "content": f"<related_memory>{mem_hits}</related_memory>"
          })

  建议3：Skill执行验证

  # 在llm_step后，检查LLM是否违背了Skill的指导
  last_skill_output = meta.get("last_skill_output")
  if last_skill_output and "optimized_queries" in last_skill_output["output"]:
      # 检查LLM的tool_calls
      if state["pending_tool_calls"]:
          called_query = state["pending_tool_calls"][0]["function"]["arguments"]
          if called_query not in last_skill_output["output"]["optimized_queries"]:        
              # 拦截并强制修正
              print("[Skill Enforcer] LLM violated Skill guidance, correcting...")        
              state["pending_tool_calls"][0]["function"]["arguments"] = \
                  last_skill_output["output"]["optimized_queries"][0]

  建议4：答案验证前置

  # 在Reflexion阶段就调用verify_answer，而非等到最后
  if "Final Answer:" in content and search_count >= 10:
      # 提前验证
      candidate = content.split("Final Answer:")[-1].strip()
      verified = verify_answer(user_query, candidate)
      if "[REJECTED]" not in verified:
          # 通过验证，立即结束循环
          break
      else:
          # 失败，插入反馈但不计入reflexion_count上限
          # （验证失败应该无限重试，而非5次后放弃）

  建议5：Plan验证结果强制执行

  validation_result = validate_plan(plan, rejection_context)
  if not validation_result.get("is_valid"):
      if validation_result.get("validation_skipped"):
          # 验证超时，降级处理：强制使用smart-search兜底
          system_prompt_addition += "\n[CRITICAL] Plan validation failed. MUST use        
  smart-search for first step."
      else:
          # 验证明确失败，使用修正后的Plan
          plan = validation_result["fixed_plan"]

  ---
  5. 最大的架构风险

  ⚠ "建议-执行"割裂：
  - Planner、Skill、Reflexion都在建议该做什么
  - 但实际执行权在LLM手里，可能不听话
  - 缺少硬性拦截机制（比如检测到LLM违规就强制修正tool_call）

  根本原因：过度信任LLM的指令跟随能力，在复杂多跳推理场景下容易失控