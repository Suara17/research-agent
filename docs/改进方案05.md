è¿™ä¸€ä»½ `question.jsonl` æ•°æ®é›†éå¸¸æœ‰æŒ‘æˆ˜æ€§ï¼Œå®ƒä¸æ˜¯ç®€å•çš„â€œæœç´¢å¼•æ“å…³é”®è¯åŒ¹é…â€ä»»åŠ¡ï¼Œè€Œæ˜¯å…¸å‹çš„**å¤šè·³æ¨ç†ï¼ˆMulti-hop Reasoningï¼‰å’ŒçŸ¥è¯†å›¾è°±å¼é—®ç­”ï¼ˆKnowledge Graph QAï¼‰**ã€‚

æˆ‘å°†åˆ†ä¸‰ä¸ªéƒ¨åˆ†ä¸ºä½ æä¾›ä¼˜åŒ–æ–¹æ¡ˆï¼š

1. **æ•°æ®é›†æ·±åº¦åˆ†æ**ï¼šåˆ†æé—®é¢˜ç‰¹ç‚¹ä¸åˆ†å¸ƒã€‚
2. **ç­–ç•¥ä¼˜åŒ–å»ºè®®**ï¼šé’ˆå¯¹æ­¤ç±»é—®é¢˜çš„æ ¸å¿ƒè§£é¢˜æ€è·¯ã€‚
3. **ä»£ç ä¸æ–‡æ¡£è½åœ°**ï¼šä¼˜åŒ–åçš„ `run.py` å’Œ `SKILL.md`ã€‚

---

### ç¬¬ä¸€éƒ¨åˆ†ï¼šæ•°æ®é›†æ·±åº¦åˆ†æ

é€šè¿‡åˆ†æè¿™ 100 ä¸ªé—®é¢˜ï¼Œå¯ä»¥å°†å…¶å½’çº³ä¸ºä»¥ä¸‹å‡ ç±»ï¼Œç›®å‰çš„è„šæœ¬åœ¨å¤„ç†è¿™äº›ç±»å‹æ—¶å­˜åœ¨å±€é™æ€§ï¼š

#### 1. æ ¸å¿ƒç‰¹ç‚¹ï¼šè°œè¯­å¼/æè¿°æ€§å®ä½“é“¾æ¥ (The "Riddle" Entity Linking) - **å æ¯”æœ€é«˜ (~50%)**

* **ç‰¹ç‚¹**ï¼šä¸ç›´æ¥è¯´åå­—ï¼Œè€Œæ˜¯é€šè¿‡ä¸€ç³»åˆ—å±æ€§æè¿°ä¸€ä¸ªäººã€å…¬å¸æˆ–ç‰©ä½“ã€‚
* **ä¾‹å­**ï¼šID 0 ("ä¸€ä½æ¬§æ´²å­¦è€…çš„æŸé¡¹å¼€æºç¡¬ä»¶é¡¹ç›®...")ï¼ŒID 5 ("A person who... was between 45 and 55...")ã€‚
* **ç—›ç‚¹**ï¼šç›®å‰çš„ `run.py` å¦‚æœç›´æ¥æŠŠè¿™ä¸€é•¿æ®µè¯ä½œä¸º `query` æ‰”ç»™æœç´¢å¼•æ“ï¼Œç»“æœé€šå¸¸æ˜¯ä¸€å †ä¹±ç æˆ–æ— å…³ä¿¡æ¯ã€‚æœç´¢å¼•æ“ä¸æ“…é•¿å¤„ç†â€œè¿™æ˜¯ä¸€ä¸ª...çš„äººâ€è¿™ç§é•¿å¥ã€‚
* **æ‰€éœ€ç­–ç•¥**ï¼š**å…³é”®è¯æå–ä¸æ‹†è§£**ã€‚å¿…é¡»ä»é•¿å¥ä¸­æå–å‡º `["æ¬§æ´²å­¦è€…", "å¼€æºç¡¬ä»¶", "å…ƒèƒè‡ªåŠ¨æœº", "å››è¾¹å½¢æ¡†æ¶"]` ç»„åˆæœç´¢ã€‚

#### 2. æ ¸å¿ƒç‰¹ç‚¹ï¼šæ—¶é—´é”šç‚¹ä¸äº¤å‰éªŒè¯ (Temporal Anchoring & Intersection) - **å æ¯” (~30%)**

* **ç‰¹ç‚¹**ï¼šé€šè¿‡ä¸€ä¸ªäº‹ä»¶ç¡®å®šæ—¶é—´ï¼Œå†ç”¨è¿™ä¸ªæ—¶é—´å»æ¨å¯¼å¦ä¸€ä¸ªäº‹ä»¶ã€‚
* **ä¾‹å­**ï¼šID 16 ("äº‹ä»¶å‘ç”Ÿçš„é‚£ä¸€å¹´ï¼Œä¸€æ¬¾...å¤§å‹è®¡ç®—æœº...å¾—åˆ°åº”ç”¨")ï¼ŒID 31 ("è¯¥æ¢æµ‹å™¨é€å…¥è½¨é“çš„...å¹´ä»½")ã€‚
* **ç—›ç‚¹**ï¼šéœ€è¦å…ˆæœ A å¾—åˆ°æ—¶é—´ Tï¼Œå†æœ "T + B"ã€‚ç›®å‰çš„è„šæœ¬æ˜¯ä¸€æ¬¡æ€§ç”Ÿæˆçš„ï¼Œç¼ºä¹â€œåˆ†æ­¥æ‰§è¡Œâ€çš„èƒ½åŠ›ã€‚
* **æ‰€éœ€ç­–ç•¥**ï¼š**å¤šæ­¥æœç´¢ï¼ˆIterative Searchï¼‰** æˆ– **å®½æ³›çš„æ—¶é—´çº¿æœç´¢**ã€‚

#### 3. æ ¸å¿ƒç‰¹ç‚¹ï¼šè¯­è¨€æ··åˆä¸ç‰¹å®šæ ¼å¼è¦æ±‚ (Cross-Lingual & Formatting) - **å æ¯” (~20%)**

* **ç‰¹ç‚¹**ï¼šä¸­æ–‡æé—®ï¼Œè¦æ±‚è‹±æ–‡å›ç­”ï¼ˆID 0ï¼‰ï¼›è‹±æ–‡æé—®ï¼Œæ¶‰åŠä¸­æ–‡å®ä½“ï¼ˆID 6 "ä¸­å›½å¤§é™†çš„å«è§†"ï¼‰ã€‚
* **ç—›ç‚¹**ï¼š`_detect_language` è¿‡äºç®€å•ã€‚å¦‚æœé—®é¢˜æ˜¯ä¸­æ–‡ä½†æ±‚è‹±æ–‡åï¼Œå•çº¯æœä¸­æ–‡å¯èƒ½æ‰¾ä¸åˆ°å‡†ç¡®çš„è‹±æ–‡å…¨ç§°ã€‚
* **æ‰€éœ€ç­–ç•¥**ï¼š**åŒè¯­å¹¶è¡Œæœç´¢**ã€‚æ— è®ºé—®é¢˜æ˜¯å“ªç§è¯­è¨€ï¼Œéƒ½å¼ºåˆ¶ç”Ÿæˆä¸­è‹±ä¸¤å¥— Queryã€‚

---

### ç¬¬äºŒéƒ¨åˆ†ï¼šä¼˜åŒ–æ–¹æ¡ˆä¸æ¨ç†ç­–ç•¥

ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæˆ‘ä»¬éœ€è¦å¯¹ Agent è¿›è¡Œâ€œæ‰‹æœ¯â€ï¼š

1. **å¼•å…¥ `deconstruct` (æ‹†è§£) ç­–ç•¥**ï¼š
* å¯¹äºé•¿æè¿°é—®é¢˜ï¼Œä¸è¦æ•´å¥æœç´¢ã€‚è„šæœ¬éœ€è¦å…·å¤‡ç®€å•çš„ NLP èƒ½åŠ›ï¼ˆæˆ–æ­£åˆ™è¡¨è¾¾å¼ï¼‰ï¼Œå°†é•¿å¥æ‹†åˆ†ä¸ºæ ¸å¿ƒåè¯çŸ­è¯­ã€‚


2. **å¢å¼º `general` ç­–ç•¥çš„é²æ£’æ€§**ï¼š
* ç›®å‰çš„ `entertainment` å’Œ `academic` åˆ’åˆ†å¯¹äºâ€œè°œè¯­é¢˜â€æ¥è¯´å¤ªç»†äº†ã€‚å¾ˆå¤šè°œè¯­æ—¢æ¶‰åŠå­¦æœ¯åˆæ¶‰åŠå†å²ã€‚
* å»ºè®®å¢åŠ ä¸€ä¸ª **`comprehensive` (ç»¼åˆ)** ç­–ç•¥ï¼Œå¼ºåˆ¶è¿›è¡Œå¤šç»´åº¦ï¼ˆå®ä½“+å…³é”®è¯+å¹´ä»½ï¼‰æœç´¢ã€‚


3. **å¼ºåˆ¶åŒè¯­æœç´¢**ï¼š
* ä¸å†åªæ ¹æ® `lang == 'zh'` å°±åªæœä¸­æ–‡ã€‚å¯¹äºè¿™ç§é«˜éš¾åº¦çŸ¥è¯†ç«èµ›é¢˜ï¼Œè‹±æ–‡ç»´åŸºç™¾ç§‘ï¼ˆWikipediaï¼‰çš„ä¿¡æ¯å¯†åº¦é€šå¸¸è¿œé«˜äºä¸­æ–‡æºã€‚


4. **å¼ºåŒ–â€œç‰¹å®šç­”æ¡ˆâ€çš„æ£€ç´¢**ï¼š
* å¦‚æœé—®é¢˜é‡Œæœ‰â€œè‹±æ–‡åâ€ã€â€œå…¨ç§°â€ã€â€œå­¦åâ€ï¼Œæœç´¢ Query å¿…é¡»æ˜¾å¼åŒ…å« `English name`, `full name`, `scientific name`ã€‚



---

### ç¬¬ä¸‰éƒ¨åˆ†ï¼šè½åœ°å®æ–½

#### 1. ä¼˜åŒ–åçš„ `run.py`

è¿™ä¸ªç‰ˆæœ¬åšäº†ä»¥ä¸‹å…³é”®æ”¹è¿›ï¼š

* **æ–°å¢ `Keyword Extraction**`ï¼šç®€å•çš„åœç”¨è¯è¿‡æ»¤ï¼Œä»é•¿å¥ä¸­æå–æ ¸å¿ƒè¯ã€‚
* **Force Bilingual (å¼ºåˆ¶åŒè¯­)**ï¼šå³ä½¿æ˜¯ä¸­æ–‡é—®é¢˜ï¼Œä¹Ÿç”Ÿæˆè‹±æ–‡ Queryï¼ˆåä¹‹äº¦ç„¶ï¼‰ï¼Œå› ä¸ºå¾ˆå¤šç­”æ¡ˆåœ¨è·¨è¯­è¨€æºä¸­ã€‚
* **Strategy èåˆ**ï¼šé’ˆå¯¹æè¿°æ€§å¾ˆå¼ºçš„é—®é¢˜ï¼Œå¢åŠ äº† `riddle` ç­–ç•¥ã€‚

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Smart Search Skill v2 - é’ˆå¯¹å¤æ‚æ¨ç†ä¸è°œè¯­å‹é—®é¢˜çš„ä¼˜åŒ–ç‰ˆ
"""

import os
import json
import sys
import re

def _extract_keywords(text: str, top_n=5) -> list:
    """ç®€å•çš„å…³é”®è¯æå–ï¼Œç”¨äºå¤„ç†é•¿å¥æè¿°"""
    # ç§»é™¤å¸¸è§åœç”¨è¯å’Œæ— æ„ä¹‰è¯æ±‡
    stopwords = [
        'what', 'is', 'who', 'the', 'a', 'an', 'in', 'on', 'at', 'of', 'for', 'to', 'by', 'with',
        'and', 'or', 'but', 'so', 'question', 'answer', 'please', 'find', 'search',
        'è¯·', 'é—®', 'æ˜¯ä»€ä¹ˆ', 'æ˜¯è°', 'å“ªä¸ª', 'å…³äº', 'å¯»æ‰¾', 'å›ç­”', 'æè¿°', 'ä¸€ä½', 'ä¸€ä¸ª'
    ]
    # æ¸…ç†å¼•ç”¨æ ‡è®° 
    text = re.sub(r'\', '', text)
    
    # ç®€å•çš„åˆ†è¯ï¼ˆæŒ‰ç©ºæ ¼å’Œéå­—æ¯æ•°å­—å­—ç¬¦ï¼‰
    words = re.split(r'[^\w\u4e00-\u9fff]+', text)
    keywords = [w for w in words if w.lower() not in stopwords and len(w) > 1]
    
    # å»é‡å¹¶ä¿æŒé¡ºåº
    seen = set()
    result = []
    for k in keywords:
        if k.lower() not in seen:
            seen.add(k.lower())
            result.append(k)
    return result[:top_n]

def _detect_language(text: str) -> str:
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
    total_chars = len(re.sub(r'\s', '', text))
    if total_chars == 0: return 'en'
    return 'zh' if chinese_chars / total_chars > 0.3 else 'en'

def _auto_select_strategy(query: str, entities: list) -> str:
    query_lower = query.lower()
    
    # 1. æè¿°æ€§/è°œè¯­ç±» (Riddle/Description) - æ•°æ®é›†ä¸­æœ€å¸¸è§
    # ç‰¹å¾ï¼šé•¿å¥å­ï¼ŒåŒ…å«"ä¸€ä½"ã€"person who"ã€å¤šé‡å®šè¯­
    if len(query) > 50 or "who is" in query_lower or "what is the name" in query_lower or "ä¸€ä½" in query or "é‚£ä¸ª" in query:
        return 'riddle'

    # 2. æ—¶é—´/å†å²äº¤å‰ç±»
    time_keywords = ['å“ªä¸€å¹´', 'when', 'year', 'date', 'æ—¶é—´', 'timeline', 'born', 'died', 'founded']
    if any(kw in query_lower for kw in time_keywords):
        return 'timeline'

    # 3. å­¦æœ¯/å®šä¹‰
    academic_keywords = ['paper', 'thesis', 'dissertation', 'study', 'research', 'article', 'journal', 'è®ºæ–‡', 'ç ”ç©¶', 'æœŸåˆŠ']
    if any(kw in query_lower for kw in academic_keywords):
        return 'academic'

    # 4. å¨±ä¹/ä½œå“
    ent_keywords = ['movie', 'film', 'series', 'show', 'game', 'actor', 'song', 'album', 'ç”µå½±', 'ç”µè§†å‰§', 'æ¸¸æˆ', 'åŠ¨ç”»', 'é…éŸ³']
    if any(kw in query_lower for kw in ent_keywords):
        return 'entertainment'

    return 'general'

def main():
    try:
        args_file = os.environ.get("SKILL_ARGS_FILE")
        if args_file and os.path.exists(args_file):
            with open(args_file, 'r', encoding='utf-8') as f:
                args = json.load(f)
        else:
            args_json = os.environ.get("SKILL_ARGS", "{}")
            if isinstance(args_json, str):
                try:
                    args = json.loads(args_json)
                except:
                    args = {}
            else:
                args = args_json

        query = args.get("query", "")
        entities = args.get("entities", [])
        strategy = args.get("strategy", "auto")

        if not query:
            print(json.dumps({"error": "No query provided"}))
            sys.exit(1)

        # é¢„å¤„ç†ï¼šæ¸…ç† query ä¸­çš„ source æ ‡ç­¾
        query = re.sub(r'\', '', query).strip()

        if strategy == "auto":
            strategy = _auto_select_strategy(query, entities)

        optimized_queries = []
        lang = _detect_language(query)
        keywords = _extract_keywords(query)
        
        # åŸºç¡€ç»„åˆï¼šå®ä½“ + æ ¸å¿ƒè¯
        base_search = " ".join(entities[:3]) if entities else " ".join(keywords[:4])

        # === ç­–ç•¥ç”Ÿæˆé€»è¾‘ ===

        if strategy == "riddle":
            # è°œè¯­/é•¿æè¿°ç­–ç•¥ï¼šæ ¸å¿ƒæ˜¯å»å™ªå’Œå¤šè¯­è¨€
            # 1. çº¯å…³é”®è¯ç»„åˆ
            optimized_queries.append(f"{base_search}")
            # 2. åªæœ‰å®ä½“çš„ç»„åˆ (æ‰©å¤§å¬å›)
            if entities:
                optimized_queries.append(f"{' '.join(entities)}")
            
            # 3. é’ˆå¯¹ç‰¹å®šè¾“å‡ºè¦æ±‚çš„å¤„ç†
            if "è‹±æ–‡" in query or "english" in query.lower():
                optimized_queries.append(f"{base_search} English name")
            if "å…¨ç§°" in query or "full name" in query.lower():
                optimized_queries.append(f"{base_search} full name")
            
            # 4. è·¨è¯­è¨€å°è¯• (éå¸¸é‡è¦)
            if lang == 'zh':
                 optimized_queries.append(f"{base_search} wikipedia") # å¼ºæœè‹±æ–‡ç»´åŸº
            else:
                 optimized_queries.append(f"{base_search} ç™¾åº¦ç™¾ç§‘")

        elif strategy == "academic":
            # å­¦æœ¯å¢å¼ºï¼šé’ˆå¯¹è®ºæ–‡ã€æœŸåˆŠ
            optimized_queries.append(f'"{base_search}" site:edu OR site:org')
            optimized_queries.append(f'"{base_search}" filetype:pdf')
            # å¾ˆå¤šé¢˜ç›®é—®çš„æ˜¯è®ºæ–‡æ ‡é¢˜ï¼Œéœ€è¦ç²¾ç¡®åŒ¹é…
            if entities:
                optimized_queries.append(f'"{entities[0]}" research paper title')
            optimized_queries.append(f'{base_search} journal')

        elif strategy == "timeline":
            # æ—¶é—´çº¿å¢å¼ºï¼šèšç„¦å¹´ä»½
            optimized_queries.append(f"{base_search} date year")
            optimized_queries.append(f"when was {base_search}")
            if entities:
                optimized_queries.append(f'"{entities[0]}" timeline')
                # é’ˆå¯¹ "In the same year..." è¿™ç±»é—®é¢˜
                optimized_queries.append(f'"{entities[0]}" history events')

        elif strategy == "entertainment":
            # å¨±ä¹å¢å¼º
            if lang == 'zh':
                optimized_queries.append(f'{base_search} site:douban.com')
                optimized_queries.append(f'{base_search} ç™¾åº¦ç™¾ç§‘')
            else:
                optimized_queries.append(f'{base_search} site:imdb.com OR site:wikipedia.org')
            
            if "cast" in query.lower() or "actor" in query.lower() or "æ‰®æ¼”" in query:
                optimized_queries.append(f'{base_search} cast list')

        else: # General
            optimized_queries.append(query)
            optimized_queries.append(f"{base_search} wiki")
            optimized_queries.append(f"{base_search} overview")

        # ç»“æœå»é‡
        optimized_queries = list(dict.fromkeys(optimized_queries))

        result = {
            "status": "success",
            "strategy_used": strategy,
            "original_query": query,
            "extracted_keywords": keywords,
            "optimized_queries": optimized_queries,
            "tips": f"æ£€æµ‹åˆ°'{strategy}'ç±»å‹é—®é¢˜ã€‚å·²è‡ªåŠ¨ç”Ÿæˆè·¨è¯­è¨€æˆ–ç‰¹å®šé¢†åŸŸæŸ¥è¯¢ã€‚"
        }
        print(json.dumps(result, ensure_ascii=False, indent=2))

    except Exception as e:
        print(json.dumps({"error": str(e), "status": "failed"}), file=sys.stdout)
        sys.exit(1)

if __name__ == "__main__":
    main()

```

#### 2. ä¼˜åŒ–åçš„ `SKILL.md`

è¿™ä¸ªæ–‡æ¡£çš„é‡ç‚¹æ˜¯æ•™è‚² Agent **â€œä¸è¦ç›´æ¥æŠŠé•¿éš¾å¥ä¸¢ç»™å·¥å…·â€**ï¼Œè€Œæ˜¯æ•™ä¼šå®ƒåˆ©ç”¨å·¥å…·ç”Ÿæˆçš„å¤šä¸ª Query è¿›è¡Œæ¨ç†ã€‚

```markdown
---
name: smart-search
description: é«˜çº§æ¨ç†æœç´¢å·¥å…·ã€‚ä¸“ä¸ºå¤æ‚è°œè¯­ã€å®ä½“é“¾æ¥ã€è·¨è¯­è¨€æ£€ç´¢å’Œå¤šè·³æ¨ç†é—®é¢˜è®¾è®¡ã€‚èƒ½å¤Ÿè‡ªåŠ¨æå–å…³é”®è¯ã€ç”Ÿæˆå¤šè¯­è¨€æŸ¥è¯¢å¹¶é’ˆå¯¹ç‰¹å®šé¢†åŸŸï¼ˆå­¦æœ¯/æ—¶é—´çº¿/å¨±ä¹ï¼‰è¿›è¡Œä¼˜åŒ–ã€‚
---

# Smart Search Skill (é«˜çº§ç‰ˆ)

## æ ¸å¿ƒèƒ½åŠ›
æœ¬æŠ€èƒ½ä¸åªæ˜¯ç®€å•çš„æœç´¢ï¼Œè€Œæ˜¯é’ˆå¯¹**å¤æ‚æè¿°æ€§é—®é¢˜**ï¼ˆå¦‚ "ä¸€ä¸ªå‡ºç”Ÿåœ¨...çš„ä½œå®¶..."ï¼‰çš„**è§£é¢˜å™¨**ã€‚å®ƒä¼šè‡ªåŠ¨å°†è‡ªç„¶è¯­è¨€æè¿°è½¬åŒ–ä¸ºæœç´¢å¼•æ“èƒ½ç†è§£çš„ç»“æ„åŒ–æŸ¥è¯¢ã€‚

## é€‚ç”¨åœºæ™¯ (Benchmark Analysis)

1.  **è°œè¯­å‹å®ä½“æœç´¢ (Riddle/Entity Linking)**
    * *ç”¨æˆ·é—®*: "ä¸€ä½æ¬§æ´²å­¦è€…çš„æŸé¡¹å¼€æºç¡¬ä»¶é¡¹ç›®ï¼Œçµæ„Ÿæºäºå…ƒèƒè‡ªåŠ¨æœº..."
    * *æŠ€èƒ½è§£*: è‡ªåŠ¨æå– "European scholar", "open source hardware", "cellular automaton"ï¼Œå¹¶ç”Ÿæˆç»„åˆæŸ¥è¯¢ã€‚
2.  **è·¨è¯­è¨€/ç‰¹å®šæ ¼å¼æ£€ç´¢ (Cross-Lingual)**
    * *ç”¨æˆ·é—®*: (ä¸­æ–‡æè¿°) "...è¯·å›ç­”è¯¥å®ä½“çš„è‹±æ–‡åç§°ã€‚"
    * *æŠ€èƒ½è§£*: å¼ºåˆ¶æœç´¢è‹±æ–‡æº (site:en.wikipedia.org)ï¼Œå¯»æ‰¾ English nameã€‚
3.  **æ—¶é—´é”šå®š (Timeline Anchoring)**
    * *ç”¨æˆ·é—®*: "äº‹ä»¶å‘ç”Ÿçš„é‚£ä¸€å¹´ï¼ŒæŸè½¯ä»¶å·¨å¤´..."
    * *æŠ€èƒ½è§£*: ç”Ÿæˆé’ˆå¯¹å¹´ä»½å’Œå†å²äº‹ä»¶çš„ç²¾ç¡®æŸ¥è¯¢ã€‚

## ç­–ç•¥è¯´æ˜

æœ¬æŠ€èƒ½ä¼šè‡ªåŠ¨è¯†åˆ«ä»¥ä¸‹ç­–ç•¥ï¼Œæ— éœ€äººå·¥å¹²é¢„ï¼š

| ç­–ç•¥ä»£ç  | è§¦å‘åœºæ™¯ | ä¼˜åŒ–é€»è¾‘ |
| :--- | :--- | :--- |
| **riddle** | é•¿éš¾å¥ã€å¤šå±æ€§æè¿°ã€"Who is..." | æå–æ ¸å¿ƒåè¯ï¼Œå»é™¤åœç”¨è¯ï¼Œå¼ºåˆ¶ä½¿ç”¨ Wikipedia/ç™¾åº¦ç™¾ç§‘æºã€‚ |
| **timeline** | è¯¢é—®å¹´ä»½ã€"In the same year" | å¢åŠ  "date", "year", "timeline" å…³é”®è¯ï¼Œä¾§é‡å†å²è®°å½•ã€‚ |
| **academic** | è®ºæ–‡ã€æœŸåˆŠã€ç ”ç©¶ã€å­¦è€… | é”å®š .edu/.org åŸŸåï¼Œå¢åŠ  "paper", "dissertation" ç­‰åç¼€ã€‚ |
| **entertainment** | å½±è§†ã€æ¸¸æˆã€åŠ¨æ¼«ã€é…éŸ³ | é”å®š IMDb, è±†ç“£, Fandom ç­‰å‚ç›´æ•°æ®åº“ã€‚ |

## æœ€ä½³å®è·µ (Agent æŒ‡å—)

å½“å¤„ç†å¤æ‚é—®é¢˜æ—¶ï¼Œè¯·éµå¾ªä»¥ä¸‹ **"æœç´¢-è§‚å¯Ÿ-æ¨ç†"** å¾ªç¯ï¼š

1.  **åˆæ¬¡æœç´¢**: ç›´æ¥ä½¿ç”¨ `smart-search`ï¼Œä¼ å…¥ç”¨æˆ·çš„åŸå§‹é•¿é—®é¢˜å’Œå·²çŸ¥çš„å®ä½“ã€‚
    * *Input*: `query="ä¸€ä½æ¬§æ´²å­¦è€…çš„..."`, `entities=["å¼€æºç¡¬ä»¶"]`
2.  **è§‚å¯Ÿç»“æœ**: å·¥å…·ä¼šè¿”å› 3-5 ä¸ªä¼˜åŒ–åçš„ Queryï¼ˆåŒ…å«å…³é”®è¯ç»„åˆï¼‰ã€‚
    * *ç³»ç»Ÿè¡Œä¸º*: å·¥å…·ä¼šè‡ªåŠ¨å°è¯• "Keywords English name" æˆ– "Keywords wiki"ã€‚
3.  **äºŒæ¬¡éªŒè¯**: å¦‚æœåˆæ¬¡æœç´¢æ‰¾åˆ°äº†å€™é€‰äººï¼ˆä¾‹å¦‚ "Adrian Bowyer"ï¼‰ï¼Œä½†éœ€è¦ç¡®è®¤ç»†èŠ‚ï¼ˆå¦‚"2010å¹´ä¸­æœŸäº¤æ˜“æƒ…å†µ"ï¼‰ï¼Œè¯·å†æ¬¡è°ƒç”¨æœ¬å·¥å…·ï¼Œä½†è¿™æ¬¡å°†å€™é€‰äººä½œä¸ºå®ä½“ä¼ å…¥ã€‚
    * *Input*: `query="Adrian Bowyer business entity 2010s"`, `entities=["Adrian Bowyer"]`

## å‚æ•°è¯´æ˜

* `query` (å¿…å¡«): ç”¨æˆ·çš„åŸå§‹é—®é¢˜æˆ–éœ€è¦æŸ¥è¯çš„æè¿°ã€‚**æ³¨æ„ï¼šå³ä½¿é—®é¢˜å¾ˆé•¿ï¼Œä¹Ÿè¯·å®Œæ•´ä¼ å…¥ï¼Œå·¥å…·å†…ç½®äº†å…³é”®è¯æå–ç®—æ³•ã€‚**
* `entities` (é€‰å¡«): ä½ å·²ç»è¯†åˆ«å‡ºçš„æ˜ç¡®å®ä½“åˆ—è¡¨ï¼ˆå¦‚äººåã€åœ°åã€ä¹¦åï¼‰ã€‚è¿™æœ‰åŠ©äºæé«˜æœç´¢ç²¾åº¦ã€‚

## ç¤ºä¾‹

**è¾“å…¥**:
```json
{
  "query": "ä¸€ä½åœ¨16ä¸–çºªç»Ÿæ²»ç€ä¸€ä¸ªåºå¤§å¸å›½çš„å›ä¸»ï¼Œæ›¾å› ä¸€ç§æ…¢æ€§å…³èŠ‚ç–¾ç—…è€Œå°è¯•è¿‡...",
  "entities": ["16ä¸–çºª", "å¸å›½"]
}

```

**å·¥å…·å†…éƒ¨é€»è¾‘**:

1. æ£€æµ‹åˆ°é•¿æè¿° -> æ¿€æ´» **riddle** ç­–ç•¥ã€‚
2. æå–å…³é”®è¯ -> "16th century monarch", "chronic joint disease", "empire"ã€‚
3. ç”Ÿæˆ Query:
* `16th century monarch chronic joint disease empire`
* `"16ä¸–çºª" "å¸å›½" å…³èŠ‚ç–¾ç—…`
* `monarch joint disease history wiki`



**è¾“å‡º**:
è¿”å›ä¼˜åŒ–åçš„æŸ¥è¯¢åˆ—è¡¨ï¼Œä¾› Agent è°ƒç”¨åº•å±‚ Web Searchã€‚

```

### æ€»ç»“

è¿™ä»½ä¼˜åŒ–çš„æ ¸å¿ƒåœ¨äº**â€œæ‰¿è®¤æœç´¢å¼•æ“çš„å±€é™æ€§â€**ã€‚æœç´¢å¼•æ“ä¸ç†è§£å¤æ‚çš„è¯­æ³•é€»è¾‘ï¼Œåªçœ‹é‡å…³é”®è¯å…±ç°ã€‚
1.  **`run.py`** å˜æˆäº†ä¸€ä¸ªâ€œç¿»è¯‘å™¨â€ï¼ŒæŠŠäººç±»çš„å¤æ‚è°œè¯­ç¿»è¯‘æˆæœç´¢å¼•æ“å–œæ¬¢çš„â€œå…³é”®è¯æ²™æ‹‰ï¼ˆKeyword Saladï¼‰â€ã€‚
2.  **`SKILL.md`** æ˜ç¡®äº† Agent åœ¨é¢å¯¹é•¿é—®é¢˜æ—¶ï¼Œä¸è¦è¯•å›¾è‡ªå·±æ¦‚æ‹¬ï¼ˆå®¹æ˜“ä¸¢å¤±ç»†èŠ‚ï¼‰ï¼Œè€Œæ˜¯æŠŠåŸè¯äº¤ç»™å·¥å…·ï¼Œè®©å·¥å…·é‡Œçš„æ­£åˆ™å’Œé€»è¾‘å»å¤„ç†ã€‚

```
è¿™æ˜¯é’ˆå¯¹ä½ æä¾›çš„ `question.jsonl` æ•°æ®é›†ï¼ˆé«˜éš¾åº¦è°œè¯­é¢˜ã€å¤šè·³æ¨ç†ã€è·¨è¯­è¨€å®ä½“æœç´¢ï¼‰çš„é’ˆå¯¹æ€§ä¼˜åŒ–æ–¹æ¡ˆã€‚

ä½ éœ€è¦ä¿®æ”¹/åˆ›å»ºä»¥ä¸‹æ–‡ä»¶ã€‚æ ¸å¿ƒé€»è¾‘æ˜¯å°† **å¤æ‚çš„è‡ªç„¶è¯­è¨€ç†è§£** ä» LLM è½¬ç§»åˆ° **ä¸“é—¨çš„ Python è„šæœ¬ (`run.py`)** ä¸­å¤„ç†ï¼Œå› ä¸ºä»£ç åœ¨å…³é”®è¯æå–ã€å»åœç”¨è¯å’Œé€»è¾‘ç»„åˆä¸Šæ¯” LLM çš„ä¸€æ¬¡æ€§ç”Ÿæˆæ›´ç¨³å®šã€‚

### ç¬¬ä¸€æ­¥ï¼šæ›´æ–° Skill æ–‡ä»¶

è¯·åœ¨ `skills/smart-search/` ç›®å½•ä¸‹åˆ›å»ºæˆ–è¦†ç›–ä»¥ä¸‹ä¸¤ä¸ªæ–‡ä»¶ã€‚

#### 1. `skills/smart-search/run.py`

è¿™æ˜¯â€œå¤§è„‘â€ã€‚å®ƒå¢åŠ äº† **`riddle` (è°œè¯­/é•¿æè¿°)** ç­–ç•¥ï¼Œä¸“é—¨å¤„ç†ä½ æ•°æ®é›†é‡Œé‚£ç§â€œä¸€ä½å‡ºç”Ÿåœ¨...çš„...â€é•¿éš¾å¥ã€‚

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Smart Search Skill v2 - é’ˆå¯¹å¤æ‚æ¨ç†ä¸è°œè¯­å‹é—®é¢˜çš„ä¼˜åŒ–ç‰ˆ
"""

import os
import json
import sys
import re

def _extract_keywords(text: str, top_n=6) -> list:
    """é’ˆå¯¹è°œè¯­å‹é•¿å¥çš„å…³é”®è¯æå–"""
    # æ‰©å±•çš„åœç”¨è¯è¡¨ï¼Œè¿‡æ»¤æ‰æ— æ„ä¹‰çš„æè¿°è¯
    stopwords = {
        'what', 'is', 'who', 'the', 'a', 'an', 'in', 'on', 'at', 'of', 'for', 'to', 'by', 'with',
        'and', 'or', 'but', 'so', 'question', 'answer', 'please', 'find', 'search',
        'è¯·', 'é—®', 'æ˜¯ä»€ä¹ˆ', 'æ˜¯è°', 'å“ªä¸ª', 'å…³äº', 'å¯»æ‰¾', 'å›ç­”', 'æè¿°', 'ä¸€ä½', 'ä¸€ä¸ª',
        'person', 'people', 'man', 'woman', 'named', 'called', 'known', 'famous',
        'during', 'between', 'before', 'after', 'years', 'year', 'time'
    }
    
    # æ¸…ç†å¼•ç”¨æ ‡è®° å’Œæ ‡ç‚¹
    text_clean = re.sub(r'\', '', text)
    text_clean = re.sub(r'[^\w\s\u4e00-\u9fff-]', ' ', text_clean)
    
    words = text_clean.split()
    keywords = []
    seen = set()
    
    for w in words:
        w_lower = w.lower()
        if w_lower not in stopwords and len(w) > 1 and w_lower not in seen:
            # è¿‡æ»¤çº¯æ•°å­—ï¼ˆé™¤éæ˜¯4ä½å¹´ä»½ï¼‰
            if w.isdigit() and not (len(w) == 4 and (w.startswith('19') or w.startswith('20'))):
                continue
            seen.add(w_lower)
            keywords.append(w)
            
    return keywords[:top_n]

def _detect_language(text: str) -> str:
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
    total_chars = len(re.sub(r'\s', '', text))
    if total_chars == 0: return 'en'
    return 'zh' if chinese_chars / total_chars > 0.3 else 'en'

def _auto_select_strategy(query: str, entities: list) -> str:
    query_lower = query.lower()
    
    # 1. æè¿°æ€§/è°œè¯­ç±» (Riddle/Description) - æ•°æ®é›†ä¸­æœ€å¸¸è§
    # ç‰¹å¾ï¼šé•¿å¥å­ï¼ŒåŒ…å«"ä¸€ä½"ã€"person who"ã€"author of"
    if len(query) > 40 and ("who" in query_lower or "what" in query_lower or "ä¸€ä½" in query or "author" in query_lower):
        return 'riddle'

    # 2. æ—¶é—´/å†å²äº¤å‰ç±»
    time_keywords = ['å“ªä¸€å¹´', 'when', 'year', 'date', 'æ—¶é—´', 'timeline', 'born', 'died', 'founded', 'century']
    if any(kw in query_lower for kw in time_keywords):
        return 'timeline'

    # 3. å­¦æœ¯/å®šä¹‰
    academic_keywords = ['paper', 'thesis', 'dissertation', 'study', 'research', 'article', 'journal', 'è®ºæ–‡', 'ç ”ç©¶', 'æœŸåˆŠ', 'professor', 'scientist']
    if any(kw in query_lower for kw in academic_keywords):
        return 'academic'

    return 'general'

def main():
    try:
        # å…¼å®¹ä¸¤ç§å‚æ•°ä¼ é€’æ–¹å¼
        args_file = os.environ.get("SKILL_ARGS_FILE")
        if args_file and os.path.exists(args_file):
            with open(args_file, 'r', encoding='utf-8') as f:
                args = json.load(f)
        else:
            args_json = os.environ.get("SKILL_ARGS", "{}")
            args = json.loads(args_json) if isinstance(args_json, str) else args_json

        query = args.get("query", "")
        entities = args.get("entities", [])
        strategy = args.get("strategy", "auto")

        if not query:
            print(json.dumps({"error": "No query provided"}))
            sys.exit(1)

        # é¢„å¤„ç†ï¼šæ¸…ç† query ä¸­çš„ source æ ‡ç­¾
        query = re.sub(r'\', '', query).strip()

        if strategy == "auto":
            strategy = _auto_select_strategy(query, entities)

        optimized_queries = []
        lang = _detect_language(query)
        keywords = _extract_keywords(query)
        
        # åŸºç¡€ç»„åˆï¼šå®ä½“ + æ ¸å¿ƒè¯
        # å¦‚æœå®ä½“è¯†åˆ«ä¸å…¨ï¼Œç”¨æå–çš„å…³é”®è¯è¡¥å……
        base_parts = entities[:3] + [k for k in keywords if k not in entities]
        base_search = " ".join(base_parts[:5])

        # === ç­–ç•¥ç”Ÿæˆé€»è¾‘ ===

        if strategy == "riddle":
            # è°œè¯­/é•¿æè¿°ç­–ç•¥ï¼šæ ¸å¿ƒæ˜¯å»å™ª + è·¨è¯­è¨€ + å¼ºå®ä½“é“¾æ¥
            
            # 1. çº¯å…³é”®è¯ç»„åˆ (æœ€å®½æ³›)
            optimized_queries.append(f"{base_search}")
            
            # 2. é’ˆå¯¹ç‰¹å®šè¾“å‡ºè¦æ±‚çš„å¤„ç† (éå¸¸å…³é”®)
            # å¾ˆå¤šé¢˜ç›®è¦æ±‚ English Name
            if "è‹±æ–‡" in query or "english" in query.lower():
                optimized_queries.append(f"{base_search} English name")
            if "å…¨ç§°" in query or "full name" in query.lower():
                optimized_queries.append(f"{base_search} full name")
            
            # 3. è·¨è¯­è¨€å°è¯• (å¼ºåˆ¶æœè‹±æ–‡/ä¸­æ–‡ç»´åŸº)
            if lang == 'zh':
                 optimized_queries.append(f"{base_search} wikipedia") # å³ä½¿ä¸­æ–‡é—®ï¼Œä¹Ÿæœè‹±æ–‡ç»´åŸº
            else:
                 optimized_queries.append(f"{base_search} ç™¾åº¦ç™¾ç§‘")

        elif strategy == "academic":
            # å­¦æœ¯å¢å¼º
            if entities:
                # è®ºæ–‡é¢˜é€šå¸¸æœ‰å¾ˆé•¿çš„å®šè¯­ï¼Œåªæœå®ä½“+Paper
                optimized_queries.append(f'{" ".join(entities[:2])} research paper')
            optimized_queries.append(f'{base_search} site:edu OR site:org')
            optimized_queries.append(f'{base_search} filetype:pdf')

        elif strategy == "timeline":
            # æ—¶é—´çº¿å¢å¼ºï¼šèšç„¦å¹´ä»½
            optimized_queries.append(f"{base_search} date year")
            optimized_queries.append(f"when was {entities[0] if entities else base_search} founded")
            if entities:
                optimized_queries.append(f'"{entities[0]}" timeline history')

        else: # General
            optimized_queries.append(query)
            optimized_queries.append(f"{base_search} wiki")

        # ç»“æœå»é‡
        optimized_queries = list(dict.fromkeys(optimized_queries))
        
        # é™åˆ¶æ•°é‡ï¼Œåªè¿”å›æœ€æœ‰ç”¨çš„3-4ä¸ªï¼Œé¿å…Agentæ‰§è¡Œå¤ªä¹…
        optimized_queries = optimized_queries[:4]

        result = {
            "status": "success",
            "strategy_used": strategy,
            "extracted_keywords": keywords,
            "optimized_queries": optimized_queries,
            "tips": f"æ£€æµ‹åˆ°'{strategy}'é—®é¢˜ã€‚å·²æå–å…³é”®è¯: {keywords[:3]}... å·²ç”Ÿæˆä¼˜åŒ–æŸ¥è¯¢ã€‚"
        }
        print(json.dumps(result, ensure_ascii=False, indent=2))

    except Exception as e:
        print(json.dumps({"error": str(e), "status": "failed"}), file=sys.stdout)
        sys.exit(1)

if __name__ == "__main__":
    main()

```

#### 2. `skills/smart-search/SKILL.md`

æ›´æ–°æè¿°ï¼Œæ˜ç¡®å‘Šè¯‰ Agent ä»€ä¹ˆæ—¶å€™ç”¨ã€‚

```markdown
---
name: smart-search
description: æ™ºèƒ½æœç´¢ä¼˜åŒ–å™¨ã€‚ä¸“ç”¨äºè§£å†³é•¿éš¾å¥æè¿°ï¼ˆè°œè¯­å‹ï¼‰ã€å®ä½“é“¾æ¥ã€å­¦æœ¯æ£€ç´¢å’Œæ—¶é—´çº¿é—®é¢˜ã€‚èƒ½è‡ªåŠ¨æå–å…³é”®è¯å¹¶ç”Ÿæˆæœç´¢å¼•æ“å‹å¥½çš„æŸ¥è¯¢ã€‚
---

# Smart Search

å½“ç”¨æˆ·çš„é—®é¢˜æ˜¯ä¸€ä¸ª**é•¿æè¿°**ï¼ˆä¾‹å¦‚ï¼šâ€œä¸€ä½å‡ºç”Ÿåœ¨...çš„...åšäº†...â€ï¼‰æˆ–è€…æ¶‰åŠ**å­¦æœ¯è®ºæ–‡**ã€**å…·ä½“å¹´ä»½**æ—¶ï¼Œç›´æ¥æœç´¢å¾€å¾€å¤±è´¥ã€‚æœ¬æŠ€èƒ½ä¼šå°†è‡ªç„¶è¯­è¨€è½¬æ¢ä¸ºæœç´¢å¼•æ“èƒ½ç†è§£çš„å…³é”®è¯ç»„åˆã€‚

## ä½•æ—¶ä½¿ç”¨
1. **é•¿æè¿°/è°œè¯­é¢˜**: "A person who was born in..." æˆ– "ä¸€ä½...çš„å­¦è€…..."
2. **éœ€è¦ç‰¹å®šæ ¼å¼**: è¯¢é—® "è‹±æ–‡å"ã€"å…¨ç§°"ã€"æ‹‰ä¸å­¦å"ã€‚
3. **å­¦æœ¯/è®ºæ–‡**: å¯»æ‰¾ç‰¹å®šè®ºæ–‡æ ‡é¢˜æˆ–ä½œè€…ã€‚
4. **æœç´¢å¤±è´¥å**: å¦‚æœ web_search è¿”å›ç©ºæˆ–æ— å…³ç»“æœï¼Œç«‹å³ä½¿ç”¨æœ¬æŠ€èƒ½ã€‚

## å‚æ•°
- `query`: ç”¨æˆ·çš„åŸå§‹å®Œæ•´é—®é¢˜ï¼ˆä¸è¦è‡ªå·±æ€»ç»“ï¼Œå·¥å…·ä¼šè‡ªåŠ¨æå–å…³é”®è¯ï¼‰ã€‚
- `entities`: ä½ å·²ç»è¯†åˆ«å‡ºçš„å®ä½“åˆ—è¡¨ï¼ˆå¦‚äººåã€åœ°åï¼‰ï¼Œè¾…åŠ©æé«˜ç²¾åº¦ã€‚
- `strategy`: (å¯é€‰) 'auto' | 'riddle' | 'academic' | 'timeline'ã€‚å»ºè®®é»˜è®¤ 'auto'ã€‚

## è¿”å›
å·¥å…·è¿”å› `optimized_queries` åˆ—è¡¨ã€‚ä½ **å¿…é¡»**ä½¿ç”¨è¿™äº›æŸ¥è¯¢è°ƒç”¨ `web_search`ã€‚

```

---

### ç¬¬äºŒæ­¥ï¼šä¿®æ”¹ `agent_loop.py`

åœ¨ `agent_loop.py` ä¸­ï¼Œä½ éœ€è¦å¢å¼º `system_prompt_addition`ï¼Œè®© Agent èƒ½å¤Ÿè¯†åˆ«å‡ºä½ æ•°æ®é›†é‡Œçš„é‚£ç§â€œè°œè¯­é¢˜â€å¹¶ä¸»åŠ¨è°ƒç”¨ Skillã€‚

**ä¿®æ”¹ä½ç½®**ï¼šåœ¨ `agent_loop` å‡½æ•°å†…éƒ¨ï¼Œ`system_prompt_addition` å­—ç¬¦ä¸²å®šä¹‰çš„ `### âš ï¸ CRITICAL: Skills ä½¿ç”¨ä¼˜å…ˆçº§` éƒ¨åˆ†ã€‚

**ä¿®æ”¹åçš„å†…å®¹å»ºè®®**ï¼š

```python
    # ... (åœ¨ agent_loop å‡½æ•°å†…)
    if skills_prompt:
        system_prompt_addition += f"\n\n{skills_prompt}"
        # æ·»åŠ  Skill ä½¿ç”¨æŒ‡å¯¼ï¼ˆå¼ºåˆ¶ä¼˜å…ˆçº§æç¤ºï¼‰
        system_prompt_addition += """

### âš ï¸ CRITICAL: Skills ä½¿ç”¨ä¼˜å…ˆçº§ï¼ˆå¿…é¡»éµå®ˆï¼‰

**ç¡¬æ€§è¦æ±‚ï¼šé‡åˆ°ä»¥ä¸‹åœºæ™¯ï¼Œå¿…é¡»å…ˆä½¿ç”¨ `smart-search` Skillï¼Œä¸¥ç¦ç›´æ¥ä½¿ç”¨ web_searchï¼**

1. **é•¿æè¿°/è°œè¯­å‹å®ä½“æœç´¢ (Riddle Queries)**
   - ç‰¹å¾ï¼šé—®é¢˜å¾ˆé•¿ï¼Œæ²¡æœ‰ç›´æ¥è¯´åå­—ï¼Œè€Œæ˜¯è¯´ "ä¸€ä½...çš„äºº"ã€"ä¸€ä¸ª...çš„å…¬å¸"ã€"A person who..."
   - ç¤ºä¾‹ï¼š"ä¸€ä½æ¬§æ´²å­¦è€…çš„æŸé¡¹å¼€æºç¡¬ä»¶é¡¹ç›®..."ã€"Who is the author of the article that..."
   - **è¡ŒåŠ¨**ï¼šç«‹å³è°ƒç”¨ `smart-search`ï¼Œå°†æ•´æ®µæè¿°ä½œä¸º `query` ä¼ å…¥ã€‚

2. **è·¨è¯­è¨€/ç‰¹å®šåç§°æœç´¢**
   - ç‰¹å¾ï¼šä¸­æ–‡æé—®ä½†è¦æ±‚ "è‹±æ–‡åç§°"ã€"å…¨ç§°"ã€"æ‹‰ä¸å­¦å"ã€‚
   - **è¡ŒåŠ¨**ï¼šè°ƒç”¨ `smart-search`ï¼Œå®ƒä¼šè‡ªåŠ¨ç”ŸæˆåŒ…å« "English name" ç­‰åç¼€çš„æŸ¥è¯¢ã€‚

3. **å­¦æœ¯/è®ºæ–‡/æ—¶é—´çº¿ç»†èŠ‚**
   - ç‰¹å¾ï¼šè¯¢é—®è®ºæ–‡æ ‡é¢˜ã€å…·ä½“å¹´ä»½ã€"å“ªä¸€å¹´"ã€‚
   - **è¡ŒåŠ¨**ï¼šè°ƒç”¨ `smart-search` (strategy='academic' æˆ– 'timeline')ã€‚

**ä½¿ç”¨æµç¨‹**
1. è¯†åˆ«é—®é¢˜æ˜¯ä¸Šè¿°ç±»å‹ -> `load_skill_file`("smart-search")
2. æ‰§è¡Œ -> `execute_script`("smart-search", {"query": "åŸé—®é¢˜", "entities": [...]})
3. **å¼ºåˆ¶**ï¼šä½¿ç”¨ Skill è¿”å›çš„ `optimized_queries` è¿›è¡Œ `web_search`ã€‚

... (ä¿ç•™åŸæœ¬çš„ multi-source-verify ç­‰éƒ¨åˆ†çš„æè¿°)
"""

```

### ç¬¬ä¸‰æ­¥ï¼šä¿®æ”¹ `agent.py` (å¾®è°ƒ)

`agent.py` ä¸­çš„ `_extract_core_entities` å’Œ `multi_hop_search` ä¸éœ€è¦å¤§æ”¹ï¼Œä½†ä¸ºäº†é…åˆ `smart-search`ï¼Œå»ºè®®åœ¨ `web_search` å¤±è´¥æ—¶ç»™å‡ºä¸€ä¸ªæ˜ç¡®çš„æç¤ºï¼Œå¼•å¯¼ Agent å»ç”¨ Skillã€‚

åœ¨ `agent.py` çš„ `web_search` å‡½æ•°æœ«å°¾ï¼ˆæˆ– `agent_loop.py` å¤„ç† `web_search` ç»“æœçš„åœ°æ–¹ï¼‰ï¼š

**`agent_loop.py` çš„ `execute_tools` å‡½æ•°ä¸­ï¼š**

```python
                    if func_name == "web_search":
                        # ... (åŸæœ‰ä»£ç )
                        if "error" in tool_result_content or "No results" in tool_result_content or "[]" in tool_result_content:
                            # ä¿®æ”¹æç¤ºï¼Œæ˜ç¡®æ¨è smart-search
                            tool_result_content += "\n[System Hint]: æœç´¢ç»“æœä¸ºç©ºã€‚è¿™é€šå¸¸æ˜¯å› ä¸ºæŸ¥è¯¢è¯å¤ªé•¿æˆ–åŒ…å«äº†å¹²æ‰°è¯ã€‚\nğŸ‘‰ **å¼ºçƒˆå»ºè®®**ï¼šç«‹å³ä½¿ç”¨ `smart-search` Skillï¼Œå®ƒå¯ä»¥è‡ªåŠ¨æå–æ ¸å¿ƒå…³é”®è¯å¹¶ç”Ÿæˆæ›´æœ‰æ•ˆçš„æŸ¥è¯¢ç»„åˆã€‚"

```

### æ€»ç»“ä¼˜åŒ–ç‚¹

1. **å…³é”®è¯æå–ç§»äº¤è„šæœ¬**ï¼šåŸæœ¬ Agent é¢å¯¹ "ä¸€ä½...çš„å­¦è€…" ä¼šä¸çŸ¥æ‰€æªï¼Œç°åœ¨ `run.py` é‡Œçš„ `_extract_keywords` ä¼šæŠŠ "ä¸€ä½"ã€"å…³äº" åˆ æ‰ï¼Œåªç•™ä¸‹ "æ¬§æ´² å­¦è€… å¼€æºç¡¬ä»¶"ï¼Œè¿™å¯¹æœç´¢å¼•æ“æåº¦å‹å¥½ã€‚
2. **å¼ºåˆ¶åŒè¯­**ï¼šä½ çš„æ•°æ®é›†å¾ˆå¤šæ˜¯ä¸­æ–‡é—®è‹±æ–‡åï¼Œ`run.py` é‡Œçš„ `riddle` ç­–ç•¥ä¼šè‡ªåŠ¨åŠ ä¸Š `English name` å’Œ `wikipedia` åç¼€ï¼Œå‘½ä¸­ç‡ä¼šå¤§å¹…æå‡ã€‚
3. **Prompt å¼•å¯¼**ï¼šåœ¨ System Prompt é‡Œæ˜ç¡®å®šä¹‰äº† "Riddle Queries" (è°œè¯­é¢˜)ï¼Œè®© Agent çŸ¥é“é‡åˆ°è¿™ç§é•¿é¢˜ä¸è¦ç¡¬æœï¼Œè¦è°ƒ Skillã€‚