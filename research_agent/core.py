import os
import json
import hashlib
import time
import re
from typing import List, Callable, Optional, AsyncIterator, cast, Literal, Any, Union, get_origin, get_args, get_type_hints
import inspect
from dataclasses import dataclass
from openai import OpenAI, BadRequestError
from openai.types.chat import ChatCompletionChunk
from difflib import SequenceMatcher
from langgraph.graph import StateGraph, END

# Assumes 'skills' module is available in path
try:
    from skills import (
        SkillIntegrationTools,
        SkillMetadata,
        build_skills_system_prompt,
        discover_skills,
    )
except ImportError:
    try:
        from ..skills import (
            SkillIntegrationTools,
            SkillMetadata,
            build_skills_system_prompt,
            discover_skills,
        )
    except ImportError:
        pass 

from .utils import (
    get_llm_client, 
    clean_answer, 
)
from .memory import MemoryStore
from .state import StateStore
from .validator import validate_plan
from .processors import _extract_core_entities
from .search import extract_answer_from_search_results
from .enhancer import get_agent_enhancer

DEFAULT_SYSTEM_PROMPT = "You are a helpful assistant."

@dataclass
class ToolCall:
    tool_call_id: Optional[str] = None
    tool_name: Optional[str] = None
    tool_arguments: Optional[dict] = None

    def to_dict(self) -> dict:
        return {
            "tool_call_id": self.tool_call_id,
            "tool_name": self.tool_name,
            "tool_arguments": self.tool_arguments
        }

@dataclass
class Chunk:
    step_index: int
    type: Literal["text", "tool_call", "tool_call_result", "final_state"]
    content: Optional[str] = None
    tool_call: Optional[ToolCall] = None
    tool_result: Optional[Any] = None

    def to_dict(self) -> dict:
        return {
            "step_index": self.step_index,
            "type": self.type,
            "content": self.content,
            "tool_call": self.tool_call.to_dict() if self.tool_call else None,
            "tool_result": str(self.tool_result) if self.tool_result else None
        }

def make_json_serializable(obj: Any) -> Any:
    if isinstance(obj, (Chunk, ToolCall)):
        return obj.to_dict()
    elif isinstance(obj, dict):
        return {k: make_json_serializable(v) for k, v in obj.items()}
    elif isinstance(obj, (list, tuple)):
        return [make_json_serializable(item) for item in obj]
    elif hasattr(obj, '__dict__'):
        return str(obj)
    else:
        return obj

def python_type_to_json_type(t):
    origin = get_origin(t)
    if t is str:
        return "string"
    elif t is int:
        return "integer"
    elif t is float:
        return "number"
    elif t is bool:
        return "boolean"
    elif t is list or origin is list:
        return "array"
    elif t is dict or origin is dict:
        return "object"
    elif origin is Union:
        args = get_args(t)
        for arg in args:
            if arg is dict or get_origin(arg) is dict:
                return "object"
            if arg is list or get_origin(arg) is list:
                return "array"
    return "string"

def function_to_schema(func: Callable) -> dict:
    type_hints = get_type_hints(func)
    signature = inspect.signature(func)
    parameters = {"type": "object", "properties": {}, "required": []}
    for name, param in signature.parameters.items():
        if name in ("self", "cls"):
            continue
        annotation = type_hints.get(name, str)
        param_type = python_type_to_json_type(annotation)
        param_info = {"type": param_type}
        if get_origin(annotation) == Literal:
            param_info["enum"] = list(get_args(annotation))
            param_info["type"] = python_type_to_json_type(type(get_args(annotation)[0]))
        parameters["properties"][name] = param_info
        if param.default == inspect.Parameter.empty:
            parameters["required"].append(name)
    return {
        "type": "function",
        "function": {
            "name": func.__name__,
            "description": (func.__doc__ or "").strip(),
            "parameters": parameters,
        },
    }

def calculate_confidence_impl(answer: str, searched_keywords: list, search_results: list) -> float:
    try:
        confidence = 0.5
        query_appearances = sum(1 for q in searched_keywords if answer in q)
        if query_appearances >= 2:
            confidence += 0.2
        for i, result in enumerate(search_results):
            snippet = result.get('summary') or result.get('snippet') or ''
            text = (result.get('title', '') + " " + snippet).lower()
            if answer.lower() in text:
                if i == 0: confidence += 0.2
                elif i < 3: confidence += 0.1
        if len(search_results) >= 3:
             confidence += 0.1
        return min(0.95, confidence)
    except Exception:
        return 0.5

def _compress_messages(messages: list) -> list:
    import json as _json
    if not messages:
        return messages
    out = []
    if messages:
        out.append(messages[0])
    if len(messages) > 1:
        out.append(messages[1])
    start_idx = max(2, len(messages) - 10)
    i = start_idx
    while i < len(messages):
        msg = messages[i]
        role = msg.get("role")
        if role == "tool":
            tool_id = msg.get("tool_call_id")
            paired_assistant_idx = None
            j = i - 1
            while j >= start_idx:
                prev = messages[j]
                if prev.get("role") == "assistant" and prev.get("tool_calls"):
                    try:
                        for tc in prev["tool_calls"]:
                            if tc.get("id") == tool_id:
                                paired_assistant_idx = j
                                break
                    except Exception:
                        pass
                    if paired_assistant_idx is not None:
                        break
                j -= 1
            if paired_assistant_idx is None:
                i += 1
                continue
            c = str(msg.get("content") or "")
            try:
                jdata = _json.loads(c)
                if isinstance(jdata, dict) and "results" in jdata:
                    safe = {"results": [{"title": r.get("title", ""), "url": r.get("url", "")} for r in jdata.get("results", [])[:3]]}
                    msg = {**msg, "content": _json.dumps(safe, ensure_ascii=False)}
            except Exception:
                # éJSONå†…å®¹ï¼Œå¦‚æœè¿‡é•¿åˆ™æˆªæ–­
                if len(c) > 1500:
                    msg = {**msg, "content": c[:1500] + "... [Content Truncated due to length]"}
                pass
            assistant_msg = messages[paired_assistant_idx]
            if assistant_msg not in out:
                out.append(assistant_msg)
            out.append(msg)
        else:
            out.append(msg)
        i += 1
    return out

def _sanitize_text(text: str) -> str:
    rep = {
        "scandal": "äº‹ä»¶",
        "corruption": "ç›¸å…³è°ƒæŸ¥",
        "investigation": "æ ¸æŸ¥",
        "family property": "å®¶åº­èƒŒæ™¯",
        "ä¸‘é—»": "äº‹ä»¶",
        "è…è´¥": "ç›¸å…³è°ƒæŸ¥",
        "è°ƒæŸ¥": "æ ¸æŸ¥"
    }
    s = str(text or "")
    for k, v in rep.items():
        s = s.replace(k, v)
    return s

def _sanitize_messages(messages: list) -> list:
    out = []
    for m in messages:
        c = str(m.get("content") or "")
        out.append({**m, "content": _sanitize_text(c)})
    return out

async def agent_loop(
    input_messages: list,
    tool_functions: List[Callable],
    skill_directories: Optional[List[str]] = ["skills"],
    max_steps: int = 200,
) -> AsyncIterator[Chunk]:
    assert os.getenv("IFLOW_API_KEY"), "IFLOW_API_KEY is not set"
    client = get_llm_client(timeout=30.0)
    enhancer = get_agent_enhancer()
    
    skills: List[SkillMetadata] = discover_skills(skill_directories) if skill_directories else []
    skills_prompt = build_skills_system_prompt(skills)
    prompt_messages = input_messages.copy()
    system_prompt_addition = ""
    if skills_prompt:
        system_prompt_addition += f"\n\n{skills_prompt}"
        system_prompt_addition += """

### ğŸš€ æœç´¢æ•ˆç‡ä¸æ‘˜è¦ä¼˜å…ˆåŸåˆ™ (Search Efficiency)
1. **ä¼˜å…ˆä½¿ç”¨ Summary**: `web_search` è¿”å›çš„ç»“æœä¸­åŒ…å« `summary` å­—æ®µã€‚è¿™æ˜¯æœç´¢ç»“æœçš„ç²¾åæ‘˜è¦ã€‚
2. **é¿å…æ»¥ç”¨ Full Content**: åœ¨å†³å®šè°ƒç”¨ `web_fetch` è¯»å–å®Œæ•´ç½‘é¡µä¹‹å‰ï¼Œè¯·**åŠ¡å¿…**å…ˆæ£€æŸ¥ `summary` æ˜¯å¦å·²ç»åŒ…å«äº†è¶³å¤Ÿå›ç­”é—®é¢˜çš„å…³é”®ä¿¡æ¯ã€‚
3. **ä½•æ—¶ä½¿ç”¨ Full Content**: ä»…å½“ `summary` è¢«æˆªæ–­(...)ã€ä¿¡æ¯æ¨¡ç³Šã€æˆ–è€…ä½ éœ€è¦æ·±åº¦éªŒè¯ç»†èŠ‚ï¼ˆå¦‚å…·ä½“æ•°æ®ã€å®Œæ•´åˆ—è¡¨ï¼‰æ—¶ï¼Œæ‰ä½¿ç”¨ `web_fetch`ã€‚
4. **èŠ‚çœèµ„æº**: å¦‚æœèƒ½é€šè¿‡ `summary` ç›´æ¥å›ç­”ï¼Œè¯·ç›´æ¥å›ç­”ï¼Œä¸è¦ä¸ºäº†"çœ‹ä¸€çœ¼"è€ŒæŠ“å–ç½‘é¡µã€‚

### ğŸ§  Planning Mode (Reasoning First)
å½“é‡åˆ°å¤æ‚é—®é¢˜ï¼ˆå¦‚"æŸå›½æ€»ç†..."ã€"20ä¸–çºªæ™šæœŸæˆç«‹çš„å…¬å¸..."ï¼‰æ—¶ï¼Œ**å¿…é¡»**éµå¾ªä»¥ä¸‹é€»è¾‘ï¼š

1. **Slot Extraction (æ§½ä½æå–)**ï¼š
   - Type (äºº/ç»„ç»‡/äº‹)
   - Hard Constraints (ç»å¯¹çº¦æŸï¼šå¹´ä»½ã€åœ°ç‚¹ã€èŒä½)
   - Soft Constraints (æè¿°æ€§çº¦æŸï¼šé£æ³¢ã€äº‹ä»¶)
   - Anchors (é”šç‚¹è¯)

2. **List-then-Filter Strategy (åˆ—è¡¨ç­›é€‰æ³•)**ï¼š
   - **ä¸è¦**ç›´æ¥æœæ•´å¥ï¼ˆå¦‚ "è°æ˜¯è’™å¤è…è´¥æ€»ç†"ï¼‰ã€‚
   - **è¦**å…ˆæœåˆ—è¡¨ï¼ˆå¦‚ "List of Prime Ministers of Mongolia"ï¼‰ã€‚
   - **ç„¶å**é€ä¸€æ ¸å¯¹å€™é€‰äººæ˜¯å¦ç¬¦åˆ Hard/Soft Constraintsã€‚

3. **Cross-Lingual Pivot (è·¨è¯­è¨€è·³æ¿)**ï¼š
   - æ¶‰åŠéè‹±è¯­å›½å®¶ï¼ˆæ—¥æœ¬ã€è’™å¤ã€æ¬§æ´²ç­‰ï¼‰ï¼Œ**å¿…é¡»**å°è¯•ç”¨è‹±è¯­æˆ–å½“åœ°è¯­è¨€æœç´¢ã€‚
   - ä½¿ç”¨ `expand_query_language` æˆ–æ‰‹åŠ¨ç¿»è¯‘å…³é”®è¯ï¼ˆå¦‚ "Mongolia Prime Minister scandals"ï¼‰ã€‚

4. **Negative Verification (åå‘è¯ä¼ª)**ï¼š
   - æ‰¾åˆ°å€™é€‰äººåï¼Œæœ "CandidateName controversy" æˆ– "CandidateName scandal" éªŒè¯è´Ÿé¢æè¿°æ˜¯å¦åŒ¹é…ã€‚
   - é‡åˆ° `[DOUBT]` æ—¶ï¼Œå¿…é¡»å¯»æ‰¾ç«äº‰å¯¹æ‰‹ï¼ˆ"Who else..."ï¼‰ã€‚

### ğŸ•’ æ·±åº¦æ€è€ƒä¸å……åˆ†éªŒè¯ï¼ˆé‡è¦ï¼‰
ä½ æ‹¥æœ‰å……è¶³çš„æ—¶é—´ï¼ˆå•æ¬¡é—®é¢˜ä¸Šé™ 60 åˆ†é’Ÿï¼‰æ¥è§£å†³é—®é¢˜ã€‚
1. **éªŒè¯åŸåˆ™ (Verification)**ï¼šä¸éœ€è¦å‡‘æœç´¢æ¬¡æ•°ã€‚åªè¦æ‰¾åˆ° **2ä¸ªç‹¬ç«‹æ¥æº** (å¦‚ SEC å®˜ç½‘ + çŸ¥åæ–°é—») èƒ½å¤Ÿäº¤å‰éªŒè¯æ ¸å¿ƒäº‹å®ï¼Œå³å¯åœæ­¢æœç´¢å¹¶å›ç­”ã€‚
2. **é¿å…å™ªéŸ³ (Avoid Noise)**ï¼šä¸è¦å› ä¸ºä¹‹å‰çš„æœç´¢å¤±è´¥è€Œå°è¯•ä¸ç›¸å…³çš„å…³é”®è¯ï¼ˆå¦‚ Crypto, Mining ç­‰ï¼‰ã€‚å¦‚æœæ‰¾ä¸åˆ°ï¼Œå°è¯•é‡ç»„æ ¸å¿ƒå®ä½“ã€‚
"""
    # === [Planner Logic] ===
    planner_user_query = ""
    rejection_context = ""
    plan_forced_skill = None 

    for m in reversed(prompt_messages):
        if m.get("role") == "user" and not planner_user_query:
            planner_user_query = str(m.get("content") or "")
        content = str(m.get("content") or "")
        if m.get("role") == "system" and "SYSTEM REMINDER" in content and "REJECTED" in content:
            rejection_context = content
            
    if len(planner_user_query) > 10: 
        print(f"--- [Planner] Start Planning for: {planner_user_query[:30]}... ---")
        planner_system_instruction = """ä½ æ˜¯ä¸€ä¸ªé«˜é˜¶æ€ç»´çš„è§„åˆ’ä¸“å®¶ (Planner)ã€‚
ä½ çš„ç›®æ ‡ä¸æ˜¯å›ç­”é—®é¢˜ï¼Œè€Œæ˜¯å°†ç”¨æˆ·å¤æ‚çš„è¯·æ±‚æ‹†è§£ä¸ºå¯æ‰§è¡Œçš„ã€é€æ­¥çš„è¡ŒåŠ¨è®¡åˆ’ (Plan)ã€‚

### è§„åˆ’åŸåˆ™
1. **è¯†åˆ«æ ¸å¿ƒéš¾ç‚¹**ï¼šæŒ‡å‡ºé—®é¢˜ä¸­çš„â€œè°œé¢˜â€ã€â€œé™·é˜±â€æˆ–â€œæ¨¡ç³Šå®ä½“â€ã€‚
2. **çº¦æŸåˆ†å±‚ (Constraint Hierarchy) - CRITICAL**:
   - **Hard Constraints (ç¡¬çº¦æŸ)**: ç»å¯¹çš„æ—¶é—´ç‚¹ï¼ˆ1990s, 2024ï¼‰ã€æ³•å¾‹/å®ªæ³•æ¡æ¬¾ã€åœ°ç†ä½ç½®ã€ç‰¹å®šèŒä½ï¼ˆå¦‚"æ”¿åºœé¦–è„‘" vs "å›½å®¶å…ƒé¦–"ï¼‰ã€‚
   - **Soft Constraints (è½¯çº¦æŸ)**: ä¸»é¢˜ï¼ˆä¸‘é—»ã€è…è´¥ï¼‰ã€æ•™è‚²èƒŒæ™¯ï¼ˆç•™å­¦ï¼‰ã€äº²å±å…³ç³»ã€‚
   - âš ï¸ **å¿…é¡»**åœ¨ç¬¬ä¸€æ­¥ä¼˜å…ˆä½¿ç”¨ç¡¬çº¦æŸæ¥ç¼©å°æœç´¢èŒƒå›´ï¼ˆå¦‚å…ˆæœ"1990å¹´å®ªæ³•ç”Ÿæ•ˆçš„å›½å®¶"ï¼‰ï¼Œè€Œä¸æ˜¯å…ˆæœè½¯çº¦æŸï¼ˆ"è…è´¥ä¸‘é—»"ï¼‰ï¼Œä»¥é¿å…è¢«é«˜çƒ­åº¦ä½†é”™è¯¯çš„å®ä½“è¯¯å¯¼ã€‚

3. **å¤šå‡è®¾å¹¶è¡ŒéªŒè¯ (Parallel Hypothesis Verification)**ï¼š
   - å¯¹äºä¸ç¡®å®šçš„å®ä½“ï¼Œ**å¿…é¡»**åˆ—å‡ºå¤šä¸ªå€™é€‰é¡¹å¹¶è¦æ±‚åˆ†åˆ«éªŒè¯ã€‚
   - ç¦æ­¢åœ¨æ²¡æœ‰ç¡®å‡¿è¯æ®å‰æ’é™¤ä»»ä½•å¯èƒ½æ€§ã€‚

4. **æ‹†è§£æ­¥éª¤ (Step-by-Step)**ï¼š
   - **Step 1**: å¿…é¡»æ˜¯åŸºäº **Hard Constraints** çš„å®½èŒƒå›´æœç´¢ (Broad Search)ã€‚
   - **Step 2**: åŸºäºæœç´¢ç»“æœæ„å»ºå€™é€‰é¡¹åˆ—è¡¨ (Candidate List)ã€‚
   - **Step 3**: å¯¹å€™é€‰é¡¹è¿›è¡Œ **Role/Type Check** (èŒä½/ç±»å‹æ£€æŸ¥)ã€‚å¦‚æœæ‰¾äººï¼Œå…ˆç¡®è®¤ä»–æ˜¯å¦æ‹…ä»»è¿‡é¢˜ç›®è¦æ±‚çš„èŒä½ï¼ˆå¦‚æ€»ç†ï¼‰ï¼Œä¸ç¬¦åˆ™ç›´æ¥æ’é™¤ã€‚
   - **Step 4+**: éªŒè¯ Soft Constraints (ä¸‘é—»ã€ç»†èŠ‚)ã€‚

5. **Answer Type Verification**:
   - ä»”ç»†æ£€æŸ¥é—®é¢˜è¦æ±‚çš„ç­”æ¡ˆç±»å‹ï¼ˆäººåã€åœ°åã€æ•°å­—ç­‰ï¼‰ã€‚
   - åœ¨ Plan ä¸­æ˜ç¡®è¦æ±‚éªŒè¯å€™é€‰ç­”æ¡ˆæ˜¯å¦ç¬¦åˆè¯¥ç±»å‹ã€‚

6. **Negative Constraints (Diversity)**:
   - å½“éœ€è¦æ’é™¤é”™è¯¯çš„å‡è®¾æˆ–æ£€æŸ¥å…¶ä»–å›½å®¶æ—¶ï¼Œæ˜¾å¼è¦æ±‚ä½¿ç”¨æ’é™¤è¯ã€‚
   - ç¤ºä¾‹ï¼š`[Search] Search for similar events in Asia -Mongolia`
   - åœ¨ Plan ä¸­æ³¨æ˜ï¼š`Use 'excluded_entities'=['Mongolia'] in smart-search`ã€‚

7. **Skill Recommendation**:
   - å¯¹äºå¤æ‚å®ä½“è¯†åˆ«ã€å­¦æœ¯è¯é¢˜æˆ–éœ€è¦ç‰¹å®šå±æ€§çš„æœç´¢ï¼Œ**å¿…é¡»**åœ¨ Plan ä¸­æ˜ç¡®æ¨èä½¿ç”¨ `smart-search`ã€‚

8. **å¼ºåˆ¶è¾“å‡ºæ ¼å¼**ï¼š
   Plan:
   1. [æœç´¢/Search] Use smart-search with Hard Constraints...
   2. [å‡è®¾/Hypothesis] Candidate A vs Candidate B
   3. [éªŒè¯/Verify] Role Check: Confirm if A was Prime Minister...
   4. [éªŒè¯/Verify] Check B...
   
æ³¨æ„ï¼šä¿æŒç®€ç»ƒï¼Œä¸è¦è¿›è¡Œå®é™…æœç´¢ï¼Œåªç”Ÿæˆè®¡åˆ’ã€‚ä¸è¦åœ¨è®¡åˆ’ä¸­åŒ…å«æœ€ç»ˆç­”æ¡ˆã€‚
"""
        planner_prompt = [
            {"role": "system", "content": planner_system_instruction}
        ]
        
        if rejection_context:
            planner_prompt.append({
                "role": "user", 
                "content": f"!!! PREVIOUS ATTEMPT REJECTED !!!\nContext:\n{rejection_context}\n\nINSTRUCTION: The previous plan/execution failed. You MUST generate a NEW plan that specifically addresses the rejection reasons.\n1. Do NOT repeat the same mistakes.\n2. If stuck in a loop, explicitly recommend `smart-search` with `excluded_entities`.\n3. If the answer type was wrong, prioritize finding the correct entity type."
            })
            
        planner_prompt.append({"role": "user", "content": f"è¯·ä¸ºè¿™ä¸ªé—®é¢˜åˆ¶å®šè°ƒç ”è®¡åˆ’ï¼š{planner_user_query}"})

        plan = ""
        # Planner Retry Loop
        for plan_attempt in range(2):
            try:
                plan_resp = client.chat.completions.create(
                    model="qwen3-max",
                    messages=planner_prompt,
                    temperature=0.5,
                    max_tokens=1024
                )
                plan = plan_resp.choices[0].message.content
                print(f"--- [Planner] Plan Generated (Attempt {plan_attempt+1}) ---\n{plan}\n--------------------------------")

                validation_result = validate_plan(plan, rejection_context)

                if not validation_result.get("is_valid", True):
                    issues = validation_result.get("issues", [])
                    suggestions = validation_result.get("suggestions", [])
                    print(f"[Planner] âš ï¸ Plan invalid (Attempt {plan_attempt+1}). Issues: {issues}")
                    
                    if plan_attempt < 1: # If not the last attempt, retry
                         feedback_msg = f"The plan you generated is INVALID based on logic checks:\nIssues: {json.dumps(issues, ensure_ascii=False)}\nSuggestions: {json.dumps(suggestions, ensure_ascii=False)}\n\nPlease REGENERATE the plan. Fix these logic errors specifically."
                         planner_prompt.append({"role": "assistant", "content": plan})
                         planner_prompt.append({"role": "user", "content": feedback_msg})
                         continue
                    else:
                         print(f"[Planner] âš ï¸ Max retries reached. Using last plan despite issues.")
                else:
                    print(f"[Planner] âœ… Plan verified successfully.")
                    break
            except Exception as e:
                print(f"[Planner Error] Attempt {plan_attempt+1}: {e}")
                if plan_attempt == 1:
                     break

        if plan:
            system_prompt_addition += f"\n\n### ğŸ“‹ CURRENT PLAN (Follow this strictly)\n{plan}\n\nINSTRUCTION: If the plan recommends a specific Skill (e.g. smart-search), you MUST use it. Execute step 1 now."

            plan_forced_skill = None
            if "smart-search" in plan.lower():
                plan_forced_skill = "smart-search"
            elif "multi-source-verify" in plan.lower():
                plan_forced_skill = "multi-source-verify"
            elif "chain-of-verification" in plan.lower():
                plan_forced_skill = "chain-of-verification"
            elif "deep-research" in plan.lower():
                plan_forced_skill = "deep-research"

            if plan_forced_skill:
                print(f"[Planner] ğŸ”’ Planæ¨èSkill '{plan_forced_skill}',å°†åœ¨ç¬¬ä¸€æ­¥å¼ºåˆ¶æ‰§è¡Œ")
    # === [End Planner Logic] ===

    system_prompt_addition += """

### Confidence Protocol
Do NOT wait for a single document to confirm all details. If you have 3 separate sources confirming (1) Name, (2) Scandal, (3) Constitution, verify them logically and submit the answer. Do not loop beyond 15 steps.
"""

    system_prompt_addition += f"\n\nIMPORTANT: You have a maximum of {max_steps} steps. Currently you are at step {{step_index}}. If you cannot find the exact answer after 5-6 steps, please synthesize the best possible answer from the information you have gathered so far. Do not get stuck in a loop of repeated searches."
    if prompt_messages:
        if prompt_messages[0].get("role") == "system":
            original_content = prompt_messages[0].get("content", "")
            prompt_messages[0] = {"role": "system", "content": original_content + system_prompt_addition}
        else:
            prompt_messages.insert(0, {"role": "system", "content": f"{DEFAULT_SYSTEM_PROMPT}{system_prompt_addition}"})
            
    # Add final answer reminder
    prompt_messages[0]["content"] += "\n\nREMINDER: Output ONLY the answer string. No explanations. Answer in the SAME LANGUAGE as the question (unless explicitly requested otherwise). Even if uncertain, guess the most likely one."
    llm_tools = (tool_functions or []).copy()
    if skills:
        skill_tools = SkillIntegrationTools(skills)
        llm_tools.extend([skill_tools.load_skill_file, skill_tools.execute_script])
    memory = MemoryStore()
    memory.build_index()
    user_query = ""
    for m in reversed(prompt_messages):
        if m.get("role") == "user":
            user_query = str(m.get("content") or "")
            break
            
    mem_hits = memory.search(user_query, top_k=4)
    if mem_hits:
        joined = "\n".join([(hit.get("text") or "")[:500] for hit in mem_hits])
        prompt_messages.insert(1, {"role": "system", "content": f"<memory_context>\n{joined}\n</memory_context>"})
        print(f"[Monitoring] memory_context_hits={len(mem_hits)} for_query='{user_query}'")
    ents = _extract_core_entities(user_query)
    if ents:
        prompt_messages.insert(1, {"role": "system", "content": f"Core entities: {', '.join(ents)}. Search precisely for these."})
        print(f"[Monitoring] core_entities_extracted={ents} for_query='{user_query}'")
        
    candidate_frequency = {}
    for m in prompt_messages:
        if m.get("role") == "assistant":
            content = str(m.get("content") or "") + str(m.get("tool_calls") or "")
            for ent in ents:
                if ent in content:
                    candidate_frequency[ent] = candidate_frequency.get(ent, 0) + 1
    
    stuck_entities = [ent for ent, count in candidate_frequency.items() if count > 6]
    if stuck_entities:
        rotation_instruction = f"""
[System Detected Loop]: You have focused on {stuck_entities} for too many steps.
STRATEGY CHANGE REQUIRED:
1. STOP investigating {stuck_entities}.
2. SWITCH to a different candidate or country immediately.
3. If you were verifying a candidate and got mixed results, MARK them as 'Uncertain' and move on.
"""
        prompt_messages.append({"role": "system", "content": rotation_instruction})
        print(f"[Monitoring] Loop detected for {stuck_entities}. Injecting rotation instruction.")

    tool_schema = [function_to_schema(tool_function) for tool_function in llm_tools]
    tool_functions_map = {func.__name__: func for func in llm_tools}
    state_store = StateStore()
    cid_src = json.dumps(prompt_messages, ensure_ascii=False)
    cid = hashlib.sha256(cid_src.encode("utf-8")).hexdigest()[:16]
    params = {
        "model": "qwen3-max",
        "stream": True,
        "tools": tool_schema,
        "max_tokens": 1024,
        "temperature": 0.4,
    }

    def llm_step(state: dict) -> dict:
        emitted: List[Chunk] = []
        tool_calls_buffer = {}
        for attempt in range(3):
            try:
                msgs = state["messages"]
                if len(msgs) > 40 or attempt > 0:
                    msgs = _compress_messages(msgs)
                msgs = _sanitize_messages(msgs)
                msgs = _compress_messages(msgs)
                
                current_step = state["step_index"]
                
                if msgs and msgs[0]["role"] == "system":
                    content = msgs[0]["content"]
                    if "{step_index}" in content:
                        msgs[0]["content"] = content.replace("{step_index}", str(current_step))
                        
                if current_step >= max_steps - 1:
                     print(f"[System] Max steps ({max_steps}) reached. Forcing synthesis...")
                     
                     # Use Enhancer for synthesis
                     try:
                         synthesis_result = enhancer.synthesize_answer_from_state(
                             user_query,
                             state.get("meta", {}).get("last_search_results", []),
                             [m.get("content", "") for m in state["messages"] if m.get("role") == "tool"]
                         )
                         synthesized_ans = synthesis_result.get('answer', '')
                         reasoning = synthesis_result.get('reasoning', '')
                     except Exception as e:
                         print(f"[System] Synthesis failed: {e}")
                         synthesized_ans = ""
                         reasoning = ""
                     
                     force_msg = {
                         "role": "user", 
                         "content": f"[System Urgent]: You have reached the maximum step limit ({max_steps}). You MUST provide a Final Answer NOW based on the information you have. Do NOT use any more tools.\n"
                     }
                     
                     if synthesized_ans:
                         force_msg["content"] += f"\nBased on optimized synthesis, the best answer appears to be:\n{synthesized_ans}\n\nReasoning: {reasoning}\n\nPlease output this (or your own better conclusion) as 'Final Answer: ...'."
                     else:
                         force_msg["content"] += "Just output 'Final Answer: [Your Answer]'."

                     msgs.append(force_msg)
                     params["tool_choice"] = "none"
                else:
                     if "tool_choice" in params:
                         del params["tool_choice"]

                stream = client.chat.completions.create(messages=msgs, **params)
                for chunk in stream:
                    chunk = cast(ChatCompletionChunk, chunk)
                    if not chunk.choices:
                        continue
                    delta = chunk.choices[0].delta
                    if delta.content:
                        emitted.append(Chunk(type="text", content=delta.content, step_index=state["step_index"]))
                        memory.add_short(delta.content)
                    if delta.tool_calls:
                        for tc_chunk in delta.tool_calls:
                            idx = tc_chunk.index
                            if idx is None:
                                idx = 0
                            if idx not in tool_calls_buffer:
                                tool_calls_buffer[idx] = {
                                    "id": tc_chunk.id or f"call_{idx}",
                                    "function": {"name": tc_chunk.function.name or "", "arguments": ""},
                                }
                            if tc_chunk.function.name:
                                tool_calls_buffer[idx]["function"]["name"] = tc_chunk.function.name
                            if tc_chunk.function.arguments:
                                tool_calls_buffer[idx]["function"]["arguments"] += tc_chunk.function.arguments
                break 
            except BadRequestError as e:
                err_msg = str(e)
                if "DataInspectionFailed" in err_msg or "inappropriate" in err_msg or "sensitive" in err_msg:
                    print(f"[Warn] Safety Triggered. Aggressively pruning context...")
                    keep_indices = [0]
                    if len(state["messages"]) > 1:
                        keep_indices.append(1)
                    start_idx = max(2, len(state["messages"]) - 2)
                    for i in range(start_idx, len(state["messages"])):
                        keep_indices.append(i)
                    state["messages"] = [state["messages"][i] for i in keep_indices]
                    state["messages"] = _sanitize_messages(state["messages"])
                    if attempt == 2:
                        emitted.append(Chunk(type="text", content="ç”±äºå†…å®¹å®‰å…¨ç­–ç•¥é™åˆ¶ï¼Œæ— æ³•å¤„ç†å½“å‰ä¸Šä¸‹æ–‡ã€‚åŸºäºå·²æœ‰ä¿¡æ¯ï¼Œç­”æ¡ˆå¯èƒ½æ˜¯ï¼š[è¯·æŸ¥çœ‹ä¹‹å‰æœç´¢ç»“æœçš„æ‘˜è¦]ã€‚", step_index=state["step_index"]))
                        break 
                    continue 
                raise e 
        assistant_tool_calls_data = []
        for idx in sorted(tool_calls_buffer.keys()):
            raw_tool = tool_calls_buffer[idx]
            assistant_tool_calls_data.append(
                {
                    "id": raw_tool["id"],
                    "type": "function",
                    "function": {
                        "name": raw_tool["function"]["name"],
                        "arguments": raw_tool["function"]["arguments"],
                    },
                }
            )
        if not assistant_tool_calls_data and (state["step_index"] == 0 or (state.get("meta") or {}).get("force_skill_next")):
            try:
                import json as _json
                plan_skill = (state.get("meta") or {}).get("plan_forced_skill")
                skill_to_use = plan_skill if plan_skill else "smart-search"

                print(f"[llm_step] ğŸ”’ å¼ºåˆ¶é¦–æ­¥æ‰§è¡ŒSkill: '{skill_to_use}'")

                forced_args = {"skill_name": skill_to_use, "args": {"query": user_query}}
                assistant_tool_calls_data = [{
                    "id": "call_forced_0",
                    "type": "function",
                    "function": {
                        "name": "execute_script",
                        "arguments": _json.dumps(forced_args, ensure_ascii=False),
                    },
                }]
                if (state.get("meta") or {}).get("force_skill_next"):
                    state["meta"]["force_skill_next"] = False
            except Exception:
                pass
        new_messages = state["messages"][:]
        if assistant_tool_calls_data:
            new_messages.append({"role": "assistant", "tool_calls": assistant_tool_calls_data})
        return {
            **state,
            "pending_tool_calls": assistant_tool_calls_data,
            "had_tool_calls": bool(assistant_tool_calls_data),
            "messages": new_messages,
            "emitted": emitted,
        }

    def execute_tools(state: dict) -> dict:
        enhancer = get_agent_enhancer()
        emitted: List[Chunk] = state.get("emitted", [])
        new_messages = state["messages"][:]
        meta = state.get("meta") or {"searched_keywords": [], "seen_entities": [], "last_skill_output": None, "dynamic_retrieval_count": 0}
        searched_before = set(meta.get("searched_keywords") or [])

        new_memory_items = []
        for tool_data in state.get("pending_tool_calls") or []:
            call_id = tool_data["id"]
            func_name = tool_data["function"]["name"]
            func_args_str = tool_data["function"]["arguments"]
            tool_result_content = ""
            parsed_args = {}
            tool_call = ToolCall(tool_call_id=call_id, tool_name=func_name, tool_arguments={})
            try:
                parsed_args = json.loads(func_args_str)
                tool_call.tool_arguments = parsed_args
                emitted.append(Chunk(step_index=state["step_index"], type="tool_call", tool_call=tool_call))
                if func_name == "web_search":
                    q0 = str(parsed_args.get("query") or "")
                    sim_high = False
                    for old_q in searched_before:
                        if SequenceMatcher(None, q0, old_q).ratio() > 0.6:
                            sim_high = True
                            break
                    if sim_high:
                        tool_result_content = f"System Error: You have already searched for '{q0}' or something very similar. STOP searching this. Synthesize what you have."
                if func_name in tool_functions_map and not tool_result_content:
                    func = tool_functions_map[func_name]
                    attempt = 0
                    last_err = None
                    while attempt < 2:
                        try:
                            result = func(**parsed_args)
                            tool_result_content = str(result)
                            last_err = None
                            break
                        except Exception as e:
                            last_err = e
                            time.sleep(0.2 * (attempt + 1))
                            attempt += 1
                    if last_err is not None and not tool_result_content:
                        tool_result_content = f"Error: Execution failed - {str(last_err)}"

                    if func_name == "execute_script":
                        try:
                            skill_name = parsed_args.get("skill_name", "")
                            skill_output = None
                            try:
                                import re
                                stdout_match = re.search(r'<stdout>\s*(.*?)\s*</stdout>', tool_result_content, re.DOTALL)
                                if stdout_match:
                                    stdout_content = stdout_match.group(1).strip()
                                    skill_output = json.loads(stdout_content)
                            except:
                                pass

                            if skill_output and isinstance(skill_output, dict):
                                meta["last_skill_output"] = {
                                    "skill_name": skill_name,
                                    "output": skill_output,
                                    "step_index": state["step_index"]
                                }

                                if "optimized_queries" in skill_output:
                                    queries = skill_output["optimized_queries"]
                                    valid_queries = []
                                    for q in queries:
                                        if len(q.strip()) < 10:
                                            print(f"[SkillQC] Rejected too-short query: '{q}'")
                                            continue
                                        generic_only = all(word.lower() in {"list", "of", "the", "and", "or", "in"} for word in q.split())
                                        if generic_only:
                                            print(f"[SkillQC] Rejected generic-only query: '{q}'")
                                            continue
                                        valid_queries.append(q)

                                    if not valid_queries:
                                        print("[SkillQC] All Skill queries rejected. Using fallback.")
                                    else:
                                        queries = valid_queries

                                    if queries and len(queries) > 0:
                                        hint = f"\n\n[System Enforced]: Skill '{skill_name}' å·²ç”Ÿæˆä¼˜åŒ–æŸ¥è¯¢ã€‚ä½ **å¿…é¡»**ä½¿ç”¨è¿™äº›æŸ¥è¯¢æ‰§è¡Œ web_searchï¼Œä¸è¦è‡ªå·±ç¼–é€ æŸ¥è¯¢è¯ã€‚\nä¼˜åŒ–æŸ¥è¯¢åˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰ï¼š\n"
                                        for i, q in enumerate(queries[:3], 1):
                                            hint += f"  {i}. {q}\n"
                                        hint += f"\nâš ï¸ å¼ºåˆ¶è¦æ±‚ï¼šä¸‹ä¸€ä¸ª Action **å¿…é¡»**æ˜¯ web_searchï¼Œä½¿ç”¨ä¸Šè¿°æŸ¥è¯¢ä¹‹ä¸€ã€‚"
                                        tool_result_content += hint
                                        print(f"[Monitoring] skill_enforcement skill={skill_name} queries_count={len(queries)}")

                                if "verification_queries" in skill_output:
                                    queries = skill_output["verification_queries"]
                                    if queries and len(queries) > 0:
                                        hint = f"\n\n[System Enforced]: Skill '{skill_name}' å·²ç”ŸæˆéªŒè¯æŸ¥è¯¢ã€‚ä½ **å¿…é¡»**ä¾æ¬¡æ‰§è¡Œè¿™äº›éªŒè¯æŸ¥è¯¢ã€‚\néªŒè¯æŸ¥è¯¢åˆ—è¡¨ï¼š\n"
                                        for i, q_obj in enumerate(queries[:3], 1):
                                            if isinstance(q_obj, dict):
                                                purpose = q_obj.get("purpose", "")
                                                query = q_obj.get("query", "")
                                                hint += f"  {i}. [{purpose}] {query}\n"
                                            else:
                                                hint += f"  {i}. {q_obj}\n"
                                        hint += f"\nâš ï¸ å¼ºåˆ¶è¦æ±‚ï¼šå¿…é¡»ä¾æ¬¡éªŒè¯ä¸Šè¿°æŸ¥è¯¢ã€‚"
                                        tool_result_content += hint
                                        print(f"[Monitoring] skill_enforcement skill={skill_name} verification_queries_count={len(queries)}")
                        except Exception as e:
                            print(f"[Monitoring] skill_enforcement_failed: {e}")

                    if func_name == "web_search":
                        if "error" in tool_result_content or "No results" in tool_result_content or "[]" in tool_result_content:
                            tool_result_content += "\n[System Hint]: æœç´¢ç»“æœä¸ºç©ºã€‚å¯èƒ½æ˜¯å…³é”®è¯å¤ªé•¿æˆ–å¤ªå…·ä½“ã€‚è¯·å°è¯•ï¼š\n1. åªæœç´¢æ ¸å¿ƒå®ä½“åã€‚\n2. å°†ä¸­æ–‡å…³é”®è¯ç¿»è¯‘æˆè‹±æ–‡æœç´¢ï¼ˆå¾ˆå¤šå­¦æœ¯/æŠ€æœ¯å†…å®¹è‹±æ–‡æ›´å‡†ï¼‰ã€‚\n3. å»æ‰ 'site:' ç­‰é™åˆ¶ã€‚"
                        try:
                            res_json = json.loads(tool_result_content)
                            if isinstance(res_json, dict) and "results" in res_json:
                                meta["last_search_results"] = res_json["results"]
                                if not res_json["results"]:
                                    meta["empty_result_count"] = int(meta.get("empty_result_count", 0)) + 1
                                else:
                                    meta["empty_result_count"] = 0
                        except:
                            pass

                    if func_name == "web_fetch":
                         if "fetch_failed" in tool_result_content or "403" in tool_result_content:
                             print(f"[Monitoring] web_fetch_403_detected url={parsed_args.get('url')}")
                             
                             last_results = meta.get("last_search_results")
                             fallback_success = False
                             if last_results:
                                 extracted = extract_answer_from_search_results(last_results, "")
                                 if extracted["candidates"]:
                                     fallback_res = {
                                         "source": "search_metadata_fallback",
                                         "candidates": extracted["candidates"],
                                         "original_error": "403/Fetch Failed"
                                     }
                                     tool_result_content = json.dumps(fallback_res, ensure_ascii=False)
                                     fallback_success = True
                             
                             if not fallback_success:
                                 url = parsed_args.get("url", "")
                                 try:
                                     domain = urllib.parse.urlparse(url).netloc
                                     if "edu" in domain or "gov" in domain:
                                         target = url.split('/')[-1].replace('-', ' ').replace('_', ' ')
                                         alt_query = f'"{target}" site:wikipedia.org OR site:imdb.com'
                                         print(f"[Monitoring] 403_fallback_trigger query='{alt_query}'")
                                         if "web_search" in tool_functions_map:
                                             ws_res = tool_functions_map["web_search"](query=alt_query)
                                             tool_result_content = ws_res
                                             fallback_success = True
                                 except Exception as e:
                                     print(f"[Monitoring] 403_fallback_error: {e}")

                             if not fallback_success:
                                 tool_result_content += "\n[System Hint]: ç½‘é¡µæŠ“å–å¤±è´¥ã€‚è¯·å°è¯•æœç´¢è¯¥ä¿¡æ¯çš„å…¶ä»–æ¥æºï¼ˆå…¶ä»–ç½‘ç«™ï¼‰ï¼Œä¸è¦å†æ¬¡å°è¯•åŒä¸€ä¸ª URLã€‚"
                    if func_name == "execute_script":
                        try:
                            import re as _re2
                            import json as _json2
                            stdout_match = _re2.search(r'<stdout>\s*(.*?)\s*</stdout>', tool_result_content, _re2.DOTALL)
                            skill_output = None
                            if stdout_match:
                                try:
                                    skill_output = _json2.loads(stdout_match.group(1).strip())
                                except Exception:
                                    skill_output = None
                            status_failed = isinstance(skill_output, dict) and skill_output.get("status") == "failed"
                            codec_err = "codec" in tool_result_content.lower() or "encode" in tool_result_content.lower()
                            if not skill_output or status_failed or codec_err:
                                q = ""
                                try:
                                    q = str((parsed_args.get("args") or {}).get("query") or "")
                                except Exception:
                                    q = ""
                                def _simple_query_optimizer(query_text: str) -> list:
                                    import re as _re3
                                    from difflib import SequenceMatcher as _SeqMatcher
                                    es = _extract_core_entities(query_text)
                                    out = []
                                    if len(es) >= 3:
                                        out.append(" ".join(es[:3]))
                                    if len(es) >= 2:
                                        out.append(f'"{es[0]}" "{es[1]}"')
                                    simplified = _re3.sub(r'\b(çš„|å› ä¸º|æ‰€ä»¥|è€Œä¸”|ä½†æ˜¯|and|or|the|a|an|in|on|at)\b', ' ', query_text, flags=_re3.IGNORECASE)
                                    simplified = _re3.sub(r'\s+', ' ', simplified).strip()
                                    if simplified and simplified.lower() != query_text.lower():
                                        out.append(simplified)
                                    return out[:3]
                                opt_queries = _simple_query_optimizer(q) if q else []
                                if opt_queries:
                                    fallback_skill = {"optimized_queries": opt_queries}
                                    try:
                                        tool_result_content += "\n[System Enforced]: Skill å¤±è´¥ï¼Œå·²ç”Ÿæˆä¼˜åŒ–æŸ¥è¯¢ï¼Œè¯·ä½¿ç”¨å…¶ä¸­ä¹‹ä¸€æ‰§è¡Œ web_searchã€‚"
                                    except Exception:
                                        pass
                                    meta["last_skill_output"] = {"skill_name": parsed_args.get("skill_name", ""), "output": fallback_skill, "step_index": state["step_index"]}
                                    meta["force_skill_next"] = False
                        except Exception as e:
                            print(f"[Monitoring] skill_fallback_error: {e}")
                else:
                    tool_result_content = f"Error: Tool '{func_name}' not found."
            except json.JSONDecodeError as e:
                tool_result_content = f"Error: Failed to parse tool arguments JSON: {func_args_str}. Error: {e}"
                emitted.append(Chunk(step_index=state["step_index"], type="tool_call", tool_call=tool_call))
            except Exception as e:
                tool_result_content = f"Error: Execution failed - {str(e)}"
            msg_display_content = tool_result_content

            if func_name in ["web_fetch", "browse_page"] and len(tool_result_content) > 1000:
                distill_prompt = f"è¯·ä»ä»¥ä¸‹å†…å®¹ä¸­æç‚¼å‡ºå¯¹å›ç­”é—®é¢˜æœ‰ä»·å€¼çš„æ ¸å¿ƒäº‹å®ï¼ˆå®ä½“ã€æ—¶é—´ã€æ•°æ®ï¼‰ï¼Œå»é™¤å¹¿å‘Šå’Œæ— å…³å¯¼èˆªä¿¡æ¯ã€‚ä¿ç•™å…³é”®ç»†èŠ‚ã€‚å†…å®¹:\n{tool_result_content[:4000]}"
                try:
                    distill_resp = client.chat.completions.create(
                        model="qwen3-max",
                        messages=[{"role": "user", "content": distill_prompt}],
                        max_tokens=512
                    )
                    summary = distill_resp.choices[0].message.content
                    memory.add_long(f"Fact Summary from {parsed_args.get('url')}: {summary}")
                    print(f"[Monitoring] memory_distilled for {func_name} len={len(tool_result_content)}->{len(summary)}")
                    msg_display_content = f"[Fact Summary from {parsed_args.get('url')}]:\n{summary}"
                except Exception as e:
                    print(f"[Monitoring] memory_distillation_failed: {e}")
                    memory.add_long(tool_result_content)
            else:
                memory.add_long(tool_result_content)

            emitted.append(Chunk(type="tool_call_result", tool_result=msg_display_content, step_index=state["step_index"], tool_call=tool_call))
            new_messages.append({"role": "tool", "tool_call_id": call_id, "content": msg_display_content})
            memory.add_short(msg_display_content)

            if func_name in ["web_fetch", "browse_page", "web_search"] and tool_result_content and len(tool_result_content) > 100:
                try:
                    # Pass more content to allow JSON parsing in _extract_core_entities
                    new_entities = _extract_core_entities(tool_result_content[:5000])
                    seen_entities = set(meta.get("seen_entities", []))

                    fresh_entities = [e for e in new_entities if e not in seen_entities and len(e) > 2]

                    if fresh_entities and meta.get("dynamic_retrieval_count", 0) < 10: 
                        meta["seen_entities"] = list(seen_entities) + fresh_entities
                        meta["dynamic_retrieval_count"] = meta.get("dynamic_retrieval_count", 0) + 1

                        print(f"[DynamicRetrieval] Detected new entities: {fresh_entities[:3]}")

                        for entity in fresh_entities[:2]: 
                            mem_hits = memory.search_and_inject(entity, top_k=2)

                            if mem_hits:
                                memory_context = "\n".join([
                                    f"- [{h['age_days']}å¤©å‰] {h['text'][:200]}"
                                    for h in mem_hits
                                ])

                                hint_msg = {
                                    "role": "system",
                                    "content": f"<related_memory entity=\"{entity}\">\n{memory_context}\n</related_memory>"
                                }
                                new_messages.append(hint_msg)

                                print(f"[DynamicRetrieval] Injected {len(mem_hits)} memory hits for '{entity}'")

                        if func_name == "web_fetch" and len(tool_result_content) > 500:
                            try:
                                result_data = json.loads(tool_result_content)
                                if isinstance(result_data, dict) and "content" in result_data:
                                    snippet = result_data["content"][:500]
                                    new_memory_items.append(f"[{func_name}] {snippet}")
                            except:
                                new_memory_items.append(tool_result_content[:500])

                except Exception as e:
                    print(f"[DynamicRetrieval] Entity detection error: {e}")

            if func_name == "web_search":
                q = str(parsed_args.get("query") or "")
                if q:
                    if q not in meta["searched_keywords"]:
                        meta["searched_keywords"].append(q)
                        print(f"[Monitoring] search_keyword_added step_index={state['step_index']} query='{q}'")
        return {
            **state,
            "messages": new_messages,
            "emitted": emitted,
            "pending_tool_calls": [],
            "step_index": state["step_index"] + 1,
            "meta": meta,
        }

    def update_memory(state: dict) -> dict:
        return {**state}

    def persist_state(state: dict) -> dict:
        dump = {
            "cid": cid,
            "step_index": state["step_index"],
            "short_memory": memory.short,
            "messages_len": len(state["messages"]),
        }
        state_store.save(cid, dump)
        return {**state}

    def memory_query(query: str, top_k: int = 5) -> str:
        hits = memory.search(query, top_k=top_k)
        return json.dumps({"results": hits}, ensure_ascii=False)

    llm_tools.append(memory_query)
    tool_schema = [function_to_schema(tool_function) for tool_function in llm_tools]
    tool_functions_map = {func.__name__: func for func in llm_tools}

    g = StateGraph(dict)
    g.add_node("llm_step", llm_step)
    g.add_node("execute_tools", execute_tools)
    g.add_node("update_memory", update_memory)
    g.add_node("persist_state", persist_state)
    g.add_edge("__start__", "llm_step")
    g.add_edge("llm_step", "execute_tools")
    g.add_edge("execute_tools", "update_memory")
    g.add_edge("update_memory", "persist_state")
    g.add_edge("persist_state", END)
    graph = g.compile()

    state = {"messages": prompt_messages, "step_index": 0, "pending_tool_calls": [], "meta": {"searched_keywords": [], "seen_entities": [], "plan_forced_skill": plan_forced_skill}, "reflection_injected": False, "reflexion_count": 0, "verification_failures": 0}
    while state["step_index"] < max_steps:
        enhancer = get_agent_enhancer()
        # SkillEnforcer logic removed - allow agent to search specific entities freely

        state = graph.invoke(state)
        
        # 1. åŠ¨æ€åæ€æ£€æŸ¥ç‚¹ (Dynamic Reflection Checkpoints)
        reflection_prompt = enhancer.should_trigger_reflection(
            state["step_index"], 
            max_steps, 
            (state.get("meta") or {}).get("searched_keywords", [])
        )
        if reflection_prompt:
             state["messages"].append({"role": "system", "content": reflection_prompt})
             print(f"[System] Injected Reflection Checkpoint at step {state['step_index']}")
             
        # 2. å®ä½“å…³ç³»å›¾æ„å»º (Dynamic Trigger around 40% progress)
        # ç¡®ä¿æœ‰è¶³å¤Ÿçš„ä¿¡æ¯é‡ï¼ˆè‡³å°‘5æ­¥ï¼‰ï¼Œä¸”åœ¨ä»»åŠ¡ä¸­æ®µè¿›è¡Œæ¢³ç†
        entity_graph_trigger_step = max(5, int(max_steps * 0.4))
        if state["step_index"] == entity_graph_trigger_step:
             print(f"[System] Step {state['step_index']}: Building Entity Relationship Graph (Dynamic Trigger)...")
             try:
                 enhancer.build_entity_graph(user_query, state["messages"])
                 targeted_queries = enhancer.generate_targeted_search_queries()
                 if targeted_queries:
                     hint = "Based on the Entity Relationship Graph, here are targeted queries to fill information gaps:\n" + "\n".join([f"- {q}" for q in targeted_queries])
                     state["messages"].append({"role": "system", "content": hint})
                     print(f"[System] Injected {len(targeted_queries)} targeted queries.")
             except Exception as e:
                 print(f"[System] Entity Graph build failed: {e}")

        # åæ€é¢‘ç‡ä¼˜åŒ–ï¼šä»æ¯3æ­¥æ”¹ä¸ºæ¯7æ­¥ï¼Œå‡å°‘æ— æ•ˆåæ€
        if (state.get("meta") or {}).get("searched_keywords") and state["step_index"] > 0 and (state["step_index"] % 7 == 0):
            kws = (state.get("meta") or {}).get("searched_keywords") or []
            state["messages"].append({"role": "system", "content": f"Reflection step: attempted keywords: {', '.join(kws[-6:])}. Avoid repeating similar queries. Prefer precise entities and advanced operators (site:edu OR site:org, filetype:pdf). If relevance remains low, synthesize best answer so far."})
            print(f"[Monitoring] reflection_step_inserted step_index={state['step_index']} keywords={kws[-6:]}")
        
        for ch in state.get("emitted") or []:
            yield ch
        if not state.get("had_tool_calls"):
            last_msg = state["messages"][-1]
            content = str(last_msg.get("content") or "")
            reflexion_msg = ""
            needs_reflexion = False
            
            searched_kws = (state.get("meta") or {}).get("searched_keywords") or []
            search_count = len(searched_kws)
            
            # [Change] é™ä½å¼ºåˆ¶æœç´¢æ¬¡æ•°ï¼Œé¿å…æ— æ•ˆå¾ªç¯
            MIN_MANDATORY_SEARCH = 4
            
            if search_count < MIN_MANDATORY_SEARCH and state.get("reflexion_count", 0) < 3:
                from collections import Counter
                entity_freq = Counter()
                for kw in searched_kws:
                    for ent in ents:
                        if ent.lower() in kw.lower():
                            entity_freq[ent] += 1
                
                dominant_entity = entity_freq.most_common(1)
                loop_detected = False
                
                if dominant_entity and dominant_entity[0][1] > max(2, search_count * 0.5):
                    stuck_entity = dominant_entity[0][0]
                    reflexion_msg = f"""Reflexion: [Strategy Switch Enforced]
ä½ åœ¨ {search_count} æ¬¡æœç´¢ä¸­,æœ‰ {dominant_entity[0][1]} æ¬¡éƒ½åœ¨æœç´¢ "{stuck_entity}".    

âš ï¸ MANDATORY ACTION:
1. STOP searching for "{stuck_entity}"
2. SWITCH to a DIFFERENT candidate country/person immediately
3. Use excluded_entities=["{stuck_entity}"] in smart-search
4. If you believe "{stuck_entity}" is the only answer, provide EXPLICIT evidence for ALL constraints (education, scandal type, timeline)
"""
                    needs_reflexion = True
                    loop_detected = True
                    
                    if "meta" not in state: state["meta"] = {}
                    state["meta"]["force_skill_next"] = True
                    state["messages"].append({"role": "user", "content": f"[System Enforced]: You are STUCK. You MUST use `smart-search` with `excluded_entities=['{stuck_entity}']` in the next step."})

                is_giving_up = any(phrase in content.lower() for phrase in ["cannot find", "unable to", "don't know", "æ— æ³•", "ä¸çŸ¥é“"])
                
                if not loop_detected:
                    if is_giving_up:
                         reflexion_msg = f"Reflexion: [System Enforced] ä½ ä¼¼ä¹æƒ³æ”¾å¼ƒï¼Œä½†æœç´¢æ¬¡æ•°ä¸è¶³ ({search_count}/5)ã€‚è¯·å°è¯•æ›´æ¢å…³é”®è¯ï¼ˆä¾‹å¦‚ç”¨è‹±æ–‡æœç´¢ã€æ‹†åˆ†å®ä½“ï¼‰å†è¯•ä¸€æ¬¡ã€‚"
                         needs_reflexion = True
                    else:
                         reflexion_msg = f"Reflexion: [System Enforced] ç›®å‰ä»…è¿›è¡Œäº† {search_count} æ¬¡æœç´¢ã€‚å¯¹äºå·²æœ‰æŠŠæ¡çš„é¢˜ç›®ï¼Œè¯·ç¡®ä¿è‡³å°‘éªŒè¯ 5 æ¬¡ï¼›è‹¥ä»ä¸ç¡®å®šï¼Œè¯·ç»§ç»­å¯»æ‰¾è¯æ®ã€‚"
                         needs_reflexion = True

            elif "Final Answer:" in content:
                final_ans = content.split("Final Answer:")[-1].strip()
                if len(final_ans) < 2 and state.get("reflexion_count", 0) < 2:
                    reflexion_msg = "Reflexion: ä½ çš„ç­”æ¡ˆå¤ªçŸ­æˆ–ä¸ºç©ºã€‚è¯·é‡æ–°æ£€æŸ¥ä¹‹å‰çš„ Observationï¼Œå¦‚æœæ‰¾ä¸åˆ°ä¿¡æ¯ï¼Œè¯·å°è¯•ç”¨è‹±æ–‡æœç´¢å…³é”®è¯ã€‚"
                    needs_reflexion = True
                elif ("å¹´ä»½" in user_query or "å¤šå°‘" in user_query) and not any(c.isdigit() for c in final_ans):
                    if state.get("reflexion_count", 0) < 2:
                        reflexion_msg = "Reflexion: ç”¨æˆ·è¯¢é—®çš„æ˜¯æ•°å­—/å¹´ä»½ï¼Œä½†ä½ çš„ç­”æ¡ˆä¸­ä¸åŒ…å«æ•°å­—ã€‚è¯·é‡æ–°æ£€ç´¢æˆ–ä»æ–‡ä¸­æå–å‡†ç¡®æ•°å€¼ã€‚"
                        needs_reflexion = True
                
                elif search_count < 25 and state.get("reflexion_count", 0) < 5:
                    uncertainty_keywords = ["å¯èƒ½", "probably", "unconfirmed", "not found", "unknown", "æœªæ‰¾åˆ°", "æ— æ³•ç¡®è®¤", "suggests", "likely"]
                    if any(k in final_ans.lower() for k in uncertainty_keywords):
                        reflexion_msg = f"Reflexion: ä½ çš„ç­”æ¡ˆåŒ…å«ä¸ç¡®å®šæ€§è¯æ±‡ ('{final_ans[:20]}...')ã€‚è¯·ç»§ç»­æœç´¢éªŒè¯ï¼Œå°è¯•æŸ¥æ‰¾æ›´å¤šæ¥æºä»¥ç¡®è®¤ç­”æ¡ˆã€‚"
                        needs_reflexion = True

            if not needs_reflexion and "Final Answer:" in content and state.get("reflexion_count", 0) < 3:
                has_verified = False
                for msg in state["messages"]:
                    if msg.get("role") == "assistant" and msg.get("tool_calls"):
                        for tc in msg["tool_calls"]:
                            if tc["function"]["name"] == "execute_script" and "multi-source-verify" in tc["function"]["arguments"]:
                                has_verified = True
                                break
                
                is_factual = len(content) < 300 and any(c.isupper() for c in content) or any(c.isdigit() for c in content)
                
                if not has_verified and is_factual:
                     reflexion_msg = f"Reflexion: [System Enforced] ä½ å·²ç»ç»™å‡ºäº†æœ€ç»ˆç­”æ¡ˆï¼Œä½†å°šæœªæ‰§è¡Œã€å¤šæºéªŒè¯ã€‘(multi-source-verify)ã€‚\nä¸ºäº†ç¡®ä¿å‡†ç¡®æ€§ï¼Œè¯·ç«‹å³è°ƒç”¨ `execute_script(skill_name='multi-source-verify', args={{'answer': '{final_ans}'}})` æ¥éªŒè¯è¯¥ç­”æ¡ˆã€‚\nè¿™æ˜¯å¼ºåˆ¶æ­¥éª¤ï¼Œç‰¹åˆ«æ˜¯å¯¹äºäººåã€æ—¶é—´ã€åœ°ç‚¹ç­‰äº‹å®æ€§é—®é¢˜ã€‚"
                     needs_reflexion = True
                     print(f"[Monitoring] Enforcing multi-source-verify for factual answer: {final_ans[:30]}")

            if needs_reflexion:
                state["messages"].append({"role": "user", "content": reflexion_msg})
                state["reflexion_count"] = state.get("reflexion_count", 0) + 1
                state["step_index"] += 1
                print(f"[Monitoring] Reflexion triggered: {reflexion_msg}")
                continue

            if "Final Answer:" in content and not needs_reflexion:
                 raw_final_ans = content.split("Final Answer:")[-1].strip()
                 if len(raw_final_ans) > 1:
                     try:
                        from .verification import verify_answer
                        print(f"[Monitoring] In-Loop Verification for: {raw_final_ans[:50]}...")
                        verified_result = verify_answer(user_query, raw_final_ans)
                        
                        rejection_keywords = ["ä¸ç¬¦åˆ", "ä¸å¯¹", "é”™è¯¯", "åº”ä¸º", "å…¶å®æ˜¯", "incorrect", "wrong", "contradiction", "refuted", "doesn't match", "does not match", "correction:"]
                        
                        is_rejected = False
                        
                        if verified_result.startswith("[REJECTED]"):
                            is_rejected = True
                            print(f"[Monitoring] Verification Explicitly Rejected: {verified_result}")
                        
                        elif len(verified_result) > len(raw_final_ans) * 1.5 and len(verified_result) > 50:
                            if any(kw in verified_result.lower() for kw in rejection_keywords):
                                is_rejected = True
                        
                        elif any("\u4e00" <= ch <= "\u9fff" for ch in raw_final_ans):
                            set_a = set(raw_final_ans)
                            set_b = set(verified_result)
                            if set_a and set_b:
                                intersection = len(set_a & set_b)
                                union = len(set_a | set_b)
                                similarity = intersection / union
                                if similarity < 0.6: 
                                    is_rejected = True
                        else:
                            set_a = set(raw_final_ans.lower().split())
                            set_b = set(verified_result.lower().split())
                            
                            if set_a and set_b:
                                intersection = len(set_a & set_b)
                                union = len(set_a | set_b)
                                similarity = intersection / union
                                if similarity < 0.6: 
                                    is_rejected = True
                                    print(f"[Monitoring] Low similarity ({similarity:.2f}) between Original and Verified. Treating as rejection/correction.")

                        if is_rejected:
                            verification_failures = state.get("verification_failures", 0) + 1
                            state["verification_failures"] = verification_failures

                            if verification_failures >= 2:
                                print(f"[Monitoring] âš ï¸ éªŒè¯å·²å¤±è´¥{verification_failures}æ¬¡ï¼Œå¼ºåˆ¶æ¥å—åŸç­”æ¡ˆ")
                                state["final_verified_answer"] = raw_final_ans
                                break 

                            reason = verified_result
                            if reason.startswith("[REJECTED]:"):
                                reason = reason.replace("[REJECTED]:", "").strip()

                            reflexion_msg = f"Reflexion: [System Enforced Verification] ä½ çš„ç­”æ¡ˆæœªé€šè¿‡æœ€ç»ˆæ ¸æŸ¥ã€‚\nåŸç­”æ¡ˆ: {raw_final_ans}\néªŒè¯åé¦ˆ: {reason}\n\nè¿™æ˜¯ç¬¬{verification_failures}æ¬¡éªŒè¯å¤±è´¥ï¼ˆæœ€å¤š2æ¬¡ï¼‰ã€‚è¯·æ ¹æ®åé¦ˆ**æ¢ä¸€ä¸ªæ–¹å‘**é‡æ–°æœç´¢å’Œæ¨ç†ã€‚ä¸è¦é‡å¤ä¹‹å‰çš„é”™è¯¯è·¯å¾„ã€‚"
                            state["messages"].append({"role": "user", "content": reflexion_msg})
                            state["reflexion_count"] = state.get("reflexion_count", 0) + 1
                            state["step_index"] += 1
                            print(f"[Monitoring] Verification Rejected Answer (Attempt {verification_failures}/2). Feedback: {reason[:100]}...")
                            continue 
                             
                     except Exception as e:
                         print(f"[Warn] In-Loop Verification failed: {e}")

                 print(f"[Monitoring] Answer found and verified, stopping loop.")
                 break
                 
            break
    final_messages = state["messages"][:]
    s = str(user_query or "")
    
    explicit_en = re.search(r"(answer|respond|output|provide).*(in|with).*english", s, re.IGNORECASE) or \
                  re.search(r"english (name|title|version)", s, re.IGNORECASE) or \
                  re.search(r"(è‹±æ–‡|è‹±è¯­)(å|å…¨å|ç§°|å›ç­”|è¾“å‡º)", s)
                  
    explicit_cn = re.search(r"(answer|respond|output|provide).*(in|with).*chinese", s, re.IGNORECASE) or \
                  re.search(r"chinese (name|title|version)", s, re.IGNORECASE) or \
                  re.search(r"(ä¸­æ–‡|æ±‰è¯­)(å|å…¨å|ç§°|å›ç­”|è¾“å‡º)", s)
    
    use_cn_prompt = False
    
    if explicit_cn:
        use_cn_prompt = True
    elif explicit_en:
        use_cn_prompt = False
    else:
        has_cn = any("\u4e00" <= ch <= "\u9fff" for ch in s)
        use_cn_prompt = has_cn

    if use_cn_prompt:
        system_content = f"""ç°åœ¨è¯·åŸºäºå·²æ£€ç´¢ä¸å·²æŠ“å–çš„å†…å®¹ï¼Œç»™å‡ºç®€æ´æ˜ç¡®çš„æœ€ç»ˆç­”æ¡ˆã€‚

### ä¸¥æ ¼çº¦æŸï¼š
1. **å»å™ª**ï¼šæœç´¢ç»“æœå¯èƒ½åŒ…å«æ— å…³ä¿¡æ¯ï¼ˆå¦‚åŠ å¯†è´§å¸ã€æ— å…³å¹´ä»½çš„æŠ¥å‘Šï¼‰ã€‚è¯·**ä»…**æå–ä¸ç”¨æˆ·é—®é¢˜ç›´æ¥ç›¸å…³çš„å®ä½“ã€‚
2. **æ ¼å¼**ï¼šå¦‚æœè¯¢é—®æœºæ„åç§°ï¼Œ**å¿…é¡»**è¾“å‡ºã€ä¸­æ–‡å®˜æ–¹å…¨ç§°ã€‘ã€‚ä¸è¦åŒ…å«è‹±æ–‡ç¼©å†™ï¼Œä¸è¦é‡å¤ï¼Œä¸è¦åŒ…å«è§£é‡Šæ€§æ–‡å­—ã€‚
3. **ç¤ºä¾‹**ï¼š
   - é”™è¯¯ï¼šFCA (è‹±å›½é‡‘èè¡Œä¸ºç›‘ç®¡å±€)
   - é”™è¯¯ï¼šè‹±å›½é‡‘èè¡Œä¸ºç›‘ç®¡å±€ï¼Œå› ä¸ºå®ƒé‡‡ç”¨äº†...
   - æ­£ç¡®ï¼šè‹±å›½é‡‘èè¡Œä¸ºç›‘ç®¡å±€

è¯·åªè¾“å‡ºæœ€ç»ˆç­”æ¡ˆå­—ç¬¦ä¸²ã€‚"""
    else:
        system_content = f"""Based on the retrieved and fetched content, please provide a concise and clear final answer.
1. **Output ONLY the answer text**, do not explain the process, do not include prefixes like "Answer is".
2. **DO NOT GIVE UP**: Even if information is incomplete, you must infer the most likely answer based on existing clues. **ABSOLUTELY FORBIDDEN** to output "Not found", "Unable to confirm", "Unknown", "I don't know", etc.
3. **Language Consistency**: Please answer in English.
4. **Final Check**: Ensure your answer strictly meets all constraints (e.g. time, role, appointment method). If contradictory, prioritize core constraints.
5. If there are multiple candidates, choose the most likely one."""

    if state.get("final_verified_answer"):
        print(f"[Monitoring] Skipping post-loop synthesis. Using verified answer from loop.")
        yield Chunk(type="text", content=state["final_verified_answer"], step_index=state["step_index"])
        return

    final_messages.insert(
        0,
        {
            "role": "system",
            "content": system_content,
        },
    )
    params2 = {
        "model": "qwen3-max",
        "stream": True,
        "max_tokens": 1024,
        "temperature": 0.2,
    }
    try:
        final_messages = _compress_messages(final_messages)
        stream2 = client.chat.completions.create(messages=final_messages, **params2)
        final_emitted = False
        full_ans = ""
        for chunk in stream2:
            chunk = cast(ChatCompletionChunk, chunk)
            if not chunk.choices:
                continue
            delta = chunk.choices[0].delta
            if delta.content:
                full_ans += delta.content
        
        if full_ans:
            try:
                from .verification import verify_answer
                
                cleaned_ans = clean_answer(full_ans)
                verified_ans = verify_answer(user_query, cleaned_ans)
                
                rejection_keywords = ["ä¸ç¬¦åˆ", "ä¸å¯¹", "é”™è¯¯", "åº”ä¸º", "å…¶å®æ˜¯", "incorrect", "wrong", "contradiction", "refuted", "doesn't match", "does not match", "unable to verify"]
                is_rejected = False
                
                if verified_ans.startswith("[REJECTED]"):
                     is_rejected = True
                elif any(kw in verified_ans.lower() for kw in rejection_keywords):
                    if len(verified_ans) < 20 or "answer is wrong" in verified_ans.lower() or "ç­”æ¡ˆé”™è¯¯" in verified_ans:
                         is_rejected = True
                
                if is_rejected:
                    if verified_ans.startswith("[REJECTED]"):
                        print(f"[Monitoring] Verified answer explicitly rejected in FINAL step: '{verified_ans}'.")
                        verified_ans = cleaned_ans
                        print(f"[Monitoring] Fallback to cleaned_ans to avoid outputting [REJECTED] tag to user.")
                    else:
                        print(f"[Monitoring] Verified answer rejected: '{verified_ans}'. Reverting to cleaned_ans.")
                        verified_ans = cleaned_ans
                
                final_ans_str = clean_answer(verified_ans)
                
                if final_ans_str:
                    full_ans = final_ans_str
                    
                print(f"[Monitoring] Final Answer Processed: '{full_ans}'")
                
            except Exception as e:
                print(f"[Warn] Verification failed: {e}")

            if is_rejected:
                 print("[Monitoring] Final Post-Loop Verification Failed. Attempting one last-ditch correction...")
                 
                 feedback_content = f"System Feedback: Your previous answer was REJECTED. \nREASON: {verified_ans}\n\nINSTRUCTION: Please use the specific entities, dates, or corrections mentioned in the 'REASON' above to construct a new, accurate answer. If the Reason suggests a specific person or fact, TRUST IT. \nOutput ONLY the corrected answer text."
                 
                 correction_prompt = [
                     {"role": "system", "content": "You are a helpful assistant. You must correct your answer based on the provided System Feedback."},
                     {"role": "user", "content": f"Question: {user_query}"},
                     {"role": "assistant", "content": cleaned_ans},
                     {"role": "user", "content": feedback_content}
                 ]
                 try:
                     correction_resp = client.chat.completions.create(
                         model="qwen3-max", 
                         messages=correction_prompt,
                         max_tokens=1024,
                         temperature=0.1 
                     )
                     new_ans = correction_resp.choices[0].message.content.strip()
                     new_ans_cleaned = clean_answer(new_ans)
                     
                     print(f"[Monitoring] Last-ditch correction candidate: {new_ans_cleaned[:50]}...")
                     
                     new_verified = verify_answer(user_query, new_ans_cleaned)
                     
                     if not new_verified.startswith("[REJECTED]"):
                         full_ans = clean_answer(new_verified)
                         print(f"[Monitoring] Last-ditch correction successful: {full_ans[:50]}...")
                     else:
                         print(f"[Monitoring] Last-ditch correction failed again: {new_verified[:50]}...")
                         full_ans = new_ans_cleaned
                         
                 except Exception as e:
                     print(f"[Warn] Last-ditch correction error: {e}")
            
            yield Chunk(type="text", content=full_ans, step_index=state["step_index"])
            final_emitted = True

        if full_ans:
             searched_kws = (state.get("meta") or {}).get("searched_keywords") or []
             last_res = (state.get("meta") or {}).get("last_search_results") or []
             conf = calculate_confidence_impl(full_ans, searched_kws, last_res)
             print(f"[Monitoring] Answer Confidence: {conf} (Answer length: {len(full_ans)})")

        if not final_emitted and not full_ans:
            try:
                last_results = (state.get("meta") or {}).get("last_search_results") or []
                extracted = extract_answer_from_search_results(last_results, user_query)
                if isinstance(extracted, dict) and extracted.get("candidates"):
                    top = extracted["candidates"][0]
                    text = str(top.get("text") or "").strip()
                    if text:
                        yield Chunk(step_index=state["step_index"], type="text", content=text)
                    else:
                        yield Chunk(step_index=state["step_index"], type="text", content="æœªæ£€ç´¢åˆ°æ˜ç¡®ç­”æ¡ˆ")
                else:
                    yield Chunk(step_index=state["step_index"], type="text", content="æœªæ£€ç´¢åˆ°æ˜ç¡®ç­”æ¡ˆ")
            except Exception:
                yield Chunk(step_index=state["step_index"], type="text", content="æœªæ£€ç´¢åˆ°æ˜ç¡®ç­”æ¡ˆ")
    except Exception as e:
        print(f"[Monitoring] Final synthesis failed: {e}")
        if "DataInspectionFailed" in str(e) or "inappropriate" in str(e) or "content" in str(e).lower():
            try:
                print(f"[Monitoring] Triggering Safety Fallback Synthesis (Reconstructing Safe Context)...")
                
                safe_context = "Summary of Search Results:\n"
                
                last_results = (state.get("meta") or {}).get("last_search_results") or []
                if last_results:
                     for i, r in enumerate(last_results[:5]):
                         safe_context += f"- {r.get('title')}: {r.get('summary') or r.get('snippet')}\n"
                
                for msg in state["messages"]:
                    if msg["role"] == "tool":
                        content = str(msg["content"])
                        if '"title":' in content and '"url":' in content and len(content) < 5000:
                            try:
                                data = json.loads(content)
                                if isinstance(data, dict) and "results" in data:
                                     for i, r in enumerate(data["results"][:3]):
                                         safe_context += f"- {r.get('title')}: {r.get('summary') or r.get('snippet')}\n"
                            except:
                                pass
                
                fallback_sys = "You are a helpful assistant. The original context contained sensitive content, so it was filtered. Based on the provided SEARCH SUMMARIES below, please answer the user question. Do not hallucinate."
                
                fallback_msgs = [
                    {"role": "system", "content": fallback_sys},
                    {"role": "user", "content": f"Question: {user_query}\n\n{safe_context[:2000]}"} 
                ]
                
                print(f"[Monitoring] Fallback Context Length: {len(safe_context)}")

                fallback_resp = client.chat.completions.create(
                    model="qwen3-max", 
                    messages=fallback_msgs,
                    max_tokens=512
                )
                fallback_ans = fallback_resp.choices[0].message.content or "ç”±äºå®‰å…¨ç­–ç•¥é™åˆ¶ï¼Œä¸”æœç´¢æ‘˜è¦ä¸è¶³ä»¥å›ç­”ï¼Œæ— æ³•ç”Ÿæˆå®Œæ•´å›ç­”ã€‚"
                yield Chunk(type="text", content=fallback_ans, step_index=state["step_index"])
                return
            except Exception as e2:
                 print(f"[Monitoring] Fallback also failed: {e2}")

        if not full_ans:
             pass

    try:
        candidates_list = []
        last_results = (state.get("meta") or {}).get("last_search_results") or []
        if last_results:
            extracted = extract_answer_from_search_results(last_results, user_query)
            if extracted and extracted.get("candidates"):
                candidates_list = extracted["candidates"]

        if "meta" not in state:
            state["meta"] = {}
        state["meta"]["candidates"] = candidates_list

        serializable_state = make_json_serializable(state)
        yield Chunk(
            type="final_state",
            content=json.dumps({"state": serializable_state}, ensure_ascii=False),
            step_index=state["step_index"]
        )
        print(f"[Monitoring] Final state yielded with {len(candidates_list)} candidates")
    except Exception as e:
        print(f"[Monitoring] Failed to yield final state: {e}")
        print(f"[Monitoring] Failed to yield final state: {e}")
